[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "blog",
    "section": "",
    "text": "Want to support my blog? \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhistorydata: judges\n\n\n\n\n\n\nJen Richmond\n\n\nJun 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\ngutenberg\n\n\n\n\n\n\nJen Richmond\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nfont notes\n\n\n\n\n\n\nJen Richmond\n\n\nJun 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\ndungeons and dragons monsters\n\n\n\n\n\n\nJen Richmond\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nselective annotations\n\n\n\n\n\n\nJen Richmond\n\n\nMay 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nsydney beaches\n\n\n\n\n\n\nJen Richmond\n\n\nMay 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\ncool stuff from tidytuesdayR\n\n\n\n\n\n\nJen Richmond\n\n\nMay 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nvesuvius\n\n\n\n\n\n\nJen Richmond\n\n\nMay 13, 2025\n\n\n\n\n\n\n\n\n\n\n\nwhat happened? what now?\n\n\n\n\n\n\nJen Richmond\n\n\nMay 9, 2025\n\n\n\n\n\n\n\n\n\n\n\nnsf grants terminated\n\n\n\n\n\n\nJen Richmond\n\n\nMay 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n30dayChartChallenge debrief\n\n\n\n\n\n\nJen Richmond\n\n\nMay 2, 2025\n\n\n\n\n\n\n\n\n\n\n\nPūteketeke Pandemonium\n\n\n\n\n\n\nJen Richmond\n\n\nApr 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearning python\n\n\n\npython\n\n\n\n\n\n\n\nJen Richmond\n\n\nJul 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nrowwise %&gt;% mean\n\n\n\ndata wrangling\n\n\n\n\n\n\n\nJen Richmond\n\n\nMay 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nerror bars on plots\n\n\n\nidhgt\n\nggplot\n\n\n\n\n\n\n\nJen Richmond\n\n\nApr 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolours that R knows\n\n\n\nggplot\n\n\n\n\n\n\n\nJen Richmond\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\ngit hints\n\n\n\n\n\n\nJen Richmond\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nanalysing smartwatch data\n\n\n\n\n\n\nJen Richmond\n\n\nJul 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nusing lists in R\n\n\n\n\n\n\nJen Richmond\n\n\nJun 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nmy favourite things about R\n\n\n\n\n\n\nJen Richmond\n\n\nJan 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nuseful bash commands\n\n\n\n\n\n\nJen Richmond\n\n\nJan 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameterised penguins\n\n\n\n\n\n\nJen Richmond\n\n\nAug 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nPAT for GitHub\n\n\n\n\n\n\nJen Richmond\n\n\nJun 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nmy first pull request\n\n\n\n\n\n\nJen Richmond\n\n\nJan 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\ncleaning penguins with the janitor package\n\n\n\n\n\n\nJen Richmond\n\n\nNov 18, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I am a psychologist by training and have been teaching myself (and others) R for a few years now.\nI am one of the founders of R-Ladies Sydney and developer of #RYouWithMe.\nI like to learn new things and write about what I have learned on my blog.\nI am good at identifying a gap and then make something to fill it.\nThis page showcases some of my favourite projects.\nI LOVE data viz and storytelling; check out my portfolio."
  },
  {
    "objectID": "pieces.html",
    "href": "pieces.html",
    "title": "resume pieces",
    "section": "",
    "text": "insert talks/links that dont print pdf here"
  },
  {
    "objectID": "pieces.html#presentations-and-workshops",
    "href": "pieces.html#presentations-and-workshops",
    "title": "resume pieces",
    "section": "",
    "text": "insert talks/links that dont print pdf here"
  },
  {
    "objectID": "pieces.html#references",
    "href": "pieces.html#references",
    "title": "resume pieces",
    "section": "References",
    "text": "References\nAvailable upon request"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html",
    "href": "charts/2025-03-31_challenge/index.html",
    "title": "day 0 challenge",
    "section": "",
    "text": "The Our World in Data is a fun place to explore data visualisations. They always make really interesting plots out of all kinds of different data, but they use an in-house data viz platform called “Grapher” so it isn’t easy to find code that can be used to reproduce the charts.\nMy goal in April is use the 30 day chart challenge prompt to find a Our World in Data plot that looks interesting, and then to try to reproduce the plot using ggplot code.\nI will post the final code for each plot in a tabset below, but if you want to see how each chart came about (i.e. my troubleshooting process), there will be a blog post associated with each one.\nThe owidapi package is an easy way to get access to the data sources that the Our World in Data site uses.\nlibrary(owidapi)\n\ncatalog &lt;- owid_get_catalog()\n\nwillingness &lt;- owid_get(\"willingness-climate-action\")"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-1-fractions",
    "href": "charts/2025-03-31_challenge/index.html#day-1-fractions",
    "title": "day 0 challenge",
    "section": "Day 1 fractions",
    "text": "Day 1 fractions\nWillingness climate action\n\nmy plotr code\n\n\n\n\n\n\nwillingness2024 %&gt;%\n  filter(entity_name != \"World\") %&gt;%\n  ggplot(aes(y =  prediction_others_willingness, x = self_willingness, colour = region)) +\n  geom_point() +\n  scale_y_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                      limits = c(0,100), expand = c(0,0), breaks = seq(0,100,20)) +\n    scale_x_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                       limits = c(0,100), expand = c(0,0), breaks = seq(0,100,20)) +\n  labs(y = \"Predicted share willing to give\", \n       x = \"Actual share who said they were willing to give 1% of their income\", \n       title = \"People underestimate others' willingness to take climate action\", \n       subtitle = \"Participants were asked if they would contribute 1% of their income to tackle \\nclimate change. \\nThe share that answered 'yes' is shown on the horizontal axis. \\nThe share of the population in their country that people think would be willing \\nis shown on the vertical axis.\") +\n  theme_bw()  +\n  easy_remove_gridlines(axis = \"both\", minor = TRUE, major = FALSE) +\n  geom_abline(\n    slope = 1, \n    intercept = 0, \n    color = \"grey\", \n    linetype = \"dotted\") +\n  scale_colour_manual(values = c(\"#a2559b\", \"#00847d\", \"#4b6a9c\", \"#e56e59\", \"#38aaba\", \"#883039\")) +\n  easy_remove_legend_title() +\n  geom_text_repel(aes(label = entity_name), size = 3, max.overlaps = 20)"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-2-slope",
    "href": "charts/2025-03-31_challenge/index.html#day-2-slope",
    "title": "day 0 challenge",
    "section": "Day 2 slope",
    "text": "Day 2 slope\nTime spent alone\n\nmy plotr code\n\n\n\n\n\n\ntime %&gt;%\n  filter(age &lt; 80) %&gt;%\n  filter(group == \"All people\") %&gt;%\n  ggplot(aes(x = age, y = hours, colour = category)) +\n  geom_point(size = 1) + \n  geom_line() +\n  scale_colour_manual(values = c(\"#496899\", \"#6b3d8d\", \"#2b8465\", \"#986d39\", \"#b03508\", \"#883039\")) +\n  theme_minimal() +\n  scale_y_continuous(expand = c(0,0), limits = c(-0.05,8.1), breaks = seq(0,9,1)) +\n  scale_x_continuous(breaks=c(15,30,40,50,60,70,80)) +\n  easy_remove_gridlines(axis = \"x\") +\n  easy_remove_gridlines(axis = \"y\", major = FALSE, minor = TRUE) +\n  theme(panel.grid = element_line(linewidth =  0.4, linetype = 2)) +\n  theme(axis.ticks.x =  element_line(linewidth = 0.5, color=\"darkgrey\") , \n        axis.line.x = element_line(linewidth = 0.2, colour = \"darkgrey\", linetype=1)) +\n  easy_remove_legend() +\n  ### the geom_text code below are created using the ggannotate package\n  geom_text(data = data.frame(x = 82, y = 7.8, \n    label = \"Alone\"), aes(x = x, y = y, label = label), size = 3, colour = \"#496899\") +\n  geom_text(data = data.frame(x = 82, y = 4.4, \n    label = \"With \\npartner\"), aes(x = x, y = y, label = label), size = 3,colour = \"#6b3d8d\") +\n  geom_text(data = data.frame(x = 80, y = 1.3, \n    label = \"With family\"),aes(x = x, y = y, label = label), size = 3, colour = \"#2b8465\") +\n  geom_text(data = data.frame(x = 83.5, y = 0.7, \n    label = \"With children\"), aes(x = x, y = y, label = label), size = 2.5,colour = \"#986d39\") +\n geom_text(data = data.frame(x = 83.5, y = 0.5, \n    label = \"With friends\"), aes(x = x, y = y, label = label), size = 2.5,colour = \"#b03508\") +\n  geom_text(data = data.frame(x = 83.5, y = 0.1, \n    label = \"With coworkers\"), aes(x = x, y = y, label = label), size = 2.5, colour = \"#883039\") +\n  labs(title = \"Who Americans spend their time with, by age, All people\", \n       subtitle = \"Measured in hours per day, based on averages from surveys in the United States \\nbetween 2010 and 2023\", \n       x = \"Age\", \n       y = \"Hours\",\n       caption = \"Data source: U.S. Bureau of Labor Statistics (2023). \\nNote: Activities such as sleeping, grooming, and personal care are not included in the data. \\nRelationships used to categorize people are not exhaustive and time spent with multiple people counts toward all \\n(e.g., attending a party with friends and partner counts toward both friends and partner)\") +\n  theme(plot.caption = element_text(hjust = 0)) # make the caption appear on the left"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-3-circular",
    "href": "charts/2025-03-31_challenge/index.html#day-3-circular",
    "title": "day 0 challenge",
    "section": "Day 3 circular",
    "text": "Day 3 circular\nWomen in government\n\nmy plotr code\n\n\n\n\n\n\nshare2024_regions %&gt;%\n  ggplot(aes(x = \"\", y = percent, fill = percent_women)) +\n  geom_col(colour = \"white\") +\n coord_polar(\"y\", start = 0) +\n  facet_wrap(~entity_name) +\n    scale_fill_manual(values = c(\"#a2559b\", \"#00847d\", \"#4b6a9c\", \"#e56e59\", \"#38aaba\", \"#6b3d8d\",  \"#986d39\" )) +\n    theme_void() +\n   easy_add_legend_title(\"Percent seats\") +\nlabs(title = \"Percent of countries by share of women in parliament in 2024, Regions\", \n       subtitle = \"Percent of seats in lower or single chamber of the legislature held by women\", \n       caption = \"Data source: V-Dem (2025) via OurWorldInData. \\nNote: Only countries with at least one legislative chamber included.\")"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-4-big-small",
    "href": "charts/2025-03-31_challenge/index.html#day-4-big-small",
    "title": "day 0 challenge",
    "section": "Day 4 big small",
    "text": "Day 4 big small\nTime spent in unpaid work\n\nmy plotr code\n\n\n\n\n\n\ndom_recent %&gt;%\n  ggplot(aes(x = male, y = female, colour = country)) +\n  geom_point(size = 2) +\n  theme_classic() +\n  easy_remove_legend() +\n  theme(panel.grid.major = element_line(color = \"lightgray\", linetype = 2)) +\n   scale_y_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                     expand = c(0,0), breaks = seq(0,25, 5), limits = c(0,30)) +\n   scale_x_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                      expand = c(0,0), breaks = seq(0,12, 2), limits = c(0,14)) +\n  geom_abline(intercept = 0, slope = 1, colour = \"gray\", linetype = 2) +\n  geom_abline(intercept = 0, slope = 2, colour = \"gray\", linetype = 2) +\n  geom_abline(intercept = 0, slope = 4, colour = \"gray\", linetype = 2) +\n  geom_text(data = data.frame(x = 10, y = 3.5, label = \"Women and men spend \\nequal time\"), \n            mapping = aes(x = x, y = y, label = label), size = 2.5, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 12.5, y = 28, label = \"Women spend 2x \\nmore time\"), \n            mapping = aes(x = x, y = y, label = label), size = 2.5,  inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 5, y = 26.8, label = \"Women spend 4x \\nmore time\"), \n            mapping = aes(x = x, y = y, label = label), size = 2.5, inherit.aes = FALSE) +\n  geom_text_repel(aes(label = country), size = 2.5, max.overlaps = 5) +\n  labs(title = \"Time women and men spend on unpaid care and domestic work, 2022\",\n       subtitle = \"The average share of each day that women and men aged 15 and older spend on unpaid care \\nand domestic work.\",\n       x = \"Men (% of 24 hour day)\", \n       y = \"Women (% of 24 hour day)\", \n       caption = \"Data source: UN Statistics Division and UN WOMEN.\nNote: Unpaid care and domestic work includes: food preparation,\ndishwashing, upkeep of a dwelling, laundry, ironing, gardening, caring\nfor pets, shopping, servicing and repair of personal and household\ngoods, childcare, and care of the sick, elderly or disabled household\nmembers, among others.\") +\n   theme(plot.caption = element_text(hjust = 0), \n         plot.subtitle = element_text(margin=margin(0,0,20,0))) +\n  geom_curve(data = data.frame(x = 5.5, y = 28.5, xend = 6.4, yend = 26),\n          mapping = aes(x = x, y = y, xend = xend, yend = yend), curvature = -0.515, \n          arrow = arrow(20L, unit(0.1, \"inches\"), \"last\", \"closed\"), alpha = 1, inherit.aes = FALSE) +\n  geom_curve(data = data.frame(x = 11.8, y = 28, xend = 12.2, yend = 26),\n          mapping = aes(x = x, y = y, xend = xend, yend = yend), curvature = 0.515, \n          arrow = arrow(20L, unit(0.1, \"inches\"), \"last\", \"closed\"), alpha = 1, inherit.aes = FALSE) +\n  geom_curve(data = data.frame(x = 8.8, y =4, xend = 8.2, yend = 7),\n          mapping = aes(x = x, y = y, xend = xend, yend = yend), curvature = -0.515, \n          arrow = arrow(20L, unit(0.1, \"inches\"), \"last\", \"closed\"), alpha = 1, inherit.aes = FALSE)"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-5-ranking",
    "href": "charts/2025-03-31_challenge/index.html#day-5-ranking",
    "title": "day 0 challenge",
    "section": "Day 5 ranking",
    "text": "Day 5 ranking\n\nR&Dspending\nR&Dresearchers\n\n\nmy plotr code\n\n\n\n\n\n\ntop5spend %&gt;%\nggplot(aes(year, rank, color = country)) +\n  geom_point(size = 5) +\n   geom_bump(size = 2, smooth = 8) +\n  scale_colour_manual(values = palette_takehe_plus) +\n  geom_text(data = top5spend %&gt;% filter(year == min(year)),\n            aes(x = year - .1, label = country), size = 3.5, hjust = 1) +\n  geom_text(data = top5spend %&gt;% filter(year == max(year)),\n            aes(x = year + .1, label = country), size = 3.5, hjust = 0) +\n    scale_y_reverse() +\n  scale_x_continuous(limits = c(2017.6, 2021.5),\n                     breaks = seq(2018, 2021, 1)) +\n  theme_minimal() +\n  easy_remove_legend() +\n  easy_remove_gridlines() +\n  labs(title = \"R&D spending percent of GDP: Top 5 countries\", \n       subtitle = \"includes basic research, applied research, and experimental development\", \n       y = \"Ranking\", x = \"Year\") +\n  geom_text(data = data.frame(x = 2019, y = 4.7, label = \"Switzerland\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 3.7, colour = \"#D1C7B5\", inherit.aes = FALSE)"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-6-florence",
    "href": "charts/2025-03-31_challenge/index.html#day-6-florence",
    "title": "day 0 challenge",
    "section": "Day 6 florence",
    "text": "Day 6 florence\nWillingness to get the COVID vaccine\n\nmy plotr code\n\n\n\n\n\n\nus_oz %&gt;%\n  ggplot(aes(x = month, y = percent, fill = status)) +\n  geom_col() +\n  scale_fill_manual(values = palette) +\n  coord_polar() +\n  facet_wrap(~country) +\n  easy_remove_axes(which = \"y\") +\n  theme(panel.background = element_rect(fill = 'white', colour = 'white')) +\n  easy_move_legend(to = \"bottom\") +\n  labs(title = \"Willingness to get vaccinated against COVID-19, Mar 15, 2021 to Oct 15, 2021\") +\n  easy_remove_axes(which = \"x\", what = \"title\")"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-7-outliers",
    "href": "charts/2025-03-31_challenge/index.html#day-7-outliers",
    "title": "day 0 challenge",
    "section": "Day 7 outliers",
    "text": "Day 7 outliers\nSex ratio outliers\n\nmy plotr code\n\n\n\n\n\n\nratio4 %&gt;%\n  ggplot(aes(x = year, y = ratio, colour = country)) +\n  geom_point() +\n  geom_line() +\n  theme_classic() +\nscale_colour_manual(values = get_pal(\"eastern_rosella\")) +\n  scale_y_continuous(expand = c(0,0), limits = c(98, 120)) +\n  scale_x_continuous(breaks = seq(1950, 2020, 10)) +\n  geom_hline(yintercept = 100, linetype = 2) +\n  geom_text(data = data.frame(x = 2005, y = 100.8, label = \"Equal numbers of newborn boys and girls\"), mapping = aes(x = x, y = y, label = label), size = 3, inherit.aes = FALSE) +\n  labs(y = \"Sex ratio\", x = \"Year\", \n       title = \"Sex ratio at birth, 1950 to 2023\", \n       subtitle = \"The sex ratio at birth is measured as the number of newborn \\nboys for every 100 newborn girls. Higher values indicate a much \\nhigher number of newborn boys than girls.\", \n       caption = \"Data source: UN, World Population Prospects (2024). \\nNote: It's considered that 105 is the biologically expected sex ratio at birth.\")"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-9-diverging",
    "href": "charts/2025-03-31_challenge/index.html#day-9-diverging",
    "title": "day 0 challenge",
    "section": "Day 9 diverging",
    "text": "Day 9 diverging\nFamily and work\n\nmy plotr code\n\n\n\n\n\n\nwork_family %&gt;%\n  ggplot(aes(x = year, y = important, colour = life_area)) +\n  geom_point(size = 2) +\n  geom_line() +\n  facet_wrap(~ country) +\n  labs(title = \"How important family and work are to people in life\", \n       subtitle = \"Share of survey respondents rating `very important` or `rather important`\", \n       x = \"Year\", y = \"Percent of people\", \n       caption = \"Data source: Integrated Values Surveys (2022)\") +\n  theme_minimal() +\n  scale_y_continuous(limits = c(70,100)) +\n  scale_x_continuous(limits = c(1990, 2022)) +\n  scale_color_brewer(palette = \"Set1\") +\n  easy_add_legend_title(\"Life Area\")"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-11-stripes",
    "href": "charts/2025-03-31_challenge/index.html#day-11-stripes",
    "title": "day 0 challenge",
    "section": "Day 11 stripes",
    "text": "Day 11 stripes\nwarming stripes\n\nmy plotr code\n\n\n\n\n\n\nmaxmin &lt;- range(qmean$annual, na.rm = T)\nmd &lt;- mean(qmean$annual, na.rm = T)\n\n\nqmean %&gt;%\n    ggplot(aes(x = date, y = 1, fill = annual)) +\n  geom_tile() +\n  scale_fill_gradientn(colors = rev(col_strip), \n                       values = scales::rescale(c(maxmin[1], md, maxmin[2])), \n                       na.value = \"gray80\") +\n  scale_x_continuous(limits = c(1972, 2022), expand = c(0,0), breaks = seq(1972,2022, 10)) +\n  labs(\n    title = \"Queenstown 1972-2022\",\n    caption = \"Data: Stats NZ\", \n    x = \"Year\") +\n  coord_cartesian(expand = FALSE) +\n  theme_strip() +\n  easy_remove_axes(which = \"x\")"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-13-clusters",
    "href": "charts/2025-03-31_challenge/index.html#day-13-clusters",
    "title": "day 0 challenge",
    "section": "Day 13 clusters",
    "text": "Day 13 clusters\n\nmy plotr code\n\n\n\n\n\n\nlibrary(factoextra)\n\nfviz_cluster(km_bodymass_bill, data = bodymass_bill, \n             show.clust.cent = FALSE, label=NA) +\n  theme_bw() +\n  labs(title = \"Cluster plot k-means penguin classification\")"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-16-negative",
    "href": "charts/2025-03-31_challenge/index.html#day-16-negative",
    "title": "day 0 challenge",
    "section": "Day 16 negative",
    "text": "Day 16 negative\nIs your country going in the right direction?\n\nmy plotr code\n\n\n\n\n\n\nright_wrong_long_new %&gt;%\n  filter(country != \"France\") %&gt;%\n  ggplot(aes(x = country, y = percent, fill = track)) +\n  scale_fill_manual(values = palette, name = \"Direction\", labels = c(\"Wrong\", \"Right\")) +\n  geom_col() +\n  coord_flip() +\n  theme_minimal() +\n  easy_remove_gridlines() +\n  scale_y_continuous(expand = c(0,0)) +\n  labs(y = \"Country\", x = \"Percent of people\", \n       title = \"Percent of the public who rate their country as heading in \\nright direction vs the wrong direction\", \n       caption = \"Source: Ipsos: What Worries The World? March 2025 • \\nBase: Representative sample of 25,746 adults aged 16-74 in 29 participating countries, \\nFebruary 21 2025 - March 7 2025.\") +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-17-birds",
    "href": "charts/2025-03-31_challenge/index.html#day-17-birds",
    "title": "day 0 challenge",
    "section": "Day 17 birds",
    "text": "Day 17 birds\nPuteketeke2023: A scrollytelling story\n\nmy plotr code\n\n\n\n\n\n\nd23 %&gt;%\n  ggplot(aes(x = rank_number, y = votes, fill = maori_name)) +\n   geom_col() +\n  scale_x_continuous(breaks = seq(1,10, 1)) +\n     scale_y_continuous(limits = c(0,350000)) +\n  labs(y = \"Number #1 Votes\", x = \"Rank\", title = \"Bird of the Century 2023\", subtitle= \"Winner = Pūteketeke / Australasian crested grebe\",\n       caption = \"Note: some argue that the 2023 results were impacted by foreign election interference\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Paired\", name = \"Maori name\")\n\n\np23 + geom_image(\n    data = tibble(rank_number = 1, votes = 320000, maori_name = \"Pūteketeke\"),\n    aes(image = here::here(\"charts\", \"2025-04-17_birds\", \"img\", \"grebe.jpg\")), size = 0.15)"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-19-smooth",
    "href": "charts/2025-03-31_challenge/index.html#day-19-smooth",
    "title": "day 0 challenge",
    "section": "Day 19 smooth",
    "text": "Day 19 smooth\n\nmy plotr code\n\n\n\n\n\n\neggs %&gt;%\n  ggplot(aes(x = date, y = price, colour = currency)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = FALSE, na.rm = TRUE) +\n  scale_colour_brewer(palette = \"Dark2\", name = \"Country\", labels = c(\"NZ\", \"USA\")) +\n  theme_minimal() +\n  labs(y = \"Price per dozen ($)\", x = \"Year\", \n       title = \"Egg prices (per dozen) over time\", \n       caption = \"NZ Data from StatsNZ; \\nUS Data from Bureau of Labor Statistics\")"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-22-stars",
    "href": "charts/2025-03-31_challenge/index.html#day-22-stars",
    "title": "day 0 challenge",
    "section": "Day 22 stars",
    "text": "Day 22 stars\nHollywood age differences\n\nmy plotr code\n\n\n\n\n\n\nage_diff_decade %&gt;%\n  filter(release_year &gt;= 1980) %&gt;%\n  ggplot(aes(x = decade, y = age_difference, fill = decade)) +\n geom_rain(alpha = .5, \n            boxplot.args.pos = list(\n              width = .1, position = position_nudge(x = 0.2)),\n            violin.args.pos = list(\n              side = \"r\",\n              width = 0.7, position = position_nudge(x = 0.3))) +\n  theme_minimal() +\n  easy_remove_legend() +\n    scale_y_continuous(expand = c(0,0), limits = c(-.2, 55)) +\n  labs(y = \"Age difference\", x = \"Decade\",  subtitle = \"1980s - current\")"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-24-who",
    "href": "charts/2025-03-31_challenge/index.html#day-24-who",
    "title": "day 0 challenge",
    "section": "Day 24 WHO",
    "text": "Day 24 WHO\nGlobal Hepatitis Report 2024\n\nmy plotr code\n\n\n\n\n\n\nhepB &lt;- hep %&gt;% \n  ggplot(aes(area = new_hepB, fill = WHO_region, layout = \"fixed\",\n             label = paste(WHO_region, new_hepB, sep = \"\\n\"))) +\n  geom_treemap(colour = \"white\") +\n  scale_fill_manual(values = palette) +\n  geom_treemap_text(colour = \"black\",\n                    place = \"topleft\",\n                    size = 5, \n                    grow = FALSE) + # option from ggfittext to NOT make font fit box\n  easy_remove_legend() +\nlabs(title = \"&lt;span style='color:#F4BD4B;'&gt;Figure 2.4&lt;/span&gt;&lt;span&gt; Number of new hepatitis B and hepatitis C infections, by WHO region, 2022&lt;/span&gt;\", \n     subtitle = \"Hepatitis B\") +\n  theme(plot.subtitle = element_text(hjust = 0.5, size = 5), \n        plot.title = element_markdown(hjust = 0, size = 8, face = \"bold\"))\n\n hepC &lt;- hep %&gt;% \n  ggplot(aes(area = new_hepC, fill = WHO_region, layout = \"fixed\",\n             label = paste(WHO_region, new_hepC, sep = \"\\n\"))) +\n  geom_treemap(colour = \"white\") +\n  scale_fill_manual(values = palette) +\n  geom_treemap_text(colour = \"black\",\n                    place = \"topleft\",\n                    size = 5, \n                    grow = FALSE) + # option from ggfittext to NOT make font fit box\n  easy_remove_legend() +\n  labs(subtitle = \"Hepatitis C\") +\n   theme(plot.subtitle = element_text(hjust = 0.5, size = 5))\n\n \n hepB  + hepC"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-25-risk",
    "href": "charts/2025-03-31_challenge/index.html#day-25-risk",
    "title": "day 0 challenge",
    "section": "Day 25 risk",
    "text": "Day 25 risk\nCDC measles data\n\nmy plotr code\n\n\n\n\n\n\nplot_usmap(data = cases2425, values = \"cases_calendar_year\", labels=FALSE) +\n  scale_fill_continuous(low = \"white\", high = \"red\", \n                         name = \"cases\", \n                        limits = c(0,600)) +\n  labs(title = \"USA Measles cases by state\", subtitle = \"Data as of April 17, 2025\", caption = \"Data from CDC \\nhttps://www.cdc.gov/measles/data-research/index.html\") +\n  facet_wrap(~ year) +\n  theme(\n    legend.position = \"bottom\",\n    strip.background = element_blank(),\n    strip.text = element_text(color = \"black\", size = 10)\n  )"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-27-noise",
    "href": "charts/2025-03-31_challenge/index.html#day-27-noise",
    "title": "day 0 challenge",
    "section": "Day 27 noise",
    "text": "Day 27 noise\nSuper Bowl Sunday noise\n\nmy plotr code\n\n\n\n\n\n\nsb %&gt;%\n  ggplot(aes(x = hours_to_game_begin, y = avg_leq, \n             colour = super_bowl_sunday, linetype = super_bowl_sunday)) +\n  geom_line() +\n  facet_grid(year~game_zone) +\n  theme_minimal() +\n   easy_remove_gridlines(axis = c(\"x\")) +\n    scale_colour_manual(values = c(\"black\", \"red\")) +\n   scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  easy_remove_legend() +\n  annotate(\"rect\", xmin = 0, xmax = 3.5, ymin = -Inf, ymax = Inf, alpha = 0.1, fill = \"darkgrey\") +\n    scale_x_continuous(limits = c(-6, 9), breaks= seq(-6, 9, 3)) +\n   scale_y_continuous(breaks= seq(50, 70, 10)) +\n  labs(title = \"Super Bowl Sunday noise exposure in decibels\", x = \"Hours from start of Super Bowl\", y = \"Average noise exposure (in decibels)\", caption = \"Data from Apple Hearing Study\") +\n  geom_text(data = data.frame(x = -1.3, y = 53, label = \"Start game \\n6:30 PM EST\", game_zone = \"Game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 2.3, y = 53, label = \"End game \\n10:00 PM EST\", game_zone = \"Game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = -1.3, y = 53, label = \"Start game \\n6:30 PM EST\", game_zone = \"Non-game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 2.3, y = 53, label = \"End game \\n10:00 PM EST\", game_zone = \"Non-game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  theme(panel.spacing = unit(1, \"lines\"))"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-28-inclusion",
    "href": "charts/2025-03-31_challenge/index.html#day-28-inclusion",
    "title": "day 0 challenge",
    "section": "Day 28 inclusion",
    "text": "Day 28 inclusion\nNZ wellbeing stats\n\nmy plotr code\n\n\n\n\n\n\nage_use %&gt;%\n  ggplot(aes(x = age, y = estimate, fill = response)) +\n  geom_col() +\n  scale_fill_manual(values = palette1) +\n  easy_add_legend_title(\"Frequency\") +\n  labs(y = \"Percent of respondents\", x = \"Age group\", , \n       title = \"Digital inclusion: Wellbeing NZ 2023\", \n       subtitle = \"Internet use\", caption = \"Data from StatsNZ\") +\n  theme_minimal() +\n  easy_remove_gridlines(axis = \"y\")"
  },
  {
    "objectID": "charts/2025-03-31_challenge/index.html#day-30-finale",
    "href": "charts/2025-03-31_challenge/index.html#day-30-finale",
    "title": "day 0 challenge",
    "section": "Day 30 finale",
    "text": "Day 30 finale\n\nmy plotr code\n\n\n\n\n\n\nchart_time %&gt;%\n  ggplot(aes(x = date, y = time, colour = challenge_week)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  scale_colour_manual(values = get_pal(\"Kereru\")) +\n  labs(x = \"Date\", y = \"Hours\", title = \"30 Day Chart Challenge April 2025\", subtitle = \"Daily time spent making plots\") +\n  scale_y_time(labels = c(0,1,2,3)) +\n  easy_remove_legend_title()"
  },
  {
    "objectID": "charts/2025-04-07_outliers/index.html",
    "href": "charts/2025-04-07_outliers/index.html",
    "title": "day 7 outliers",
    "section": "",
    "text": "The plots at Our World in Data unpacking changes in sex ratio at birth across the world uncover some interesting outliers. Across the world it is typical for there to be slightly more boys born than girls. The sex ratio at conception is equal but across pregnancy, the risk of miscarriage is slightly higher for female than male fetuses, resulting in an average of 105 male to 100 female live births. But countries like China, India, and South Korea have much higher than average sex ratios.\nI am going to reproduce this plot, adding the US, UK and Australia in for comparison.\n\n\n\nload packages\n\nlibrary(tidyverse)\nlibrary(owidapi)\nlibrary(scales)\nlibrary(ggeasy)\nlibrary(janitor)\nlibrary(feathers)\n\n\nread the data\nHere I am reading in the data, cleaning names and renaming variables.\n\nratio &lt;- read_csv(\"https://ourworldindata.org/grapher/sex-ratio-at-birth.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;% \n  clean_names() %&gt;%\n  rename(country = entity, ratio = sex_ratio_sex_all_age_0_variant_estimates)\n\n\n\nclean it up\nI am filtering the data to include only China, India, and South Korea, as well as Australia, United States, and United Kingdom for comparison.\n\ncountries &lt;- c(\"China\", \"India\", \"South Korea\", \"Australia\", \"United States\", \"United Kingdom\")\n\nratio4 &lt;- ratio %&gt;%\n  filter(country %in% countries)\n\n\n\nplot\n\nratio4 %&gt;%\n  ggplot(aes(x = year, y = ratio, colour = country)) +\n  geom_point() +\n  geom_line() \n\n\n\n\n\n\n\n\nBasic plot check! Things I would like to change…\n\nbackground theme and axis labels\nhorizontal line at 100\ncolour palette\ntitles and captions\n\n\n\ntheme, hline, and labels\n\nratio4 %&gt;%\n  ggplot(aes(x = year, y = ratio, colour = country)) +\n  geom_point() +\n  geom_line() +\n  theme_classic() +\n  scale_y_continuous(expand = c(0,0), limits = c(95, 120)) +\n  scale_x_continuous(breaks = seq(1950, 2020, 10)) +\n  geom_hline(yintercept = 100, linetype = 2) +\n  geom_text(data = data.frame(x = 2005, y = 100.8, label = \"Equal numbers of newborn boys and girls\"), mapping = aes(x = x, y = y, label = label), size = 3, inherit.aes = FALSE) +\n  labs(y = \"Sex ratio\", x = \"Year\")\n\n\n\n\n\n\n\n\n\n\ncolours, titles, and captions\nPracticing using a new palette package for this one; this time Aussie birds from the feathers package.\n\nratio4 %&gt;%\n  ggplot(aes(x = year, y = ratio, colour = country)) +\n  geom_point() +\n  geom_line() +\n  theme_classic() +\nscale_colour_manual(values = get_pal(\"eastern_rosella\")) +\n  scale_y_continuous(expand = c(0,0), limits = c(98, 120)) +\n  scale_x_continuous(breaks = seq(1950, 2020, 10)) +\n  geom_hline(yintercept = 100, linetype = 2) +\n  geom_text(data = data.frame(x = 2005, y = 100.8, label = \"Equal numbers of newborn boys and girls\"), mapping = aes(x = x, y = y, label = label), size = 3, inherit.aes = FALSE) +\n  labs(y = \"Sex ratio\", x = \"Year\", \n       title = \"Sex ratio at birth, 1950 to 2023\", \n       subtitle = \"The sex ratio at birth is measured as the number of newborn \\nboys for every 100 newborn girls. Higher values indicate a much \\nhigher number of newborn boys than girls.\", \n       caption = \"Data source: UN, World Population Prospects (2024). \\nNote: It's considered that 105 is the biologically expected sex ratio at birth.\")"
  },
  {
    "objectID": "charts/2025-04-13_clusters/index.html",
    "href": "charts/2025-04-13_clusters/index.html",
    "title": "day 13_clusters",
    "section": "",
    "text": "Today I am again departing from Our World in Data to play around with something I know nothing about: k-means clustering. This animation by Allison Horst from the tidymodels documentation is super cute and explains what k-means clustering does.\n\n\n\nArt by Allison Horst\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nlibrary(factoextra)\nlibrary(janitor)\nlibrary(gt)\n\nset.seed(123)\n\nhttps://www.tidymodels.org/learn/statistics/k-means/\nThe penguins data from the palmerpenguins package seemed like a good place to start.\n\n#read the dataset\npenguins &lt;- penguins %&gt;%\n  na.omit()\n\n# pull just species for later\nspecies &lt;- penguins %&gt;%\n  select(species)\n\n\nBody mass x bill length\nI am interested in whether we can cluster the penguins using information about their body mass and bill length. Here I am selecting just those variables, scaling the data and then using the kmeans function to compute 3 clusters. The output shows which cluster the model has assigned each data point to and tells us that this cluster arrangement accounts for 73% of the variance.\n\n# select a subset of the variables\nbodymass_bill &lt;- penguins %&gt;%\n  select(body_mass_g, bill_length_mm) \n\n# scale the data\nbodymass_bill_scaled &lt;- scale(bodymass_bill) \n\n# compute 3 clusters\nkm_bodymass_bill &lt;- kmeans(bodymass_bill_scaled, centers = 3)\n\n\nkm_bodymass_bill\n\nK-means clustering with 3 clusters of sizes 106, 148, 79\n\nCluster means:\n  body_mass_g bill_length_mm\n1   1.2331389      0.7262020\n2  -0.6697839     -0.9559782\n3  -0.3998065      0.8165489\n\nClustering vector:\n  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [38] 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2\n [75] 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 1\n[149] 3 1 1 1 1 1 3 1 2 1 1 1 3 1 2 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1\n[186] 2 1 3 1 1 1 3 1 1 1 1 1 3 1 1 1 3 1 3 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1\n[260] 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 2 3 3 3 3 3 3 3 2\n[297] 3 2 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 2 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1] 62.12498 74.59241 43.24261\n (between_SS / total_SS =  72.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\nplot\nTo visualise the clusters I am using the fviz_cluster() function from the factoextra package.\n\nfviz_cluster(km_bodymass_bill, data = bodymass_bill, \n             show.clust.cent = FALSE, label=NA) +\n  theme_bw() +\n  labs(title = \"Cluster plot k-means penguin classification\")\n\n\n\n\n\n\n\n\nWell that was almost too easy. Those clusters look nice but I know that the model isn’t 100% accurate. Which points have been misclassified?\nHere I am using augment() from tidymodels to add the cluster information to the original data, and then joining the species vector onto that dataframe, so that I end up with a joined dataframe that includes information about species, body mass, bill length and the cluster values. Then I mutate a new variable that codes whether each data point was correctly classified or not.\n\n# use augment to add clusters to original data\nclusters &lt;- augment(km_bodymass_bill, bodymass_bill)\n\n# add species column to clusters\njoined &lt;- bind_cols(species, clusters) %&gt;%\n  rename(cluster = .cluster) %&gt;%\n  mutate(correct = case_when(species == \"Adelie\" & cluster == 2 ~ \"TRUE\", \n                             species == \"Gentoo\" & cluster == 1 ~ \"TRUE\", \n                             species == \"Chinstrap\" & cluster == 3 ~ \"TRUE\", \n                             TRUE ~ \"FALSE\"))\n\nThen I am using tabyl from janitor to count the misclassifications by species and plotting by species and classification accuracy.\nThis shows that most of the Adelie and Chinstrap penguins are grouped into the same cluster but some of the smaller Gentoo penguins seem to have been misclassified.\n\njoined %&gt;%\n  tabyl(species,correct) %&gt;%\n  gt()\n\n\n\n\n\n\n\nspecies\nFALSE\nTRUE\n\n\n\n\nAdelie\n7\n139\n\n\nChinstrap\n7\n61\n\n\nGentoo\n17\n102\n\n\n\n\n\n\njoined %&gt;%\n  ggplot(aes(x = body_mass_g, y = bill_length_mm, colour = species, shape = correct)) +\n  geom_point(size = 2) +\n  theme_bw() +\n  scale_shape_manual(values=c(4, 1))"
  },
  {
    "objectID": "charts/2025-04-25_risk/index.html",
    "href": "charts/2025-04-25_risk/index.html",
    "title": "day 25 measles risk",
    "section": "",
    "text": "For the Day 25 prompt risk, I am going to try my hand at maps and reproduce the CDC plots relating to measles vaccination and cases in the US.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(usmap)\nlibrary(janitor)\n\nvax &lt;- read_csv(here(\"charts/2025-04-25_risk/kindy_vax _rates.csv\")) %&gt;%\n  mutate(year = str_sub(school_year, 1,4)) %&gt;%\n  select(state = geography, year, estimate_pct, categories) %&gt;%\n  mutate(estimate_pct = parse_number(estimate_pct)) \n\ncases2425 &lt;- read_csv(here(\"charts/2025-04-25_risk/cases20242025.csv\")) %&gt;%\n  select(state = geography, year, cases_calendar_year)\n\nvax$year &lt;- as.numeric(vax$year)\n\nglimpse(vax)\n\nRows: 769\nColumns: 4\n$ state        &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\",…\n$ year         &lt;dbl&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 202…\n$ estimate_pct &lt;dbl&gt; 93.8, 84.3, 89.3, 92.5, 96.2, 88.3, 97.7, 93.8, 92.0, 88.…\n$ categories   &lt;chr&gt; \"90-94.9%\", \"Less than 90%\", \"Less than 90%\", \"90-94.9%\",…\n\n\n\nvaccination rates\nIt looks like there is data for the school year going back to 2009- maybe that would be cool to animate later but for the moment I am interested in the most recent school year.\n\nvax2324 &lt;- vax %&gt;%\n  filter(year == \"2023\") \n\nThe dataset has vax rates as both percent estimate and categories. First I am going to try colouring by the numeric percent values. I would like the lowest to show up red.\n\nmin(vax2324$estimate_pct)\n\n[1] 79.6\n\nmax(vax2324$estimate_pct)\n\n[1] 98.3\n\nplot_usmap(data = vax2324, values = \"estimate_pct\", labels=FALSE) +\n  scale_fill_continuous(low = \"red\", high = \"white\", \n                         name = \"percent\", \n                        limits = c(75,100)) +\n  labs(title = \"Percent of kindergarten children vaccinated for measles by state\", \n       subtitle = \"School year 2023-24\")\n\n\n\n\n\n\n\n\nWhat would it look like if we used the categories instead?\n\nvax2324$categories &lt;- fct_relevel(vax2324$categories, c(\"95%+\", \"90-94.9%\", \"Less than 90%\"))\n\nlevels(vax2324$categories)\n\n[1] \"95%+\"          \"90-94.9%\"      \"Less than 90%\"\n\nplot_usmap(data = vax2324, values = \"categories\", labels=FALSE) +\n  scale_fill_manual(values = c(\"#ffcccc\", \"#ff7380\", \"#ff1933\"))  +\n   labs(title = \"Percent of kindergarten children vaccinated for measles by state\", \n       subtitle = \"School year 2023-24\") +\n  theme(\n    legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nmeasles cases\n\nplot_usmap(data = cases2425, values = \"cases_calendar_year\", labels=FALSE) +\n  scale_fill_continuous(low = \"white\", high = \"red\", \n                         name = \"cases\", \n                        limits = c(0,600)) +\n  labs(title = \"USA Measles cases by state\", subtitle = \"Data as of April 17, 2025\", caption = \"Data from CDC \\nhttps://www.cdc.gov/measles/data-research/index.html\") +\n  facet_wrap(~ year) +\n  theme(\n    legend.position = \"bottom\",\n    strip.background = element_blank(),\n    strip.text = element_text(color = \"black\", size = 10)\n  )"
  },
  {
    "objectID": "charts/2025-04-24_who/index.html",
    "href": "charts/2025-04-24_who/index.html",
    "title": "day 24 WHO",
    "section": "",
    "text": "The prompt for Day 24 is the World Health Organisation. I spent some time exploring the WHO website for a chart that uses a geom that I hadn’t tried before and came up with this one from the World Health Statistics 2024 report.\n\nI found a table of the data in the Global Hepatitis Report 2024 and used this Rscript to get it into csv format.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(treemapify)\nlibrary(ggeasy)\nlibrary(patchwork)\nlibrary(ggtext)\n\nhep &lt;- read_csv(here(\"charts\", \"2025-04-24_who\", \"hepBC.csv\"))\n\nglimpse(hep)\n\nRows: 6\nColumns: 5\n$ WHO_region  &lt;chr&gt; \"African Region\", \"Region of the Americas\", \"South-East As…\n$ new_hepB    &lt;dbl&gt; 771000, 8000, 266000, 18000, 86000, 83000\n$ new_hepC    &lt;dbl&gt; 172000, 176000, 225000, 126000, 183000, 98000\n$ deaths_hepB &lt;dbl&gt; 272000, 20000, 218000, 32000, 41000, 518000\n$ deaths_hebC &lt;dbl&gt; 35000, 38000, 42000, 21000, 65000, 43000\n\n\n\nwhat kind of plot?\nNow I don’t even know what this kind of chart is called, let alone how to make it with ggplot, so I asked claude.ai for some suggestions.\n\n\n\nclaude’s code\nI was surprised that the code that claude wrote ran without throwing an error and made a passable treemap plot. It is actually impressive that the LLM pulled the data and colour palette from the png file I gave it. Of course, if I want to learn about how treemaps work, I should probably start from scratch.\n\nsource(here::here(\"charts\", \"2025-04-24_who\", \"claudes_code.R\"))\n\nfinal_plot\n\n\n\n\n\n\n\n\n\n\nmy code\n\nhep %&gt;% \n  ggplot(aes(area = new_hepB, fill = WHO_region)) +\n  geom_treemap()\n\n\n\n\n\n\n\n\nBasic plot check! Things I would like to change…\n\ncolour scheme\nlabels on the boxes\ntitles, captions\nlayout (how do I get the large African Region box to appear on the top, rather than left)\n\n\ncolours\nIt is interesting that the colours that claude used in his code are close but not exactly the same as the ones I pulled from the plot using the ColorZilla tool.\n\nclaudes_colors &lt;- c(\n  \"African Region\" = \"#5EB3D5\", \"South-East Asia Region\" = \"#A9D18E\",\n  \"Eastern Mediterranean Region\" = \"#F6B18F\", \"Western Pacific Region\" = \"#FFD966\",\n  \"European Region\" = \"#C293CE\", \"Region of the Americas\" = \"#A593C7\"\n)\n\npalette &lt;- c(\"African Region\" =\"#69C1E6\",  \"South-East Asia Region\" = \"#B5D47C\", \n                 \"Eastern Mediterranean Region\" = \"#F3A078\",  \n               \"Western Pacific Region\" =  \"#FACF70\",\n                 \"European Region\" =\"#CC71B2\", \"Region of the Americas\" =\"#A687B6\")\n\nOK here I have sorted the colour palette and added I fine white line between the boxes using colour = “white”. Next labels…\n\nhep %&gt;% \n  ggplot(aes(area = new_hepB, fill = WHO_region)) +\n  geom_treemap(colour = \"white\") +\n  scale_fill_manual(values = palette)\n\n\n\n\n\n\n\n\n\n\nlabels\n\nhep %&gt;% \n  ggplot(aes(area = new_hepB, fill = WHO_region, \n             label = paste(WHO_region, new_hepB, sep = \"\\n\"))) +\n  geom_treemap(colour = \"white\") +\n  scale_fill_manual(values = palette) +\n  geom_treemap_text(colour = \"black\",\n                    place = \"topleft\",\n                    size = 5, \n                    grow = FALSE) + # option from ggfittext to NOT make font fit box\n  easy_remove_legend()\n\n\n\n\n\n\n\n\n\n\nlayout\n\nnote it doesn’t seem possible to control the layout of the boxes using the treemapify package; I played with the layout argument which has “squarified” as the default but allows you to specific layout = “srow” which forces the tile placement to begin with a row… it didn’t seem to do anything.\n\n\nhep %&gt;% \n  ggplot(aes(area = new_hepB, fill = WHO_region, layout = \"srow\",\n             label = paste(WHO_region, new_hepB, sep = \"\\n\"))) +\n  geom_treemap(colour = \"white\") +\n  scale_fill_manual(values = palette) +\n  geom_treemap_text(colour = \"black\",\n                    place = \"topleft\",\n                    size = 5, \n                    grow = FALSE) + # option from ggfittext to NOT make font fit box\n  easy_remove_legend()\n\n\n\n\n\n\n\n\n\n\ntitles / captions\nI am going to make an equivalent HepC plot and then paste them together before adding chart titles and captions. Using the layout = “fixed” option allow a side by side comparison of the HepB and HepC plots\n\nhepB &lt;- hep %&gt;% \n  ggplot(aes(area = new_hepB, fill = WHO_region, layout = \"fixed\",\n             label = paste(WHO_region, new_hepB, sep = \"\\n\"))) +\n  geom_treemap(colour = \"white\") +\n  scale_fill_manual(values = palette) +\n  geom_treemap_text(colour = \"black\",\n                    place = \"topleft\",\n                    size = 5, \n                    grow = FALSE) + # option from ggfittext to NOT make font fit box\n  easy_remove_legend() +\n  labs(title = \"Figure 2.4 Number of new hepatitis B and hepatitis C infections, by WHO region, 2022\", subtitle = \"Hepatitis B\") +\n  theme(plot.subtitle = element_text(hjust = 0.5, size = 5), \n        plot.title = element_text(size = 10)) \n\n\n\n hepC &lt;- hep %&gt;% \n  ggplot(aes(area = new_hepC, fill = WHO_region, layout = \"fixed\",\n             label = paste(WHO_region, new_hepC, sep = \"\\n\"))) +\n  geom_treemap(colour = \"white\") +\n  scale_fill_manual(values = palette) +\n  geom_treemap_text(colour = \"black\",\n                    place = \"topleft\",\n                    size = 5, \n                    grow = FALSE) + # option from ggfittext to NOT make font fit box\n  easy_remove_legend() +\n  labs(subtitle = \"Hepatitis C\") +\n   theme(plot.subtitle = element_text(hjust = 0.5, size = 5))\n\n \n hepB  + hepC \n\n\n\n\n\n\n\n\nAnd lastly, claude helped get the yellow/bold formatting for the title. The ggtext package is required to allow for element_markdown()\n\nhepB &lt;- hep %&gt;% \n  ggplot(aes(area = new_hepB, fill = WHO_region, layout = \"fixed\",\n             label = paste(WHO_region, new_hepB, sep = \"\\n\"))) +\n  geom_treemap(colour = \"white\") +\n  scale_fill_manual(values = palette) +\n  geom_treemap_text(colour = \"black\",\n                    place = \"topleft\",\n                    size = 5, \n                    grow = FALSE) + # option from ggfittext to NOT make font fit box\n  easy_remove_legend() +\nlabs(title = \"&lt;span style='color:#F4BD4B;'&gt;Figure 2.4&lt;/span&gt;&lt;span&gt; Number of new hepatitis B and hepatitis C infections, by WHO region, 2022&lt;/span&gt;\", \n     subtitle = \"Hepatitis B\") +\n  theme(plot.subtitle = element_text(hjust = 0.5, size = 5), \n        plot.title = element_markdown(hjust = 0, size = 8, face = \"bold\"))\n\n hepC &lt;- hep %&gt;% \n  ggplot(aes(area = new_hepC, fill = WHO_region, layout = \"fixed\",\n             label = paste(WHO_region, new_hepC, sep = \"\\n\"))) +\n  geom_treemap(colour = \"white\") +\n  scale_fill_manual(values = palette) +\n  geom_treemap_text(colour = \"black\",\n                    place = \"topleft\",\n                    size = 5, \n                    grow = FALSE) + # option from ggfittext to NOT make font fit box\n  easy_remove_legend() +\n  labs(subtitle = \"Hepatitis C\") +\n   theme(plot.subtitle = element_text(hjust = 0.5, size = 5))\n\n \n hepB  + hepC"
  },
  {
    "objectID": "charts/2025-04-28_inclusion/index.html",
    "href": "charts/2025-04-28_inclusion/index.html",
    "title": "day 28 inclusion",
    "section": "",
    "text": "Stats NZ recently released an update to their Wellbeing 2023 dataset that included variables related to digital inclusion. I downloaded the raw dataset from the site and used this script to do a bit of cleaning before reading the data into RStudio and plotting how internet use and satisfaction differs across the lifespan below.\nI found it particularly interesting that more than half of kiwis 75+ use the internet daily, but older people are less likely to say they are very satisfied with the internet than are younger people.\n\nget data\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggeasy)\n\n\nage_use &lt;- read_csv(here(\"charts/2025-04-28_inclusion/age_group.csv\")) %&gt;%\n  filter(category == \"Internet use\") %&gt;%\n   mutate(response = factor(response, levels = c(\"Many times a day\",  \"Once or twice a day\",  \"A few times a week or less\", \"Never use it\"))) %&gt;%\n   mutate(age = factor(age, levels = c(\"15-24\", \"25-34\",\"35-44\", \"45-54\",\"55-64\",\"65-74\",\"75+\"))) %&gt;%\n  mutate(estimate = as.numeric(estimate)) %&gt;%\n  select(category, age, response, estimate)\n\n      \nage_sat &lt;- read_csv(here(\"charts/2025-04-28_inclusion/age_group.csv\")) %&gt;%\n  filter(category == \"Internet satisfaction\") %&gt;%\n   mutate(response = factor(response, levels = c(\"Very satisfied\",  \"Satisfied\", \n                                           \"No feeling either way\", \"Dissatisfied/very dissatisfied\n\"))) %&gt;%\n   mutate(age = factor(age, levels = c(\"15-24\", \"25-34\",\"35-44\", \"45-54\",\"55-64\",\"65-74\",\"75+\"))) %&gt;%\n  mutate(estimate = as.numeric(estimate)) %&gt;%\n  select(category, age, response, estimate)\n\n\n\n\nplot\n\nage_use %&gt;%\n  ggplot(aes(x = age, y = estimate, fill = response)) +\n  geom_col() \n\n\n\n\n\n\n\n\nBasic plot check! Things I would like to change…\n\ncolour scheme\ntheme\naxis and legend labels\ntitle/caption\nmake same style plot for internet satisfaction\ncombine plot using panel tabset\n\n\n\ncolours/themes etc\n\npalette1 &lt;- c (\"#A092B7\", \"#7d9fc2\", \"#C582B2\", \"#51806a\") # Kereru from manu package\n\npalette2 &lt;- c(\"#85BEDC\",  \"#647588\" , \"#CCBBCD\") # Korora from manu package\n\n\nage_use %&gt;%\n  ggplot(aes(x = age, y = estimate, fill = response)) +\n  geom_col() +\n  scale_fill_manual(values = palette1) +\n  easy_add_legend_title(\"Frequency\") +\n  labs(y = \"Percent of respondents\", x = \"Age group\", , \n       title = \"Digital inclusion: Wellbeing NZ 2023\", \n       subtitle = \"Internet use\", caption = \"Data from StatsNZ\") +\n  theme_minimal() +\n  easy_remove_gridlines(axis = \"y\") \n\n\n\n\n\n\n\n\n\n\nsame plot for satisfaction\n\nage_sat %&gt;%\n  ggplot(aes(x = age, y = estimate, fill = response)) +\n  geom_col() +\n  scale_fill_manual(values = palette2) +\n  easy_add_legend_title(\"Satisfaction\") +\n  labs(y = \"Percent of respondents\", x = \"Age group\", \n        title = \"Digital inclusion: Wellbeing NZ 2023\", \n       subtitle = \"Internet satisfaction\", caption = \"Data from StatsNZ\") +\n  theme_minimal() +\n  easy_remove_gridlines(axis = \"y\") \n\n\n\n\n\n\n\n\n\n\ncombine plots\n\nDigital inclusion: Wellbeing NZ 2023\n\nInternet use by age groupInternet satisfaction by age group"
  },
  {
    "objectID": "charts/2025-04-03_circular/index.html",
    "href": "charts/2025-04-03_circular/index.html",
    "title": "day 3 circular",
    "section": "",
    "text": "library(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(ggeasy)\nlibrary(ggannotate)\nlibrary(plotly)\n\nThis plot illustrates changing representation of women in government leadership. It is great that in 2024 fewer than 2% of governments have no women in leadership, but it is not amazing that fewer than 2% have 50% or more. I am wondering if I can look at this pattern across different areas of the world by making circular graphs in ggplot.\n\n\n\n\nI am mostly interested in the current state of affairs so I read in the data from the owidapi, filter the data to only include 2024 and rename variables to make them more friendly.\n\nshare &lt;- read_csv(\"https://ourworldindata.org/grapher/countries-by-share-of-women-in-parliament.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names()\n\n\n\nshare2024 &lt;- share %&gt;% \n  filter(year == 2024) %&gt;%\n  select(-code) %&gt;%\n  rename(number_0pct = num_countries_wom_parl_category_0pct_women, \n         number_0_10pct = num_countries_wom_parl_category_0_10pct_women,\n         number_10_20pct = num_countries_wom_parl_category_10_20pct_women, \n           number_20_30pct = num_countries_wom_parl_category_20_30pct_women, \n           number_30_40pct = num_countries_wom_parl_category_30_40pct_women, \n           number_40_50pct = num_countries_wom_parl_category_40_50pct_women,\n           number_50pluspct = num_countries_wom_parl_category_50pctplus_women) \n\nglimpse(share2024)\n\nRows: 7\nColumns: 9\n$ entity           &lt;chr&gt; \"Africa\", \"Asia\", \"Europe\", \"North America\", \"Oceania…\n$ year             &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024\n$ number_0pct      &lt;dbl&gt; 0, 2, 0, 0, 0, 0, 2\n$ number_0_10pct   &lt;dbl&gt; 4, 7, 0, 0, 4, 0, 15\n$ number_10_20pct  &lt;dbl&gt; 17, 16, 5, 1, 0, 1, 40\n$ number_20_30pct  &lt;dbl&gt; 15, 11, 11, 6, 0, 3, 46\n$ number_30_40pct  &lt;dbl&gt; 10, 5, 15, 3, 1, 5, 39\n$ number_40_50pct  &lt;dbl&gt; 5, 2, 10, 2, 1, 3, 23\n$ number_50pluspct &lt;dbl&gt; 1, 0, 0, 2, 0, 0, 3\n\n\n\n\n\nI want to plot data for the World and each region separately so create separate dataframes here using filter(). I also want to make the data long using pivot_longer, mutate new variables that represent the number of countries as percent, improve the labels on the percent categories and make them a factor so that they order correctly in my plot.\n\n# create world dataframe\n\nshare2024_world &lt;- share2024 %&gt;%\n  filter(entity == \"World\") %&gt;%\n  pivot_longer(names_to = \"category\", values_to = \"number\", number_0pct:number_50pluspct) %&gt;%\n    mutate(total = sum(number), percent = number/total) %&gt;%\n           mutate(percent_women = case_when(category == \"number_0pct\" ~ \"no women\", \n                           category == \"number_0_10pct\" ~ \"less than 10%\",\n                           category == \"number_10_20pct\" ~ \"10-20%\", \n                           category == \"number_20_30pct\" ~ \"20-30%\", \n                           category == \"number_30_40pct\" ~ \"30-40%\", \n                           category == \"number_40_50pct\" ~ \"40-50%\",\n                           category == \"number_50pluspct\" ~ \"50+%\")) %&gt;%\n    mutate(percent_women = fct_relevel(percent_women, \n                            c(\"no women\" , \"less than 10%\" ,\"10-20%\" ,\n                              \"20-30%\" ,\"30-40%\" , \"40-50%\" ,\n                              \"50+%\"))) \n\n# create regions dataframe\n\nshare2024_regions &lt;- share2024 %&gt;%\n  filter(entity != \"World\") %&gt;%\n  pivot_longer(names_to = \"category\", values_to = \"number\", number_0pct:number_50pluspct) %&gt;%\n  group_by(entity) %&gt;%\n    mutate(total = sum(number), percent = number/total) %&gt;%\n           mutate(percent_women = case_when(category == \"number_0pct\" ~ \"no women\", \n                           category == \"number_0_10pct\" ~ \"less than 10%\",\n                           category == \"number_10_20pct\" ~ \"10-20%\", \n                           category == \"number_20_30pct\" ~ \"20-30%\", \n                           category == \"number_30_40pct\" ~ \"30-40%\", \n                           category == \"number_40_50pct\" ~ \"40-50%\",\n                           category == \"number_50pluspct\" ~ \"50+%\")) %&gt;%\n    mutate(percent_women = fct_relevel(percent_women, \n                            c(\"no women\" , \"less than 10%\" ,\"10-20%\" ,\n                              \"20-30%\" ,\"30-40%\" , \"40-50%\" ,\n                              \"50+%\"))) \n\n\n\n\n\n\nStarting with world and using inspo from the R Graph gallery here I make an ugly pie chart. There is no pie geom in ggplot, so the workaround is to make a column chart and use coord_polar() to make it circular.\n\nshare2024_world %&gt;%\n  ggplot(aes(x = \"\", y = percent, fill = percent_women)) +\n  geom_col(colour = \"white\") +\n coord_polar(\"y\", start = 0) +\n    theme_void() +\n  easy_add_legend_title(\"Percent seats\") +\n  scale_fill_manual(values = c(\"#a2559b\", \"#00847d\", \"#4b6a9c\", \"#e56e59\", \"#38aaba\", \"#6b3d8d\",  \"#986d39\" )) +\n  labs(title = \"Percent of countries by share of women in parliament in 2024, World\", \n       subtitle = \"Percent of seats in lower or single chamber of the legislature held by women\", \n       caption = \"Data source: V-Dem (2025) via OurWorldInData. \\nNote: Only countries with at least one legislative chamber included.\")\n\n\n\n\n\n\n\n\nThe more interesting question is does this pattern of representation differ across regions?\n\n\n\n\nshare2024_regions %&gt;%\n  ggplot(aes(x = \"\", y = percent, fill = percent_women)) +\n  geom_col(colour = \"white\") +\n coord_polar(\"y\", start = 0) +\n  facet_wrap(~entity) +\n    scale_fill_manual(values = c(\"#a2559b\", \"#00847d\", \"#4b6a9c\", \"#e56e59\", \"#38aaba\", \"#6b3d8d\",  \"#986d39\" )) +\n    theme_void() +\n   easy_add_legend_title(\"Percent seats\") +\nlabs(title = \"Percent of countries by share of women in parliament in 2024, Regions\", \n       subtitle = \"Percent of seats in lower or single chamber of the legislature held by women\", \n       caption = \"Data source: V-Dem (2025) via OurWorldInData. \\nNote: Only countries with at least one legislative chamber included.\")\n\n\n\n\n\n\n\n\nThere is a lot that surprises me about this plot. Oceania is not doing well; the majority of countries have less than 10% of government seats held by women. This dataset only includes the number of countries in each region that have women represented in each of the percent categories. This includes 6 Oceanic countries, but it doesn’t include information about which 6 countries are included. I am interested to dig into the country level data here.\nI was also surprised that Asia is the only region that still has 2 countries that have no women in government. Again, I am curious about which 2; Afghanistan maybe? Perhaps there are Middle East countries that do not have women in government."
  },
  {
    "objectID": "charts/2025-04-03_circular/index.html#clean-it-up",
    "href": "charts/2025-04-03_circular/index.html#clean-it-up",
    "title": "day 3 circular",
    "section": "",
    "text": "I want to plot data for the World and each region separately so create separate dataframes here using filter(). I also want to make the data long using pivot_longer, mutate new variables that represent the number of countries as percent, improve the labels on the percent categories and make them a factor so that they order correctly in my plot.\n\n# create world dataframe\n\nshare2024_world &lt;- share2024 %&gt;%\n  filter(entity == \"World\") %&gt;%\n  pivot_longer(names_to = \"category\", values_to = \"number\", number_0pct:number_50pluspct) %&gt;%\n    mutate(total = sum(number), percent = number/total) %&gt;%\n           mutate(percent_women = case_when(category == \"number_0pct\" ~ \"no women\", \n                           category == \"number_0_10pct\" ~ \"less than 10%\",\n                           category == \"number_10_20pct\" ~ \"10-20%\", \n                           category == \"number_20_30pct\" ~ \"20-30%\", \n                           category == \"number_30_40pct\" ~ \"30-40%\", \n                           category == \"number_40_50pct\" ~ \"40-50%\",\n                           category == \"number_50pluspct\" ~ \"50+%\")) %&gt;%\n    mutate(percent_women = fct_relevel(percent_women, \n                            c(\"no women\" , \"less than 10%\" ,\"10-20%\" ,\n                              \"20-30%\" ,\"30-40%\" , \"40-50%\" ,\n                              \"50+%\"))) \n\n# create regions dataframe\n\nshare2024_regions &lt;- share2024 %&gt;%\n  filter(entity != \"World\") %&gt;%\n  pivot_longer(names_to = \"category\", values_to = \"number\", number_0pct:number_50pluspct) %&gt;%\n  group_by(entity) %&gt;%\n    mutate(total = sum(number), percent = number/total) %&gt;%\n           mutate(percent_women = case_when(category == \"number_0pct\" ~ \"no women\", \n                           category == \"number_0_10pct\" ~ \"less than 10%\",\n                           category == \"number_10_20pct\" ~ \"10-20%\", \n                           category == \"number_20_30pct\" ~ \"20-30%\", \n                           category == \"number_30_40pct\" ~ \"30-40%\", \n                           category == \"number_40_50pct\" ~ \"40-50%\",\n                           category == \"number_50pluspct\" ~ \"50+%\")) %&gt;%\n    mutate(percent_women = fct_relevel(percent_women, \n                            c(\"no women\" , \"less than 10%\" ,\"10-20%\" ,\n                              \"20-30%\" ,\"30-40%\" , \"40-50%\" ,\n                              \"50+%\")))"
  },
  {
    "objectID": "charts/2025-04-03_circular/index.html#plot",
    "href": "charts/2025-04-03_circular/index.html#plot",
    "title": "day 3 circular",
    "section": "",
    "text": "Starting with world and using inspo from the R Graph gallery here I make an ugly pie chart. There is no pie geom in ggplot, so the workaround is to make a column chart and use coord_polar() to make it circular.\n\nshare2024_world %&gt;%\n  ggplot(aes(x = \"\", y = percent, fill = percent_women)) +\n  geom_col(colour = \"white\") +\n coord_polar(\"y\", start = 0) +\n    theme_void() +\n  easy_add_legend_title(\"Percent seats\") +\n  scale_fill_manual(values = c(\"#a2559b\", \"#00847d\", \"#4b6a9c\", \"#e56e59\", \"#38aaba\", \"#6b3d8d\",  \"#986d39\" )) +\n  labs(title = \"Percent of countries by share of women in parliament in 2024, World\", \n       subtitle = \"Percent of seats in lower or single chamber of the legislature held by women\", \n       caption = \"Data source: V-Dem (2025) via OurWorldInData. \\nNote: Only countries with at least one legislative chamber included.\")\n\n\n\n\n\n\n\n\nThe more interesting question is does this pattern of representation differ across regions?\n\n\n\n\nshare2024_regions %&gt;%\n  ggplot(aes(x = \"\", y = percent, fill = percent_women)) +\n  geom_col(colour = \"white\") +\n coord_polar(\"y\", start = 0) +\n  facet_wrap(~entity) +\n    scale_fill_manual(values = c(\"#a2559b\", \"#00847d\", \"#4b6a9c\", \"#e56e59\", \"#38aaba\", \"#6b3d8d\",  \"#986d39\" )) +\n    theme_void() +\n   easy_add_legend_title(\"Percent seats\") +\nlabs(title = \"Percent of countries by share of women in parliament in 2024, Regions\", \n       subtitle = \"Percent of seats in lower or single chamber of the legislature held by women\", \n       caption = \"Data source: V-Dem (2025) via OurWorldInData. \\nNote: Only countries with at least one legislative chamber included.\")\n\n\n\n\n\n\n\n\nThere is a lot that surprises me about this plot. Oceania is not doing well; the majority of countries have less than 10% of government seats held by women. This dataset only includes the number of countries in each region that have women represented in each of the percent categories. This includes 6 Oceanic countries, but it doesn’t include information about which 6 countries are included. I am interested to dig into the country level data here.\nI was also surprised that Asia is the only region that still has 2 countries that have no women in government. Again, I am curious about which 2; Afghanistan maybe? Perhaps there are Middle East countries that do not have women in government."
  },
  {
    "objectID": "charts/2025-04-30_natgeo/index.html",
    "href": "charts/2025-04-30_natgeo/index.html",
    "title": "day 30 finale",
    "section": "",
    "text": "The last day of the #30DayChartChallenge- woot! As predicted I did not manage to make a chart every day; the latter half of the challenge clashed with school holidays and a trip away, but 19/30 days is not too bad.\nI initially planned to focus on Our World in Data charts and that worked well initially, but it became more difficult to find prompt-relevant data there as the challenge went on. Once I let myself look more broadly, I had a lot of fun to making plots about everything from egg prices to hollywood star age differences to measles vax rates and Puteketeke controversies.\nFor the last of the challenge I wanted to look at my own data about how much time I spent on the challenge in April. I pulled data from my Timing app and learned some useful things about working with date and time data in the process.\nHere I am reading my timing data in to RStudio and miraculously R is recognising my dates as dates and time values as time- that should make things easier right?\n\nget the data\n\nlibrary(tidyverse) # tidyverse now includes the `lubridate` package\nlibrary(here)\nlibrary(gt)\nlibrary(plotly)\nlibrary(ggeasy)\nlibrary(Manu)\n\n\nchart_time &lt;- read_csv(here(\"charts/2025-04-30_natgeo/chart_time.csv\"))\n\nglimpse(chart_time)\n\nRows: 35\nColumns: 2\n$ date &lt;date&gt; 2025-03-27, 2025-03-28, 2025-03-29, 2025-03-30, 2025-03-31, 2025…\n$ time &lt;time&gt; 02:06:00, 02:13:00, 01:05:00, 00:00:00, 01:58:00, 02:19:00, 01:4…\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are less lucky and your dates have read in as characters, you can make them into dates using functions from the lubridate package. If you have 2025-04-01 format dates, df$date &lt;- ymd(df$date), tells R that your date variable is a date in year-month-day format.\n\n\nI am interested in how much time I spent during each week of the challenge, so here make a new variable using case_when() and the day and month values, to make another variable with challenge week values Week0 to Week4.\n\nchart_time &lt;- chart_time %&gt;%\n  mutate(day = day(date)) %&gt;%\n  mutate(month = month(date)) %&gt;%\n  mutate(challenge_week = case_when(month == 3 ~ \"Week0\", \n                                    day %in% 1:7 ~ \"Week1\", \n                                    day %in% 8:14 ~ \"Week2\", \n                                    day %in% 15:21 ~ \"Week3\", \n                                    day %in% 22:30 ~ \"Week4\"))\n                                    \n\nglimpse(chart_time)\n\nRows: 35\nColumns: 5\n$ date           &lt;date&gt; 2025-03-27, 2025-03-28, 2025-03-29, 2025-03-30, 2025-0…\n$ time           &lt;time&gt; 02:06:00, 02:13:00, 01:05:00, 00:00:00, 01:58:00, 02:1…\n$ day            &lt;int&gt; 27, 28, 29, 30, 31, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, …\n$ month          &lt;dbl&gt; 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4…\n$ challenge_week &lt;chr&gt; \"Week0\", \"Week0\", \"Week0\", \"Week0\", \"Week0\", \"Week1\", \"…\n\n\n\n\ndescriptives\n\nmonth\nHow much time did I spent across the month? Here I am summing all time values in the dataset. I was initially surprised to find that when you sum a variable in time format, the output is in seconds (156000 seconds!). It took me a little time to work out how to convert those values into meaningful units.\nInitially I tried dividing by 60 a couple of times to get minutes and hours but the values were still seconds (turns out that only works when the data is numeric). A bit of lubridate googling and I found that you can use the seconds_to_period() function will reformat values so that they display as hours, minutes, and seconds.\n\n# total time spent\nsum_total &lt;- chart_time %&gt;%\n  summarise(total_seconds = sum(time)) %&gt;%\n  mutate(total_period = seconds_to_period(total_seconds))\n\ngt::gt(sum_total)\n\n\n\n\n\n\n\ntotal_seconds\ntotal_period\n\n\n\n\n156000\n1d 19H 20M 0S\n\n\n\n\n\n\n\n\n\nweek\nHow many hours did I spend each week? On average about 8 hours per week, a few more than that in the first week when I did 7 charts in 7 days, and the week before we headed away during the school holidays so that I could schedule some posts in advance.\n\n# time per week\n\nweek_total &lt;- chart_time %&gt;%\n  group_by(challenge_week) %&gt;%\n  summarise(total_seconds = sum(time)) %&gt;%\n  mutate(total_period = seconds_to_period(total_seconds)) \n\ngt::gt(week_total)\n\n\n\n\n\n\n\nchallenge_week\ntotal_seconds\ntotal_period\n\n\n\n\nWeek0\n26520\n7H 22M 0S\n\n\nWeek1\n43380\n12H 3M 0S\n\n\nWeek2\n23580\n6H 33M 0S\n\n\nWeek3\n37560\n10H 26M 0S\n\n\nWeek4\n24960\n6H 56M 0S\n\n\n\n\n\n\n\n\n\n\nplot\n\nweek\nExpressing the time as a “period” displays nicely when put in a gt() table as above, but when I tried plotting the same data, the y axis is STILL seconds.\n\nweek_total %&gt;%\n  ggplot(aes(x = challenge_week, y = total_period)) +\n  geom_col(fill = \"#7d9fc2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nMy first instinct was to use the hours() function to pull the hours out of the total period variable, but that was unsuccessful.\n\nweek_total %&gt;%\n  ggplot(aes(x = challenge_week, y = hours(total_period))) +\n  geom_col(fill = \"#7d9fc2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSo I went back to basics and turned the time period back to numeric and worked out the minutes and hours by dividing by 60.\n\nweek_total &lt;- week_total %&gt;%\n  mutate(total_period_numeric = as.numeric(total_period)) %&gt;%\n  mutate(total_minutes = total_period_numeric / 60) %&gt;%\n  mutate(total_hours = round(total_minutes / 60))\n\nglimpse(week_total)\n\nRows: 5\nColumns: 6\n$ challenge_week       &lt;chr&gt; \"Week0\", \"Week1\", \"Week2\", \"Week3\", \"Week4\"\n$ total_seconds        &lt;drtn&gt; 26520 secs, 43380 secs, 23580 secs, 37560 secs, 2…\n$ total_period         &lt;Period&gt; 7H 22M 0S, 12H 3M 0S, 6H 33M 0S, 10H 26M 0S, 6H …\n$ total_period_numeric &lt;dbl&gt; 26520, 43380, 23580, 37560, 24960\n$ total_minutes        &lt;dbl&gt; 442, 723, 393, 626, 416\n$ total_hours          &lt;dbl&gt; 7, 12, 7, 10, 7\n\n\n\nweek_total %&gt;%\n  ggplot(aes(x = challenge_week, y = total_hours)) +\n  geom_col(fill = \"#7d9fc2\") +\n  theme_minimal() +\n  labs(x = \"Challenge week\", y = \"Hours\", title = \"30 Day Chart Challenge April 2025\", subtitle = \"Time each week spent making plots\") +\n  theme_minimal() +\n  scale_y_continuous(limits = c(0,15))\n\n\n\n\n\n\n\n\n\n\nday\n\np &lt;- chart_time %&gt;%\n  ggplot(aes(x = date, y = time, colour = challenge_week)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  scale_colour_manual(values = get_pal(\"Kereru\")) +\n  labs(x = \"Date\", y = \"Hours\", title = \"30 Day Chart Challenge April 2025\", subtitle = \"Daily time spent making plots\") +\n  scale_y_time(labels = c(0,1,2,3)) +\n  easy_remove_legend_title()\n\np\n\n\n\n\n\n\n\n\n\n\n\ninteractive\n\nggplotly(p)"
  },
  {
    "objectID": "charts/2025-04-05_ranking/index.html",
    "href": "charts/2025-04-05_ranking/index.html",
    "title": "day 5 ranking",
    "section": "",
    "text": "load packages\n\nlibrary(tidyverse)\nlibrary(Hmisc) # for %nin%\nlibrary(janitor)\nlibrary(owidapi)\nlibrary(ggbump)\nlibrary(ggeasy)\nlibrary(patchwork)\n\nThese plots from Our World in Data caught my eye this week given all that is going on in the USA with layoffs and funding cuts (R&Dspending ; R&Dresearchers).\nThey illustrate the amount of money that countries spend on research and development as a percent of GDP and the number of people that work in R&D as a function of population and GDP. The theme today is ranking, so I wonder whether I can look at where the USA ranks relative to other countries on these metrics.\n\n\n\n\n\nread in the data\nHere I am reading the data and renaming variables. I am also filtering out some regions that had appeared in the country variable.\n\n# read spend data\n\nspend &lt;- read_csv(\"https://ourworldindata.org/grapher/research-spending-gdp.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names %&gt;%\n  select(country = entity, year, rdgdp = gb_xpd_rsdv_gd_zs) \n\nremove &lt;- c(\"North America (WB)\"    , \"East Asia and Pacific (WB)\", \"High-income countries\", \"World\", \"European Union (27)\", \"Europe and Central Asia (WB)\", \"South Asia (WB)\")\n\nspend &lt;-  spend %&gt;%\n  filter(country %nin% remove)\n\n# read people data\n\npeople &lt;- read_csv(\"https://ourworldindata.org/grapher/researchers-in-rd-per-million-people-vs-gdp-pc.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names %&gt;%\n  select(country = entity, year, researchers_per_mill = sp_pop_scie_rd_p6, gdp_per_cap = ny_gdp_pcap_pp_kd) %&gt;%\n  filter(!is.na(researchers_per_mill)) \n\n\n\nget ranks\nI am most interested in the top5 ranked countries in terms of the percent of GDP they spend on R&D. I group by year and create a new variable that is the rank of the researchers as a function of gdp. I remember to ungroup() the data before arranging by year and rank.\nThen I make a dataframe with only the top 5 countries by rank for each year and filter the data to only include 2018-2021, and repeat the process for the people data.\n\n# get spend ranks\nspend_rank &lt;- spend %&gt;%\n  group_by(year) %&gt;%\n  mutate(rank = rank(-rdgdp, ties.method = \"random\")) %&gt;% \n  ungroup() %&gt;%\n  arrange(-year, rank)\n\ntop5spend &lt;- spend_rank %&gt;%\n  filter(rank &lt;=5) %&gt;%\n  filter(year &gt;= 2018) %&gt;%\n  filter(year &lt; 2022)\n\n# get people ranks\n\npeople_rank &lt;- people %&gt;%\n  group_by(year) %&gt;%\n  mutate(rank = rank(-researchers_per_mill, ties.method = \"random\")) %&gt;% \n  ungroup() %&gt;%\n  arrange(-year, rank)\n\ntop5people &lt;- people_rank %&gt;%\n  filter(rank &lt;=5) %&gt;%\n  filter(year &gt;= 2018) %&gt;%\n  filter(year &lt; 2022)\n\n\n\nbump plots\nI hadn’t really heard of a “bump” plot before I looked at the RGraphGallery for ranking data inspiration. The ggbump package makes it really easy to plot rankings in this way.\n\nspend\n\ntop5spend %&gt;%\nggplot(aes(year, rank, color = country)) +\n  geom_point(size = 5) +\n   geom_bump(size = 2, smooth = 8) +\n  geom_text(data = top5spend %&gt;% filter(year == min(year)),\n            aes(x = year - .1, label = country), size = 3.5, hjust = 1) +\n  geom_text(data = top5spend %&gt;% filter(year == max(year)),\n            aes(x = year + .1, label = country), size = 3.5, hjust = 0) +\n    scale_y_reverse() +\n  scale_x_continuous(limits = c(2017.6, 2021.5),\n                     breaks = seq(2018, 2021, 1)) +\n  theme_minimal() +\n  easy_remove_legend() +\n  easy_remove_gridlines()  \n\n\n\n\n\n\n\n\nIt is interesting that Israel and South Korea are clearly in the lead when it comes to investment in R&D as a percent of GDP. The United States only features in the top 5 since 2020.\n\n\npeople\nWhat about the R&D workforce? How many researchers are working in each of these countries?\nI expected more overlap between the number of people working the R&D per million and the GDP spend. Israel and the USA are nowhere to be seen. South Korea comes up on top, and Scandinavian countries are really highly represented.\n\ntop5people %&gt;%\nggplot(aes(year, rank, color = country)) +\n  geom_point(size = 5) +\n   geom_bump(size = 2, smooth = 8) +\n  geom_text(data = top5people %&gt;% filter(year == min(year)),\n            aes(x = year - .1, label = country), size = 3.5, hjust = 1) +\n  geom_text(data = top5people %&gt;% filter(year == max(year)),\n            aes(x = year + .1, label = country), size = 3.5, hjust = 0) +\n    scale_y_reverse() +\n  scale_x_continuous(limits = c(2017.6, 2021.5),\n                     breaks = seq(2018, 2021, 1)) +\n  theme_minimal() +\n  easy_remove_legend() +\n  easy_remove_gridlines() \n\n\n\n\n\n\n\n\nOK- adapting the code from the ggbump package vignette gets me most of the way there… but there are a couple things I want to change.\n\nthe colour palette (I need to make the colour associated with each country consistent across this and the people plot below)\ntitles, axis labels\n\n\n\n\ncolours\nI would like to present these two plots together, so need to make the colours associated with each country distinct, but the same across plots (i.e. South Korea and Sweden need to be associated with the same colour in both plots).\nAfter much fiddling with scale_colour_manual(), I realised that a colour palette was probably the best way to go. The manu package has lovely palettes inspired by New Zealand birds.\nI need one palette that has 8 colours for the spend plot and another that has 6 colours for the people plot. I decided to use the 6 colour kereru palette for the people plot and to add a couple of those colours to the takehe palette (plus an extra blue), to make up the 8 colours needed for the spend plot.\nTo ensure that the order of the countries / colours lined up, I made country a factor and checked that the levels of that factor matched the palette.\n\ntop5spend$country &lt;- as.factor(top5spend$country)\n\nlevels(top5spend$country)\n\n[1] \"Belgium\"       \"Germany\"       \"Israel\"        \"Japan\"        \n[5] \"South Korea\"   \"Sweden\"        \"Switzerland\"   \"United States\"\n\npalette_takehe_plus &lt;- c(\"#DD3C51\", \"#313657\", \"#1F6683\", \"#6C90B9\", \"#4d5f8e\",\"#A092B7\",\"#D1C7B5\", \"#85BEDC\")\n\n\ntop5people$country &lt;- as.factor(top5people$country)\n\nlevels(top5people$country)\n\n[1] \"Denmark\"     \"Finland\"     \"Norway\"      \"Singapore\"   \"South Korea\"\n[6] \"Sweden\"     \n\npalette_kereru &lt;- c(\"#325756\", \"#7d9fc2\", \"#C582B2\", \"#51806a\", \"#4d5f8e\", \"#A092B7\")\n\n\ns &lt;- top5spend %&gt;%\nggplot(aes(year, rank, color = country)) +\n  geom_point(size = 5) +\n   geom_bump(size = 2, smooth = 8) +\n  scale_colour_manual(values = palette_takehe_plus) +\n  geom_text(data = top5spend %&gt;% filter(year == min(year)),\n            aes(x = year - .1, label = country), size = 3.5, hjust = 1) +\n  geom_text(data = top5spend %&gt;% filter(year == max(year)),\n            aes(x = year + .1, label = country), size = 3.5, hjust = 0) +\n    scale_y_reverse() +\n  scale_x_continuous(limits = c(2017.6, 2021.5),\n                     breaks = seq(2018, 2021, 1)) +\n  theme_minimal() +\n  easy_remove_legend() +\n  easy_remove_gridlines() +\n  labs(title = \"R&D spending percent of GDP: Top 5 countries\", \n       subtitle = \"includes basic research, applied research, and experimental development\", \n       y = \"Ranking\", x = \"Year\") +\n  geom_text(data = data.frame(x = 2019, y = 4.7, label = \"Switzerland\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 3.7, colour = \"#D1C7B5\", inherit.aes = FALSE)\n\ns\n\n\n\n\n\n\n\np &lt;- top5people %&gt;%\nggplot(aes(year, rank, color = country)) +\n  geom_point(size = 5) +\n   geom_bump(size = 2, smooth = 8) +\n   scale_colour_manual(values = palette_kereru) +\n  geom_text(data = top5people %&gt;% filter(year == min(year)),\n            aes(x = year - .1, label = country), size = 3.5, hjust = 1) +\n  geom_text(data = top5people %&gt;% filter(year == max(year)),\n            aes(x = year + .1, label = country), size = 3.5, hjust = 0) +\n    scale_y_reverse() +\n  scale_x_continuous(limits = c(2017.6, 2021.5),\n                     breaks = seq(2018, 2021, 1)) +\n  theme_minimal() +\n  easy_remove_legend() +\n  easy_remove_gridlines() +\n  labs(title = \"R&D researchers per million people: Top 5 countries\", \n       subtitle = \"Professionals conceiving or creating new knowledge, products, processes, methods, or systems\", \n       y = \"Ranking\", x = \"Year\") \n\np\n\n\n\n\n\n\n\n\n\n\npatchwork\nThe patchwork package makes it super easy to combine different ggplots into a single output. Here I am using the / operator to combine plots vertically (if you wanted them side by side you would use the + operator).\n\ns / p"
  },
  {
    "objectID": "charts/2025-04-04_bigsmall/index.html",
    "href": "charts/2025-04-04_bigsmall/index.html",
    "title": "day 4 big small",
    "section": "",
    "text": "This plot illustrates the disproportionate amount of time that women across the world spend doing unpaid care and domestic work relative to men. Only Belgium comes anywhere near close to equal time spent by men and women.\nI am particularly interested in whether there has been any shift in this ratio in the last 20 years, so my goal is to reproduce this plot in ggplot using the most recent data for each country and then look at change over time.\n\n\n\nload packages\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggeasy)\nlibrary(ggrepel)\nlibrary(plotly)\nlibrary(janitor)\n\n\nread in the data\n\ndom &lt;- read_csv(\"https://ourworldindata.org/grapher/time-spend-in-domestic-work-female-vs-male.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names()\n\nRows: 59211 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Entity, Code, owid_region\ndbl (4): Year, _5_4_1__sl_dom_tspd__15_years_old_and_over__all_areas__female...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nclean it up\nHere I am renaming some unweildy variable names, filtering the data to include only years since 2000, and getting rid of NA values. I then filter that dataframe to only include the most recent data year for each country.\n\ndom_clean &lt;- dom %&gt;%\n  rename(country = \"entity\", \n         female = \"x5_4_1_sl_dom_tspd_15_years_old_and_over_all_areas_female\", \n         male = \"x5_4_1_sl_dom_tspd_15_years_old_and_over_all_areas_male\") %&gt;%\n  filter(year &gt; 2000) %&gt;%\n  filter(!is.na(female)) \n\n\ndom_recent &lt;- dom_clean %&gt;%\n  group_by(country) %&gt;%\n  filter(year == max(year))\n\n\n\nplot\n\ndom_recent %&gt;%\n  ggplot(aes(x = male, y = female)) +\n  geom_point() \n\n\n\n\n\n\n\n\nBasic plot check! Things I need to change…\n\nwhite background with dotted gridlines\ncoloured points\nfix scale on axis and % labels\nadd diagonal lines\nadd labels to diagonal lines and points\nadd titles, subtitles, x and y\n\n\n\ntheme, axis, diagonal lines\ntheme_classic does a good job of getting rid of the grey background and adding dark axis lines. I add gridlines and with theme(panel.grid.major() and fix the scale on each axis by setting breaks and limits. I colour the points by country here (because the dataset is missing region information) and use easy_remove_legend() from ggeasy to hide the legend. I add diagonal lines representing equal, twice, and four times using geom_abline() with different slopes.\n\ndom_recent %&gt;%\n  ggplot(aes(x = male, y = female, colour = country)) +\n  geom_point(size = 2) +\n  theme_classic() +\n  easy_remove_legend() +\n  theme(panel.grid.major = element_line(color = \"lightgray\", linetype = 2)) +\n  scale_y_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                     expand = c(0,0), breaks = seq(0,25, 5), limits = c(0,30)) +\n   scale_x_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                      expand = c(0,0), breaks = seq(0,12, 2), limits = c(0,14)) +\n  geom_abline(intercept = 0, slope = 1, colour = \"gray\", linetype = 2) +\n  geom_abline(intercept = 0, slope = 2, colour = \"gray\", linetype = 2) +\n  geom_abline(intercept = 0, slope = 4, colour = \"gray\", linetype = 2) \n\n\n\n\n\n\n\n\n\n\nannonations\nThe ggannotate package is super useful here. I didn’t realise that there was an angle option in the Shiny app popup that helps you place your annotation. Here I have used ggannotate to place labels on the geom_abline() lines, and geom_text_repel() to label each point with a country label.\n\n\n\n\n\n\nNote\n\n\n\nIt is SUPER fiddly to get the x, y position and angle of these labels just right and then… when you render the doc they can look slightly different to the inline output and then… when you export via ggsave they can look slight different again. I decide below it might not be worth the effort!\n\n\n\ndom_recent %&gt;%\n  ggplot(aes(x = male, y = female, colour = country)) +\n  geom_point(size = 2) +\n  theme_classic() +\n  easy_remove_legend() +\n  theme(panel.grid.major = element_line(color = \"lightgray\", linetype = 2)) +\n   scale_y_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                     expand = c(0,0), breaks = seq(0,25, 5), limits = c(0,30)) +\n   scale_x_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                      expand = c(0,0), breaks = seq(0,12, 2), limits = c(0,14)) +\n  geom_abline(intercept = 0, slope = 1, colour = \"gray\", linetype = 2) +\n  geom_abline(intercept = 0, slope = 2, colour = \"gray\", linetype = 2) +\n  geom_abline(intercept = 0, slope = 4, colour = \"gray\", linetype = 2) +\n  \n  geom_text(data = data.frame(x = 12, y = 13, label = \"Women and men spend equal time\"), \n            mapping = aes(x = x, y = y, label = label), size = 3, angle = 16L, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 12, y = 25, label = \"Women spend 2x more time\"), \n            mapping = aes(x = x, y = y, label = label), size = 3, angle = 29L, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 6.2, y = 25.8, label = \"Women spend 4 x more time\"), \n            mapping = aes(x = x, y = y, label = label), size = 3, angle = 48L, inherit.aes = FALSE) +\n  geom_text_repel(aes(label = country), size = 2.5, max.overlaps = 10)\n\n\n\n\n\n\n\n\n\n\ntitles and captions\nAdding a title and subtitle, messed with my line annotations and I am not really sure why. I thought I had fixed the problem by adding some margin below the subtitle, making the font on the annotations smaller, and adjusting the xy position/angle of each label, but then got annoyed the that rendered angle didn’t look the same as my inline output.\nI decided that it wasn’t important for the text to sit along the line and decided to add arrows instead! I also set a seed here after realising the everytime your run the code the geom_text_repel() positions the labels slightly differently if you don’t have a seed.\n\nseed = 44\n\np &lt;- dom_recent %&gt;%\n  ggplot(aes(x = male, y = female, colour = country)) +\n  geom_point(size = 2) +\n  theme_classic() +\n  easy_remove_legend() +\n  theme(panel.grid.major = element_line(color = \"lightgray\", linetype = 2)) +\n   scale_y_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                     expand = c(0,0), breaks = seq(0,25, 5), limits = c(0,30)) +\n   scale_x_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                      expand = c(0,0), breaks = seq(0,12, 2), limits = c(0,14)) +\n  geom_abline(intercept = 0, slope = 1, colour = \"gray\", linetype = 2) +\n  geom_abline(intercept = 0, slope = 2, colour = \"gray\", linetype = 2) +\n  geom_abline(intercept = 0, slope = 4, colour = \"gray\", linetype = 2) +\n  geom_text(data = data.frame(x = 10, y = 3.5, label = \"Women and men spend \\nequal time\"), \n            mapping = aes(x = x, y = y, label = label), size = 2.5, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 12.5, y = 28, label = \"Women spend 2x \\nmore time\"), \n            mapping = aes(x = x, y = y, label = label), size = 2.5,  inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 5, y = 26.8, label = \"Women spend 4x \\nmore time\"), \n            mapping = aes(x = x, y = y, label = label), size = 2.5, inherit.aes = FALSE) +\n  geom_text_repel(aes(label = country), size = 2.5, max.overlaps = 5) +\n  labs(title = \"Time women and men spend on unpaid care and domestic work, 2022\",\n       subtitle = \"The average share of each day that women and men aged 15 and older spend on unpaid care \\nand domestic work.\",\n       x = \"Men (% of 24 hour day)\", \n       y = \"Women (% of 24 hour day)\", \n       caption = \"Data source: UN Statistics Division and UN WOMEN.\nNote: Unpaid care and domestic work includes: food preparation,\ndishwashing, upkeep of a dwelling, laundry, ironing, gardening, caring\nfor pets, shopping, servicing and repair of personal and household\ngoods, childcare, and care of the sick, elderly or disabled household\nmembers, among others.\") +\n   theme(plot.caption = element_text(hjust = 0), \n         plot.subtitle = element_text(margin=margin(0,0,20,0))) +\n  geom_curve(data = data.frame(x = 5.5, y = 28.5, xend = 6.4, yend = 26),\n          mapping = aes(x = x, y = y, xend = xend, yend = yend), curvature = -0.515, \n          arrow = arrow(20L, unit(0.1, \"inches\"), \"last\", \"closed\"), alpha = 1, inherit.aes = FALSE) +\n  geom_curve(data = data.frame(x = 11.8, y = 28, xend = 12.2, yend = 26),\n          mapping = aes(x = x, y = y, xend = xend, yend = yend), curvature = 0.515, \n          arrow = arrow(20L, unit(0.1, \"inches\"), \"last\", \"closed\"), alpha = 1, inherit.aes = FALSE) +\n  geom_curve(data = data.frame(x = 8.8, y =4, xend = 8.2, yend = 7),\n          mapping = aes(x = x, y = y, xend = xend, yend = yend), curvature = -0.515, \n          arrow = arrow(20L, unit(0.1, \"inches\"), \"last\", \"closed\"), alpha = 1, inherit.aes = FALSE) \np\n\n\n\n\n\n\n\n\n\n\ninteractivity\nIt is still miraculous to me that you can get an interactive plot like this in a single line of code. plotly for the win!\n\n\n\n\n\n\nNote\n\n\n\nNote I have ignored a lot of red warning messages from plotly here about how it doesn’t like geom_text_repel() or geom_curve()\n\n\n\nggplotly(p)\n\n\n\n\n\n\n\nbonus\nHow has the ratio of time that women and men spend in unpaid domestic labour changed over time? Given the theme today is big/small, I am going to choose the countries that have the biggest vs smallest discrepancy.\nI am dropping extra columns and computing a new variable that is the difference between the percent of time that women and men spend in unpaid domestic labour for each country and year. I am arranging these scores in descending order and then looking at the top 3 (head) and the bottom 3 (tail).\n\ndifference &lt;-  dom_clean %&gt;%\n  select(-code, -population_historical, -owid_region) %&gt;%\n  rowwise() %&gt;%\n  mutate(difference = female - male) %&gt;%\n  arrange(desc(difference)) \n\nhead(difference, 3)\n\n# A tibble: 3 × 5\n# Rowwise: \n  country  year female  male difference\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 Mexico   2009   31.0 10.4        20.7\n2 Armenia  2004   24.0  4.58       19.5\n3 Tunisia  2006   21.9  2.71       19.2\n\ntail(difference, 3)\n\n# A tibble: 3 × 5\n# Rowwise: \n  country   year female  male difference\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 Norway    2011  15.3  12.3        2.98\n2 Colombia  2013   4.22  2.03       2.19\n3 Belgium   2013   2.43  2.04       0.39\n\n\nI am filtering the data to only include these top and bottom countries. Unfortunately we don’t have data for several years in all of these countries, so lets plot Columbia and Mexico for illustration.\n\ntopbottom &lt;- c(\"Norway\", \"Colombia\", \"Belgium\", \"Mexico\", \"Armenia\", \"Tunisia\")\n\ntb &lt;- difference %&gt;%\n  filter(country %in% topbottom) %&gt;%\n  arrange(country, year)\n\ntb %&gt;%\n  tabyl(country)\n\n  country n    percent\n  Armenia 2 0.18181818\n  Belgium 1 0.09090909\n Colombia 3 0.27272727\n   Mexico 3 0.27272727\n   Norway 1 0.09090909\n  Tunisia 1 0.09090909\n\n\n\n\nplot\nIt is interesting that the difference in the amount of time that men and women engage in unpaid domestic labour has come down in Mexico (one of the most discrepant countries in 2009) but become more different in Colombia, who in 2013 had one of the smallest differences between men and women.\n\ntb %&gt;%\n  filter(country %in% c(\"Mexico\", \"Colombia\")) %&gt;%\n  ggplot(aes(x = year, y = difference, colour = country)) +\n  geom_point() +\ngeom_line() +\n  facet_wrap(~country) +\n  easy_remove_legend() +\n  scale_y_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                     expand = c(0,0), breaks = seq(0,25, 5), limits = c(0,25)) +\n  scale_x_continuous(breaks = seq(2009,2021, 2)) +\n  theme_classic() +\n  labs(y = \"Percent difference in women and men\", y = \"Year\")"
  },
  {
    "objectID": "projects/2025-02_02_academic_writing/index.html",
    "href": "projects/2025-02_02_academic_writing/index.html",
    "title": "Jenny’s (very opinionated) tips for academic writing",
    "section": "",
    "text": "I was an academic in Psychology for more than 15 years. I supervised 6 PhD students and 30 honours students and over that time, I created this guide to academic writing for them. It is just my opinion. If anything in it differs from the advice your research supervisor gives you, always go with their opinion."
  },
  {
    "objectID": "projects/2025-02_02_academic_writing/index.html#the-language-of-academic-writing",
    "href": "projects/2025-02_02_academic_writing/index.html#the-language-of-academic-writing",
    "title": "Jenny’s (very opinionated) tips for academic writing",
    "section": "The language of academic writing",
    "text": "The language of academic writing\n\nWords\n\nMake friends with the Grammar Girl\nBe careful about subject-verb agreement\n\n\nThe processes underlying X has been a topic of debate -&gt; The processes underlying X have been a topic of debate\n\n\nAvoid the undefined “this”. If there is any doubt about what “this” refers to, spell it out. Also the undefined “they”. Are you talking about participants? Or the study authors?\nAlmost always use “because” instead of “as”. Read Grammar Girl’s opinion here\n\n\n“Happy and angry expressions were chosen as/because they elicit the most distinct muscle activity. “As” can confuse the reader…\n\n\nBe careful about “which” vs. “that”. A good rule of thumb is that which almost always needs a comma before it and the which phrase is optional, see Grammar Girl’s opinion here\n\n\n“Stimuli were presented to participants using Eprime 3.0 software, which was synchronized with the Powerlab system”\n\n\nChoose your verb carefully when describing research. Some students like to use So & So (2016) demonstrated that….. but it is often more accurate to say reported or if even just showed. See Thesis Whisperer’s verb tips tips.\nThis is your thesis and it is ok to take responsibility for the work you did by using first person. When you try and avoid pronouns you inevitably end up writing in the passive i.e. “it was discovered that….”, which should be avoided.\nBe careful about words that are most commonly used to refer to statistical effects (i.e. significant, moderate, mediate, correlated). If you are not using them to refer to particular stat findings, probably best to think of another word. Your reader might assume that when you say “In addition to moderating the effect of X….” you are actually talking about statistical moderation. \nRemember that greater and lesser (and more and less) require a comparison.\n\n\n“muscles involved in frowning (corrugator supercili) show greater activity when an individual mimics angry expressions.” This needs to include the comparison; greater than what?\n\n\nAvoid acronyms and abbreviations. It might feel like you can save lots of words by not spelling out repeated phrases over and over, but acronyms tax your reader’s working memory (particularly when they are unfamiliar and there is more than one) making it less likely that your reader will get into a “flow” as they read. More about how they alienate your reader here.\n\n\n\nSentences\n\nTry to avoid writing in passive voice by bringing whoever is doing the action to the front of the sentence.\n\n\ninstead of “the participant will be told about a memory of the experimenter to ensure the child understands the concept of the task” it is better to say “the experimenter will share a memory with the child to ensure they understand the task”.\n\n\nNominalisation can also make you write in passive voice; avoid creating zombie nouns\nSemicolons- use them accurately or not at all. It is often less risky to just use a full stop instead\nMake your sentence construction is balanced\n\n\nunbalanced:\n\nfor children in the play group, the toys present were identical to the child-preferable exemplars while children in the Current First and Future first groups played with toys that were unrelated to the study\n\nbalanced:\n\nfor children in the play group, the toys present were identical to the child-preferable exemplars and for children in the Current First and Future first groups, the toys were unrelated to the study”\n\nunbalanced:\n\nIt is the ability to bring to mind personally-relevant and perceptually-rich events or episodes that have happened in the past (i.e. episodic memory) or that may possibly happen in the future (i.e episodic future thinking) (Atance & O’Neil, 2001; Tulving, 1985).”\n\nbalanced:\n\nrephrase to use the same structure across both parts of the sentence… “It is the ability to bring to mind personally-relevant and perceptually-rich events or episodes that have happened in the past (i.e. episodic memory) or could happen in the future (i.e episodic future thinking) (Atance & O’Neil, 2001; Tulving, 1985).”\n\n\n\nRemember that simple sentence structures (and simple words) are easier to read. You don’t want to make the reader wait for the main message. Front load your sentences so that you give away the main message first.\n\n\nrather than… “When affiliation and forming relationships is deemed important, adults have been shown to engage in higher levels of mimicry”\ntry… “Adults have been shown to engage in higher levels of mimicry, when affiliation and forming relationships is deemed important”\nor even better, see how many simple words you can swap for complex ones, or how many you can remove entirely… “Adults show more mimicry, when affiliation and forming relationships are important”\n\n\nTIP if you find yourself repeating an idea and explaining it in simple terms, scrap the jargony description and just use the one that you thought you needed to reexplain the complicated one using  “That is… much simpler description in lay language”\n\n\n\nParagraphs\n\nMake sure that each paragraph has a topic sentence (what is the take home message I want to convey in this paragraph?) and then 2-3+ sentences providing support (i.e. explanation, examples, evidence) for that idea. Here is a good video about paragraph recipes.\nMake sure your topic sentences are pulling their weight. The TS should state what the main point of the paragraph is, allowing you to go on to talk about research that supports that main point.\nNever start a paragraph with “A recent study by So & So (2016) showed that ….” or even worse “So & So (2016) showed that…” Use your topic sentence to explain to the reader the point of the paragraph and use the research as an example for your main point. \nNever start a topic sentence with “Other research/studies….” tell the reader how these new studies are related and what they show.\nYour topic sentences should guide the reader through your rationale so clearly that if you put all the topic sentences into 1 paragraph (i.e. make a topic sentence paragraph) it should read like an argument. \nAvoid abrupt transitions, make sure your topic sentence holds your readers hand when there is a switch of focus For example, to move from talking about prosocial behaviour in adults and switching to talk about development, you can use your topic sentence to reach back to the previous paragraph and forward to your next point \n\n\n“While we most commonly think of adults behaving prosocially toward each other, the origins of helping behaviour can be seen early in development”"
  },
  {
    "objectID": "projects/2025-02_02_academic_writing/index.html#the-structure-of-academic-writing",
    "href": "projects/2025-02_02_academic_writing/index.html#the-structure-of-academic-writing",
    "title": "Jenny’s (very opinionated) tips for academic writing",
    "section": "The structure of academic writing",
    "text": "The structure of academic writing\n\nIntros\n\nIntroductory paragraphs are important. They determine what the reader will expect from the rest of your proposal/thesis. Your introductory paragraph should introduce the general research area that you are planning to study and explain to the reader why this is an interesting area of research to address AND why it is significant/important.\nRe structure- doesn’t matter what you are writing (proposal, thesis intro, manuscript intro, grant) they ALL HAVE THE SAME STRUCTURE \n\n\n\nThis is what this project is about (broad overview of the problem)\n\n\nThis is what we know about the problem\n\n\nThis is what we don’t know about it\n\n\nThis is how THIS project fills that gap in what we don’t know\n\n\n\nLayers of subheading can sometimes let topic sentences off the hook. If you TSs are working hard they should negate the need for lots of sections. They should hold the reader’s hand through transitions by linking back to what you just said and outlining how it is related to what you are going to say. Only use a subheading if you are really switching gears and talking about something quite different.\nA literature review should be funnel shaped, starting really broad and gradually getting more specific (detailed) as you work your way towards your rationale. BUT the size of your funnel should differ depending on what you are writing. Your proposal (where your lit review needs to be 4-5 pages) needs to start at its broadest point more narrowly than your thesis (where your lit review needs to be ~15 pages)\nWhen describing a study, be sure to describe what they did, what they measured AND what they found. Always explain methods/procedures in the most simply jargon free concrete terms possible; what did the kids experience?\nIf there is a history to the literature, describe it is chronological order. Talk about the seminal study first, and then what has happened since, leading up to what is missing, and what you are going to do.\nWhen you are writing hypotheses, it is really important to explain (in terms of theoretical and/or empirical work) WHY you are predicting what you predict. You can’t just say “it is hypothesised that…..” You need to say “Given what we know about X and Y, and previous work showing Z, we predict that…”\nInclude a minimethod between your aims and hypotheses. Hypotheses are difficult for your marker to follow if you haven’t provided a little bit of detail about what participants will do and what you will measure\nYour job is to SYNTHESISE  the literature, not just describe it.\n\n\nThe Thesis Whisperer does a really great job in this post of describing how mostly science writers don’t use verbs as effectively as they could. Using verbs strategically is a great way to show your marker that you are synthesising the literature and critically evaluating it, rather than just describing it and leaving the thinking to the reader (your marker is WAY too tired to think hard while they read). This Verb cheat sheet might be useful \n\n\nProf Gernsbacher also makes the distinction between synthesising the literature and “mad-libbing” it in this video https://vimeo.com/223781358\n\n\n\nMethods\n\nParticipants do what they do for us because they want to, not because we require them to. They give informed consent and can withdraw from the experience or refuse any part of the procedure without consequence. When describing what participants experience, avoid using “participants will have to do X…” or “participants will be required to…”\nUse APA style when using numbers. \n\n\nIf the number is less than 10, write it out (i.e. These three tasks will be ….). There are exceptions to this, including age and time. We always use digits for age (i.e. 4-year-old children) and time (i.e. 10 minute delay), except when starting a sentence, then use the word (i.e. Four-year olds were recruited…)\n\n\nIn your methods section, make sure everything about the equipment and stimuli stays in the Apparatus/Stimuli . Your Procedure section should describe in simple terms what the participants experienced.\n\n\n\nResults\n\nRemember that “data” is plural, you always collect more than one datum\n\n\nData from three infants was were excluded from this looking time analysis There was no difference in the magnitude of preferences across trial 1 and 2, t(19)= .762, p= .455, d=.17, so data was were averaged.\n\n\nBe careful in the way you describe correlational analyses. Relationships refer to people not variables. Variables can be related or there can be a relation between variables but they don’t have relationships :)\n\n\n\nDiscussion\n\nFor multiple studies, I often suggest writing a Results & Discussion section, rather than a mini discussion by itself. The role of the mini discussion is just to summarise the findings from that experiment, what we have learned thus far and what we don’t yet know. You need just enough detail to provide aims/rationale for your second study. Your mini discussion should only be a few paragraphs long. Save the big picture stuff  (relating findings to past work, theory, practical implications, limitations etc) for the General Discussion\nYour general discussion should make “moves” that your marker expects\n\n\n\nwhat do we NOW know about the problem?\nhow does that relate to what we knew before?\nwhat can we say about theory/real world that we couldn’t before?\nwhat problems/limitations do we need to acknowledge?\nwhat do we still not know… what are the next steps/future directions?  f.what is the take home message?\n\n\n\nThis twitter thread from Prof Carlton Fong re planning your general discussion is REALLY good\n\n\n\nstart by writing the limitations—future directions (its the easiest bit)\n\navoid the suicide discussion- aka “We did everything wrong and nothing worked and therefore this thesis was a waste of time.”\nframe limitations as opportunities for future work rather than a list of reasons why you didn’t get what you thought you would.  \nmake sure suggestions for future research are genuine ideas for the next step in the research program. Simply suggesting that the tasks should be tried with a different age group or with different stimuli is a cop out; these suggestions will apply to EVERY thesis in your markers pile and doesn’t help your marker distinguish between students who can really think vs those who are just using a script to write their GD. Explain to your marker WHY they should care about the next question in this research program and HOW you would conduct that study, specifically. Your marker is looking for you to show them that you have ACTUALLY thought about it. \n\npick the 3-5 (probably only 3) most important findings, for each map out…(Spider diagrams might be helpful here)\n\nwhat did you find\nhow does the finding relate to past research, are the findings consistent? \n\nif yes, unpack how…\nif no, why not? here you will need to go back to your data or to the literature to try and explain why you got the pattern of results you got\n\nwhat does this finding add to the literature? \nwhat do we know now that we didn’t know before?\n\nfor each above, think about how that finding relates to/advances theory\nfor each above, think about practical/policy/real world implications/applications\nDON’T TRY TO COVER EVERYTHING, much better to do a deep dive on a handful of important ideas than to skim over a zillion possibilities but leave your reader unconvinced of any one of them \nCheck out these phrases from the APA style guide to help\nGood tips for striking a balance in your General Discussion here \nThesis Whisperer posts\n\nHow do I start my discussion chapter\nThe difficult discussion\nSpider diagrams"
  },
  {
    "objectID": "projects/2025-02_02_academic_writing/index.html#general-writing-advice",
    "href": "projects/2025-02_02_academic_writing/index.html#general-writing-advice",
    "title": "Jenny’s (very opinionated) tips for academic writing",
    "section": "General writing advice",
    "text": "General writing advice\n\nNEVER use quotes- yes in undergrad we say that if you really can’t write something any other way then put it in quotes, but real scientists can always write something in another way. When i am marking honours theses and I see ideas in quotes, my gut reaction is the student doesn’t understand that idea enough to write it in their own words.\nFind the original source.\n\nIt is mostly not sufficient to base your lit review on review articles or meta-analysis. You need to find the articles that are cited in these reviews and cite the original authors.\nsimilarly secondary citations are not cool (i.e. According to Datyner (2015) So & So (1982) suggest…..) no, you need to find So and So (1982) read it yourself and write about what they actually say, not about what Datyner says that they say (you can’t guarantee that other people have represented the old ideas accurately)\n\nWrite phrases that are balanced in their construction. For example, rather than “Recollecting memories and imagining future events …” say “Remembering past events and imagining future events….”\nIf there is a list of things, use signposting. The aims of the current research are twofold. First…. Second…. But don’t use Firstly…. Secondly…..or so says Grammar Girl\nPhrases to avoid\n\nA study by So & So (2016) …… [not only is this passive, but it puts the emphasis on WHO did the research rather than what they did and what they found]\n\nUse APA style consistently\n\nin your results\n\nMore good advice by Morton Ann Gernsbacher re writing for clarity and memorability here"
  },
  {
    "objectID": "projects/2025-02_02_academic_writing/index.html#your-thesis",
    "href": "projects/2025-02_02_academic_writing/index.html#your-thesis",
    "title": "Jenny’s (very opinionated) tips for academic writing",
    "section": "Your thesis",
    "text": "Your thesis\nMore often than not, honours students have a hard time “expanding” their proposal intro into a thesis intro and waste one of the two times I’m allowed to read their intro on a piece that has the scope or specificity wrong. You are aiming to fill 14-16 pages; if it is shorter than 14 pages (double spaced 2 cm margins) DON’T SEND IT TO ME. Instead do a reverse outline and put together a topic sentence paragraph and send that to me first. Remember that you turn a 4 pg proposal intro into a 14-16 pg thesis intro by starting MUCH broader AND then eventually getting MUCH more specific/detailed. REMEMBER that your intro should NOT read like a list of study descriptions.\nIf it sounds like, So & So did this and then Jo and Co did that and then Big Dudes did the next thing, you need to look carefully at your topic sentences and make sure they are pulling their weight. Use your topic sentences to carve out an argument and then use specific examples of research to support your argument.\nYour literature review should tell a story\n\nCitations and references\nYour thesis is a BIG document and you will have a lot of citations to keep track of. APA formatting is important but to be honest I no longer remember the actual formatting rules. APA changes its mind so often, I don’t bother keeping up with the latest version, but I can do that because I use software to automatically format citations/references for me. \nI would HIGHLY recommend using software to deal with the references in your thesis. Don’t use Endnote. It gets really annoyed when there is the same author who is listed on different pubs in different format. It thinks they are different people. Libraries can get corrupted and cause disaster (right when you want to print your thesis and hand it in).\nAt the moment I use Mendeley and really like it. You can download an app to your desktop/laptop and use it in the cloud. You can drag pdfs into the software and it will extract the details it can from the pdf (although it has a hard time with older ones). It is important to check that it has the details right (particularly abbreviations on the journal name and authors).\nYou can add an addin to your Word so Mendeley buttons appear under the References tab. When you add citations it will automatically order them, format them, recognise when you are citing a long list of authors the first time vs subsequent times, it is really good."
  },
  {
    "objectID": "projects/2025-02_02_academic_writing/index.html#recommended-resources",
    "href": "projects/2025-02_02_academic_writing/index.html#recommended-resources",
    "title": "Jenny’s (very opinionated) tips for academic writing",
    "section": "Recommended Resources",
    "text": "Recommended Resources\n\nBlogs I like\n\nwww.thesiswhisperer.com\n\nhow to unfuck your writing post\n\nPat Thompson\nExplorations of style\n\n\n\nTwitter threads I like\nJessica Rodrigues @rodriguesjm6 5 tips for academic writing\nSara Hart @ saraannhart Best academic writing tips\nCarlton Fong @carlton_fong Tips for Discussion writing\n\n\nBooks I think are good\nBecoming an academic writer- Patricia Goodson\nAir, light time and space- Helen Sword\nHow to write a lot- Paul Silvia\nWrite it up- Paul Silvia\n\n\nPhrases that might be useful\n(find more REALLY GOOD discussion examples from APA here and from Manchester Academic Phrasebank\n\nSummarise\n\nThe results indicate that…\nThere was a correlation between…\nThis analysis supports/opposes the theory that…\nThe data suggests that…\nThe analysis identifies X and Y as key factors in….\n\n\n\nInterpret\n\nContrary to the hypothesized association…\nIn line with the hypothesis…\nThe results contradict the claims of Smith (2007) that…\nThe results might suggest that X. However, based on the findings of similar studies, a more plausible explanation is Y.\n\n\n\nImplications\n\nThese results build on existing evidence of…\nThe results do not fit with the theory that…\nThe experiment provides a new insight into the relationship between…\nThese results should be taken into account when considering how to…\nThe data contributes a clearer understanding of…\nWhile previous research has focused on X, these results demonstrate that Y.\n\n\n\nLimitations\n\nThe generalizability of the results is limited by…\nThe reliability of this data is impacted by…\nDue to the lack of data on X, the results cannot confirm…\nThe methodological choices were constrained by…\nIt is beyond the scope of this study to…\n\n\n\n\nThinking bundles\nFrom Thesis Whisperer thinking bundles\nPractice thinking about the literature, methods, or your contribution using these thinking bundle templates. \n\nA large and growing body of literature has investigated …… Numerous scholars have argued that ……. (for example, Smith , 1996; Kelly, 1998; Johnson, 2002). In her seminal study, Mewburn (2010) showed that …….. One question that needs to be asked, however, is ………\nMost studies of this phenomena suffer from some serious [pick one word from the following] limitations / weaknesses / disadvantages / drawbacks / flaws. It was decided that the best method to adopt for this investigation was ……………………. This method is [pick one] The chief advantage of this method is……………….. Some limitations of this method that should be noted are……………..\nOne of the more significant findings to emerge from this study is that ……….. it was also shown that……… The results of this research support the idea that ……. The most important limitation lies in the fact that …… Further work needs to be done on …… These findings suggest several courses of action for …… The findings of this study have a number of important implications for future practice, especially ……\n\n\nJoining words\n(from TW)\n\n\nOpinionated verbs\n(also from TW)"
  },
  {
    "objectID": "projects/2025-11-30_PSYCHEdpodcast/index.html",
    "href": "projects/2025-11-30_PSYCHEdpodcast/index.html",
    "title": "PsychEd podcast",
    "section": "",
    "text": "I have run lots of workshops for Psychology students about writing. In 2024, my colleague Lisa Williams and I turned some of these workshop materials into a podcast that we called PSYCHEd. The podcast is available on Spotify."
  },
  {
    "objectID": "projects/2025-11-30_PSYCHEdpodcast/index.html#crafting-your-introduction",
    "href": "projects/2025-11-30_PSYCHEdpodcast/index.html#crafting-your-introduction",
    "title": "PsychEd podcast",
    "section": "Crafting your introduction",
    "text": "Crafting your introduction\n\nPart 1\nIn this episode, A/Prof Jenny Richmond and A/Prof Lisa Williams share their expertise on writing introductions for honours theses and proposals. They discuss the four essential “moves” of a strong introduction, strategies for effective literature review, and the importance of creating perceptual fluency for readers. The hosts offer invaluable insights on structuring arguments, being selective with content, and adapting approaches for proposals versus full theses.\n\n\nPart 2\nIn this follow-up episode, the hosts dive deeper into the art of writing a compelling thesis introduction, focusing on the structure and flow of your paragraphs. They emphasize the importance of guiding your reader through your argument with clear topic sentences, transitions, and signposting. With their expert advice, you’ll learn how to create smooth, logical flow throughout your introduction, ensuring that your reader stays engaged and follows your argument effortlessly.\n\n\nPart 3\nIn this episode, the hosts share strategies for polishing your thesis or proposal. They delve into the art of clear and concise academic writing, emphasizing the importance of clarity over complexity. Listeners will learn practical tips for refining their drafts, including the significance of defining key concepts, using consistent terminology, and the effective use of acronyms. The hosts also introduce the “1-5-25 rule” for structuring paragraphs and discuss techniques for editing, such as reading aloud to catch awkward phrasing. This episode is a must-listen for students seeking to enhance their writing skills and ensure their work stands out to supervisors and markers.\nUseful slides"
  },
  {
    "objectID": "projects/2025-11-30_PSYCHEdpodcast/index.html#the-art-of-general-discussion-writing",
    "href": "projects/2025-11-30_PSYCHEdpodcast/index.html#the-art-of-general-discussion-writing",
    "title": "PsychEd podcast",
    "section": "The Art of General Discussion writing",
    "text": "The Art of General Discussion writing\n\nPart 1\nIn this episode, A/Prof Jenny Richmond and A/Prof Lisa Williams share their wealth of experience in supervising and marking honours theses. They focus on the often-dreaded general discussion section, breaking down its purpose and structure. Learn why unexpected results can be a blessing in disguise and how to approach your discussion with confidence. The hosts provide a clear six-step formula for crafting a compelling discussion that will impress your markers. Whether you’re currently writing your thesis or preparing for future academic writing, this episode offers valuable insights to help you succeed. )\n\n\nPart 2\nIn this episode, the hosts dive deep into the art of writing the General Discussion section for an honours thesis. They share invaluable tips on summarizing results effectively, relating findings to existing literature, and handling unexpected outcomes. The hosts emphasize the importance of demonstrating critical thinking skills and provide strategies for explaining results that don’t align with hypotheses. This episode is essential listening for any student working on their thesis, offering insights that can elevate their writing from good to outstanding.\n\n\nPart 3\nIn this episode, the hosts continue exploring the “moves” your marker will expect you to make in your General Discussion section They offering invaluable advice on discussing theoretical and real-world implications, addressing limitations, proposing future directions, and crafting a strong conclusion. The hosts emphasize the importance of strategic thinking, internal consistency, and finishing strong. This episode is crucial for students looking to elevate their thesis writing and impress their markers.\nUseful slides"
  },
  {
    "objectID": "charts.html",
    "href": "charts.html",
    "title": "portfolio",
    "section": "",
    "text": "Want to support my chart making? \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nday 0 challenge\n\n\n\n\n\n\n\n\nMar 31, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 1 fractions\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 2 slope\n\n\n\n\n\n\n\n\nApr 2, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 3 circular\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 4 big small\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 5 ranking\n\n\n\n\n\n\n\n\nApr 5, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 6 florence\n\n\n\n\n\n\n\n\nApr 6, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 7 outliers\n\n\n\n\n\n\n\n\nApr 7, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 9_diverging\n\n\n\n\n\n\n\n\nApr 9, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 11_stripes\n\n\n\n\n\n\n\n\nApr 11, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 13_clusters\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 16_negative\n\n\n\n\n\n\n\n\nApr 16, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 17_birds\n\n\n\n\n\n\n\n\nApr 17, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 19_smooth\n\n\n\n\n\n\n\n\nApr 19, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 22 stars\n\n\n\n\n\n\n\n\nApr 22, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 24 WHO\n\n\n\n\n\n\n\n\nApr 24, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 25 measles risk\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 27 noise\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 28 inclusion\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nJen Richmond\n\n\n\n\n\n\n\n\n\n\n\n\nday 30 finale\n\n\n\n\n\n\n\n\nApr 30, 2025\n\n\nJen Richmond\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-05-13_tt_vesuvius/index.html",
    "href": "posts/2025-05-13_tt_vesuvius/index.html",
    "title": "vesuvius",
    "section": "",
    "text": "The TidyTuesday dataset this week is about seismic activity at Mt Vesuvius. Given we are talking about a volcano, I am expecting that there might be cyclical patterns in this data i.e. the volcano is active for a few months and then quietens down and then is active again.\n\n\n\nMt Vesuvius (image source: Wikipedia)\n\n\nDuring the #30DayChart challenge I learned how to use geom_tile() to make a “show us your stripes” plot illustrating climate-related changes in temperature. Here I am testing whether a striped plot might be a good way to capture changes in seismic activity over time.\n\nread the data\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(RColorBrewer)\nlibrary(ggeasy)\nlibrary(patchwork)\nlibrary(ggpubr)\nlibrary(ggtext)\n\noptions(scipen = 999)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 19)\n\nv &lt;- tuesdata$vesuvius\n\nv &lt;- v %&gt;%\n  mutate(month = month(time, label = TRUE))\n\nglimpse(v)\n\n\nRows: 12,027\nColumns: 12\n$ event_id              &lt;dbl&gt; 4251, 4252, 22547, 22546, 22545, 22544, 22543, 2…\n$ time                  &lt;dttm&gt; 2011-04-20 00:27:24, 2012-06-19 21:29:48, 2013-…\n$ latitude              &lt;dbl&gt; 40.81800, 40.80883, 40.82217, NA, NA, NA, NA, NA…\n$ longitude             &lt;dbl&gt; 14.43000, 14.42717, 14.42800, NA, NA, NA, NA, NA…\n$ depth_km              &lt;dbl&gt; 0.42, 1.31, 0.06, NA, NA, NA, NA, NA, NA, NA, NA…\n$ duration_magnitude_md &lt;dbl&gt; 1.2, 0.7, 2.2, 0.2, 0.2, 0.0, 0.8, 1.4, -0.2, 0.…\n$ md_error              &lt;dbl&gt; 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3…\n$ area                  &lt;chr&gt; \"Mount Vesuvius\", \"Mount Vesuvius\", \"Mount Vesuv…\n$ type                  &lt;chr&gt; \"earthquake\", \"earthquake\", \"earthquake\", \"earth…\n$ review_level          &lt;chr&gt; \"revised\", \"revised\", \"preliminary\", \"preliminar…\n$ year                  &lt;dbl&gt; 2011, 2012, 2013, 2013, 2013, 2013, 2013, 2013, …\n$ month                 &lt;ord&gt; Apr, Jun, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan…\n\n\n\n\ncreate stripe theme\nThis theme is adapted from Dominic Roye’s blog. It uses theme_minimal as a base and then gets rid of the y axis axis and labels, gridlines, legend title, adds some space to the x axis labels, makes the legend slightly narrower than default, adds a coloured background, more space around the plot and changes the titlefont.\n\n\nCode\ntheme_stripe &lt;- function() { \n  \n  theme_minimal() %+replace%\n    theme(\n      axis.text.y = element_blank(),\n      axis.line.y = element_blank(),\n      axis.title = element_blank(),\n      panel.grid.major = element_blank(),\n      legend.title = element_blank(),\n      axis.text.x = element_text(vjust = 3),\n      panel.grid.minor = element_blank(),\n      legend.key.width = unit(0.5, \"lines\"), \n      panel.background = element_rect(fill = \"papayawhip\", color = NA),\n      plot.background = element_rect(fill = \"papayawhip\", color = NA),\n      plot.margin = margin(t = 20, r = 20, b = 20, l = 20, unit = \"pt\"), \n      plot.title = element_text(\n       family = \"Lato\",\n        size = 14,\n        hjust = 0, \n        color = \"black\"\n      )\n    )\n}\n\n# specific theme for patchwork plot, less white space\n\ntheme_stripe_pw &lt;- function() { \n  \n  theme_minimal() %+replace%\n    theme(\n      axis.text.y = element_blank(),\n      axis.line.y = element_blank(),\n      axis.title = element_blank(),\n      panel.grid.major = element_blank(),\n      legend.title = element_blank(),\n      panel.grid.minor = element_blank(),\n      legend.key.width = unit(0.5, \"lines\"), \n      panel.background = element_rect(fill = \"papayawhip\", color = NA),\n      plot.background = element_rect(fill = \"papayawhip\", color = NA),\n      plot.margin = margin(t = 20, r = 10, b = 10, l = 10, unit = \"pt\"), \n      plot.title = element_text(family = \"Lato\", size = 16,hjust = 0, color = \"black\"), \n      plot.subtitle = element_text(size = 14),\n    axis.text.x = element_text(vjust = 3, size = 16),\n    legend.text = element_text(size = 14),\n    plot.caption = element_text(size = 12)\n    )\n}\n\n\n\ncol_stripe &lt;- brewer.pal(9, \"YlOrRd\")\n\n\n\n\nhow many seismic events are recorded at Mt Vesuvius?\n\nper year\nLets start by counting how many seismic events happen at Vesuvius each year. Most of the data was measured between 2013 and 2024, but there were a couple of events relating to 2011/2012 readings, so I am filtering those out.\n\n\nCode\n# count events per year\nevents_year &lt;- v %&gt;%\n  group_by(year) %&gt;%\n  summarise(count = n()) %&gt;%\n  filter(year &gt;=2013)\n\n# get mean and range for use in gradient\n\nyear_maxmin &lt;- range(events_year$count, na.rm = T)\nyear_md &lt;- mean(events_year$count, na.rm = T)\n\n# plot \nevents_year %&gt;%\n    ggplot(aes(x = year, y = 1, fill = count)) +\n  geom_tile() +\n  scale_x_continuous(breaks = seq(2014, 2024, 2)) +\n  scale_fill_gradientn(colors = col_stripe, \n                       values = scales::rescale(c(year_maxmin[1], year_md, year_maxmin[2])), \n                       na.value = \"gray80\", \n                         breaks = seq(0, 1500, 250),\n                      limits = c(0, 1500)) + \n  theme_stripe()  +\n  labs(title = \"Number of seismic events per year at Vesuvius\")\n\n\n\n\n\n\n\n\n\nYikes! Looks like 2024 was a particularly active year for Vesuvius, with 1315 events recorded.\n\n\nper month\nWhat would that look like if we counted the number of events per month and plotted that as a stripe plot? This code is exactly the same as above, except I am grouping by both year and month because summarising the number of events.\n\n\nCode\nevents_month &lt;- v %&gt;%\n  group_by(year, month) %&gt;%\n  summarise(count = n()) %&gt;%\n  unite(year_month, c(year, month), sep = \"_\", remove = FALSE) %&gt;%\n  arrange(year, month) %&gt;%\n  filter(year &gt;= 2013) %&gt;%\n  ungroup()\n\n\nmonth_maxmin &lt;- range(events_month$count, na.rm = T)\nmonth_md &lt;- mean(events_month$count, na.rm = T)\n\njan_each_year &lt;- events_month %&gt;% \n              filter(str_detect(year_month, \"_Jan$\")) %&gt;% \n              pull(year_month)\n\njust_year &lt;- events_month %&gt;% \n              filter(str_detect(year_month, \"_Jan$\")) %&gt;% \n              pull(year_month) %&gt;% \n              str_sub(1, 4)\n\n\nevents_month %&gt;%\n    ggplot(aes(x = year_month, y = 1, fill = count)) +\n  geom_tile() +\n  scale_fill_gradientn(colors = col_stripe, \n                       values = scales::rescale(c(month_maxmin[1], month_md, month_maxmin[2])), \n                       na.value = \"gray80\", \n                        breaks = seq(0, 250, 50),\n                      limits = c(0, 250)) + \n  scale_x_discrete(breaks = jan_each_year, labels = just_year) +\n  theme_stripe() +\n  labs(title = \"Number of seismic events recorded per month at Vesuvius\")\n\n\n\n\n\n\n\n\n\nPlotted by month, 2024 still looks like it had quite high activity, but not as high as periods in 2018 and 2019. In March 2019, there were 247 events recorded.\n\n\n\nhow big are the seismic events at Mt Vesuvius?\nThe duration_magnitude_md variable in the vesuvius dataset captures the duration magnitude (Md) of each event, which is an index of the amount of energy released. Here I am grouping by year and month before summarising the mean duration magnitude values per month.\n\n\nCode\nmag_month &lt;- v %&gt;%\n  group_by(year, month) %&gt;%\n  summarise(mean = mean(duration_magnitude_md, na.rm = TRUE)) %&gt;%\n  unite(year_month, c(year, month), sep = \"_\", remove = FALSE) %&gt;%\n  arrange(year, month) %&gt;%\n  filter(year &gt;= 2013) %&gt;%\n  ungroup()\n\n\nmag_maxmin &lt;- range(mag_month$mean, na.rm = T)\nmag_md &lt;- mean(mag_month$mean, na.rm = T)\n\nmag_month %&gt;%\n    ggplot(aes(x = year_month, y = 1, fill = mean)) +\n  geom_tile() +\n  scale_fill_gradientn(colors = col_stripe, \n                       values = scales::rescale(c(mag_maxmin[1], mag_md, mag_maxmin[2])), \n                       na.value = \"gray80\", \n                        breaks = seq(-0.3, 0.5, 0.1),\n                      limits = c(-0.3, 0.5), \n                      labels = scales::label_number(accuracy = 0.1)) +\n  theme_stripe() +\n  labs(title = \"Mean duration magnitude of seismic events per month at Vesuvius\") +\n    scale_x_discrete(breaks = jan_each_year, labels = just_year)\n\n\n\n\n\n\n\n\n\n\n\nis the number of events related to the duration magnitude?\nOk this is kinda cool- I know nothing about how volcanos work, but if we line up these two plots using the patchwork package, it looks a bit like months that have LOTS of events tend to have smaller mean duration magnitude, whereas months that have fewer events, tend to have larger duration magnitudes.\nI guess it makes sense that the volcano is releasing pressure, either via many small events, or by fewer larger events.\n\n\nCode\ncombo &lt;- count / mag +\n  plot_annotation(title = \"The frequency and magnitude of seismic events at Mt Vesuvius\",\n                  subtitle = \"Pressure is released either in many small events or fewer larger events\",\n                  caption = \"Data from Italian Istituto Nazionale di Geofisica e Vulcanologia (INGV)\",\n                  theme = theme_stripe_pw())\n\ncombo \n\n\n\n\n\n\n\n\n\nHere I am combining the summarising I did above to get the count of events and mean duration magnitude in the same dataframe and switching to the ggpubr::ggscatter to make it easy to add correlation coefficients to the plot.\n\n\nCode\nmag_count &lt;- v %&gt;%\n  group_by(year, month) %&gt;%\n  summarise(count = n(), mean_mag = mean(duration_magnitude_md, na.rm = TRUE)) %&gt;%\n  unite(year_month, c(year, month), sep = \"_\", remove = FALSE) %&gt;%\n  arrange(year, month) %&gt;%\n  filter(year &gt;= 2013) %&gt;%\n  ungroup()\n\nmag_count %&gt;%\n  ggscatter(x = \"count\", y = \"mean_mag\",\n   add = \"reg.line\",  # Add regressin line\n   add.params = list(color = \"blue\", fill = \"lightgray\"), # Customize reg. line\n   conf.int = TRUE, # Add confidence interval\n   cor.coef = TRUE, # Add correlation coefficient. see ?stat_cor\n   cor.coeff.args = list(method = \"pearson\", label.x = 150, label.sep = \"\\n\"), \n   title = \"At Mt Vesuvius, months with more seismic activity tend to \\nhave, on average, smaller events\", \n   caption = \"Data from Italian Istituto Nazionale di \\nGeofisica e Vulcanologia (INGV); 2013-2024\",\n   xlab = \"Number of seismic events per month\", \n   ylab = \"Mean monthly duration magnitude\"\n   )"
  },
  {
    "objectID": "posts/2025-05-02_30daychartdebrief/index.html",
    "href": "posts/2025-05-02_30daychartdebrief/index.html",
    "title": "#30dayChartChallenge debrief",
    "section": "",
    "text": "Where did April go? The #30DayChartChallenge is all done. As predicted, I didn’t manage to do it every day, but committing to try and make charts related to each of the daily prompts was a super useful exercise in pushing my data visualisation and storytelling skills. You can check out all the plots I made in April at this listing page.\nA few takeaways from the exercise…"
  },
  {
    "objectID": "posts/2025-05-02_30daychartdebrief/index.html#learning-through-play",
    "href": "posts/2025-05-02_30daychartdebrief/index.html#learning-through-play",
    "title": "#30dayChartChallenge debrief",
    "section": "learning through play",
    "text": "learning through play\nMaking charts in the month of April, just for the fun of it, has reminded me about the value of play. Play is how children naturally test ideas, relate to each other, and learn about cause and effect. Play is intrinsically motivating. Children don’t need to be told that play matters; they do it because it feels good. Play is associated with flow, that state of focused, joyful engagement where time disappears and learning feels effortless.\nYet somewhere along the path to adulthood, most of us are conditioned out of playful learning. We internalize the idea that learning should be serious, efficient, and outcome-driven. But when we give ourselves permission to play, perhaps via a challenge like the #30DayChartChallenge, it is possible to remove the pressure. We might bravely try a new chart type we haven’t used before (i.e. ggbump for ranking data) or reimagine a theme in a way that makes us laugh (i.e stars could be Hollywood stars). And through that process, we can learn more deeply, because we are in the moment and not afraid to make mistakes.\n\nstarsranking\n\n\n\n\n\nMovie star love interests age differences\n\n\n\n\n\n\n\nR&D spending rankings"
  },
  {
    "objectID": "posts/2025-05-02_30daychartdebrief/index.html#learning-sticks-when-it-matters",
    "href": "posts/2025-05-02_30daychartdebrief/index.html#learning-sticks-when-it-matters",
    "title": "#30dayChartChallenge debrief",
    "section": "learning sticks when it matters",
    "text": "learning sticks when it matters\nI started the challenge trying to stick to data from Our World in Data, but I found that I learned more when I released myself from that constraint and looked for data that was meaningful to me. Whether it was local temperature data for the stripes plot, data comparing egg prices where I live to prices in the US, or the controversy surrounding the native bird that lives in the lake near my house, I found myself more having more fun when the topic touched my own life or community.\nThat’s not a coincidence. When we connect learning to something we care about, we activate existing knowledge networks. In cognitive terms, this makes new information easier to integrate. The new learning doesn’t exist in isolation, it’s linked to other associations that we have built over time. In motivational terms, data we care about taps into curiosity and emotional investment. These values are far more powerful drivers of learning than arbitrary goals or practice for the sake of it.\nPlaying with data that is personally meaningful does more than keep you motivated. It also sharpens your thinking. During the challenge, I asked better questions, told better stories, and was more critical about how the data was presented, when I was working with data that was relevant to my life.\n\nstripessmooth\n\n\n\n\n\nTemperature in Queenstown\n\n\n\n\n\n\n\nEgg prices in NZ vs USA"
  },
  {
    "objectID": "posts/2025-05-02_30daychartdebrief/index.html#familiar-is-the-foundation",
    "href": "posts/2025-05-02_30daychartdebrief/index.html#familiar-is-the-foundation",
    "title": "#30dayChartChallenge debrief",
    "section": "familiar is the foundation",
    "text": "familiar is the foundation\nWhen learning to write code to produce beautiful and meaningful visualisations you can quickly become overwhelmed because everything is new. I have started to realise that it can be helpful to reduce the cognitive load by controlling some variables (the data) while playing with others (the method). Sometimes it can be useful to pick a dataset that you know really well, so your mental energy can go toward grappling with the truly new parts of what you are trying to learn, rather than everything feeling novel all at once.\nThis worked really well when I was trying out k-means clustering for the first time. Instead of choosing a random or unfamiliar dataset, I used the palmerpenguins dataset, which I had worked with many times before. I already knew the structure of the data, the kinds of patterns to expect, and how the species varied. This meant I could focus entirely on understanding how k-means works and how best to visualise the clusters, rather than trying to learn the method and the data and the context all at once. It was a good reminder that scaffolding new knowledge onto an existing base doesn’t just make learning easier, it also makes it stickier.\n\n\n\npalmer penguins k-means clusters\n\n\nHowever, there is also value in questioning the familiar when learning new visualisation methods. I often encountered data that had been plotted in very familiar ways (points and lines or stacked bars) and rather than simply reproducing it, had to challenge myself to think outside the box to reimagine how a different chart type might tell a better story.\nBy taking data about vaccine hesitancy in Australia and the U.S. and plotting it using a Florence Nightingale-style “flower” plot, the relative stability of vaccine hesitancy rates in the USA compared to Australia was more obvious, highlighting the impact of differences in public health messaging. Similarly, in plotting measles case rates by state on a map, rather than in a faceted bar plot, the story really stood out. It was the same data, but by layering it onto geography, it is easy to see not only what the problem is, but also where it is, making the story about public health and regional variation more compelling.\n\nCOVID vaccine hesitancymeasles cases\n\n\n\n\n\nvaccine hesitancy Australia vs USA\n\n\n\n\n\n\n\nMeasles case rates 2024-2025 by state\n\n\n\n\n\nThese examples taught me that familiarity is a powerful foundation, rather than a limitation. When we use the familiar as a starting point, we give ourselves the stability to take creative risks. We can experiment with new forms, tools, or techniques without being overwhelmed, because we’re not learning everything at once. And sometimes, taking a familiar plot and imagining how the data might be represented differently can reveal things about the data we might have otherwise missed."
  },
  {
    "objectID": "posts/2025-05-02_30daychartdebrief/index.html#plots-are-not-neutral",
    "href": "posts/2025-05-02_30daychartdebrief/index.html#plots-are-not-neutral",
    "title": "#30dayChartChallenge debrief",
    "section": "plots are not neutral",
    "text": "plots are not neutral\nAs I explored different ways of representing data, I was reminded again and again that plots are not neutral. Every design decision we make (scales, colors, chart types, comparisons) will shape how people understand a dataset. That means that we have an ethical responsibility to be intentional, transparent, and honest about the choices we make.\nOne clear example came from working with data on noise levels during Super Bowl Sunday. The original plot had adjusted the y-axis scale in a way that made the differences in noise between Super Bowl Sunday and a typical Sunday look more dramatic than they really were. When I replotted the data using a regular, evenly spaced scale, the difference was still there, but it was less sensational. This reminded me how easy it is to unintentionally (or maybe intentionally) exaggerate effects through the choices we make, without manipulating the data itself. It is possible for a plot to be technically technically accurate, but misleading.\nSimilarly, when mapping measles vaccination rates by U.S. state, I experimented with filling each state on the map using either a continuous scale (i.e. exact percentages of vaccine coverage) or categorical groupings (i.e. 95%+, 90-94.9%, less than 90%). The decision wasn’t just aesthetic, it was interpretive. A continuous scale may be more precise, but a categorical one might better serve public communication goals, if we are interested in identifying at-risk states for intervention.\n\nsuperbowl noisemeasles vax continuousmeasles vax categorical\n\n\n\n\n\n\n\n\n\n\n\n\nThese experiences reinforced the idea that ethical visualisation is about clarity, fairness, and usefulness. Sometimes that means pulling back from precision to focus on communication. Sometimes it means thinking hard about whether you really want to plot to be more striking but less honest. We need to be asking ourselves: Is this visualisation telling the story that most needs to be told, for the people who are most affected by it?"
  },
  {
    "objectID": "posts/2025-05-02_30daychartdebrief/index.html#from-data-to-stories",
    "href": "posts/2025-05-02_30daychartdebrief/index.html#from-data-to-stories",
    "title": "#30dayChartChallenge debrief",
    "section": "from data to stories",
    "text": "from data to stories\nAcross the #30DayChartChallenge I have been thinking more and more about storytelling. One example that really brought this home was the story of the Pūteketeke, which won New Zealand’s Bird of the Year competition in 2023 under somewhat chaotic circumstances, largely thanks to a campaign by comedian John Oliver.\n\n\n\nVote numbers for Bird of the Century 2023\n\n\nThe vote rate data clearly portray that something had happened to the competition that year, but by using the closeread package to create a scrollytelling article I was able to combine images, narrative and data visualizations, in a way that slowed down the viewer’s experience, piqued their curiosity and helped them connect the data to the context."
  },
  {
    "objectID": "posts/2025-05-02_30daychartdebrief/index.html#take-home",
    "href": "posts/2025-05-02_30daychartdebrief/index.html#take-home",
    "title": "#30dayChartChallenge debrief",
    "section": "take home",
    "text": "take home\nThe real magic here isn’t just in making a good-looking plot, it’s in telling a good story. You can learn all the geoms in the world, but if you can’t use them to say something that is useful and memorable, what’s the point? I have learned that turning data into stories with code gives your technical skills purpose, and it is way more fun that just churning out plots all day.\n\nAcknowledgement: Thanks to chatGPT for assistance in crafting this post"
  },
  {
    "objectID": "posts/2023-04-25-error-bars-on-plots/index.html",
    "href": "posts/2023-04-25-error-bars-on-plots/index.html",
    "title": "error bars on plots",
    "section": "",
    "text": "Repurposing this APA figures post as a IDHTG (I don’t have to google) post.\nAs I write my first paper reporting data analysis coming out of R (woot!!!), here are some notes summarising all the googling I have done this morning about how to produce APA style figures in ggplot.\n\nLoad libraries\nStart by loading tidyverse to get ggplot, here to make finding the data easy, and papaja to get the theme_apa() function.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/jenrichmond/Dropbox/ALL_R_stuff/2. WEBSITES/jenrichmond.github.io\n\nlibrary(papaja)\n\nLoading required package: tinylabels\n\n\n\n\nRead in data\n\nplotdata &lt;- read_csv(\"plotdata.csv\")\n\nNew names:\nRows: 8 Columns: 9\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(4): direction, group, detailtype, groupnew dbl (5): ...1, mean, stdev, n,\nstderr\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nhead(plotdata)\n\n# A tibble: 6 × 9\n   ...1 direction group     detailtype  mean stdev     n stderr groupnew       \n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          \n1     1 future    control   episodic    9.46  4.04    28  0.764 control group  \n2     2 future    control   semantic    4.57  2.35    28  0.444 control group  \n3     3 future    induction episodic    9.38  3.62    29  0.672 induction group\n4     4 future    induction semantic    4.69  2.85    29  0.530 induction group\n5     5 past      control   episodic   11.2   6.67    28  1.26  control group  \n6     6 past      control   semantic    5.5   5.53    28  1.05  control group  \n\n\n\n\nBasic ggplot (columns)\nPlot separate bars for episodic vs semantic details, by past and future events, separately for kids in the control group vs. induction group. Get pairs of columns using position = “dodge”.\n\nplotdata %&gt;%\n  ggplot(aes(x= detailtype, y = mean, fill = direction)) +\n    geom_col(position = \"dodge\") +\n  facet_wrap(~ groupnew)\n\n\n\n\n\n\n\n\n\nAdd error bars\n\nplotdata %&gt;%\n  ggplot(aes(x= detailtype, y = mean, fill = direction)) +\n    geom_col(position = \"dodge\") +\n  facet_wrap(~ groupnew) + geom_errorbar(aes(ymin=mean-stderr, ymax=mean+stderr),\n                  size=.3,    # Thinner lines\n                    width=.2,\n                      position=position_dodge(.9))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nAPA-ise\nThe theme_apa() from the pajaja package does most of the APAising. Gets rid of the grey and gridlines. But for some reason, now the bars are floating.\n\nplotdata %&gt;%\n  ggplot(aes(x= detailtype, y = mean, fill = direction)) +\n    geom_col(position = \"dodge\") +\n  facet_wrap(~ groupnew) + geom_errorbar(aes(ymin=mean-stderr, ymax=mean+stderr),\n                  size=.3,    # Thinner lines\n                    width=.2,\n                      position=position_dodge(.9)) +\n  theme_apa(base_size = 14)\n\n\n\n\n\n\n\n\n\n\nFix x and y axis\nExtend y axis scale and make the bars sit on the x axis\n\nplotdata %&gt;%\n  ggplot(aes(x= detailtype, y = mean, fill = direction)) +\n    geom_col(position = \"dodge\") +\n  facet_wrap(~ groupnew) + geom_errorbar(aes(ymin=mean-stderr, ymax=mean+stderr),\n                  size=.3,    # Thinner lines\n                    width=.2,\n                      position=position_dodge(.9)) +\n  theme_apa(base_size = 14) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 15)) # expand 0,0 to make the bars sit down\n\n\n\n\n\n\n\n\n\n\nFix axis labels\nUse the \\n notation to break a label or title across two lines\n\nplotdata %&gt;%\n  ggplot(aes(x= detailtype, y = mean, fill = direction)) +\n    geom_col(position = \"dodge\") +\n  facet_wrap(~ groupnew) + geom_errorbar(aes(ymin=mean-stderr, ymax=mean+stderr),\n                  size=.3,    # Thinner lines\n                    width=.2,\n                      position=position_dodge(.9)) +\n  theme_apa(base_size = 14) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 15)) +\n   labs(x=\"Detail type\", y=\"Mean number of details \\n produced\")\n\n\n\n\n\n\n\n\n\n\nMake grey scale\nUse scale_fill_grey(), values 1 = white and 0 = black, specify values in between to get shades of grey\n\nplotdata %&gt;%\n  ggplot(aes(x= detailtype, y = mean, fill = direction)) +\n    geom_col(position = \"dodge\") +\n  facet_wrap(~ groupnew) + geom_errorbar(aes(ymin=mean-stderr, ymax=mean+stderr),\n                  size=.3,    # Thinner lines\n                    width=.2,\n                      position=position_dodge(.9)) +\n  theme_apa(base_size = 14) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 15)) +\n   labs(x=\"Detail type\", y=\"Mean number of details \\n produced\") +\n  scale_fill_grey(start = 0.40, end = 0.6) \n\n\n\n\n\n\n\n\n\n\nSave as png to add to your paper\nUse ggsave(“nameoffile.png”) to save the last plot as png.\n\nggsave(\"featured.png\")\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "posts/2025-05-19_font_notes/index.html",
    "href": "posts/2025-05-19_font_notes/index.html",
    "title": "font notes",
    "section": "",
    "text": "I have been getting brave with fonts in my ggplots and running into problems with fonts not appearing the same in my quarto doc and exported png files. BlueSky advice was check out posts from data viz queen Cara Thompson.\nHere are my notes from Cara’s post re getting fonts to work for the next time fonts are misbehaving.\n\n1. check you have systemfonts()\n\nsystemfonts::system_fonts()\n\n# if not install.packages(\"systemfonts\")\n\n\n\n2. check you have the font you want\nView() will bring up a dataframe and you can search it to check the font you want is installed\n\n systemfonts::system_fonts() |&gt; \n    View()\n\nIf the font you want doesn’t appear when you use the search bar in the View(), install it from Google Fonts, restart RStudio, and check again.\n\n\n3. set graphics device to AGG\n\nTools &gt; Global options &gt; General-Graphics\n\n\n\n4. make a plot, check that your font shows up\n\np +\ntheme_minimal() +\n  theme(text = element_text(family = \"Karla\"),\n        legend.position = \"none\")\n\n\n\n5. test ggsave to make sure the fonts show up there too\n\nggsave(filename = \"test_plot.png\",\n       dpi = 400,\n       height = 5, width = 8,\n       bg = \"#FFFFFF\")\n\n\n\n6. set graphics in your quarto set up chunk\n\nknitr::opts_chunk$set(echo = TRUE,\n                      dev = \"ragg_png\",\n                      dpi = 400)"
  },
  {
    "objectID": "posts/2022-07-13-analysing-smartwatch-data/index.html",
    "href": "posts/2022-07-13-analysing-smartwatch-data/index.html",
    "title": "analysing smartwatch data",
    "section": "",
    "text": "Sometimes trying to replicate what someone is doing in a blogpost you find on twitter is a great way to learn something new. I am half heartedly thinking about trying to learn Python so when I saw this post about analysing smartwatch data on twitter I thought that it looked like interesting data and perhaps if I tried to do what they had done in R, that would be a useful way of starting to translate my R knowledge into python… maybe.\nSo here we go…."
  },
  {
    "objectID": "posts/2022-07-13-analysing-smartwatch-data/index.html#the-goal",
    "href": "posts/2022-07-13-analysing-smartwatch-data/index.html#the-goal",
    "title": "analysing smartwatch data",
    "section": "the goal",
    "text": "the goal\n\nIn the python plot they use a “ols” trendline but I don’t really know what that is so using “lm” instead. The graph in the post has the size of the points plotting very active minutes but there isn’t a legend on the plot, so I am using a function from ggeasy to remove the legend. Also worked out how to make the y axis be labelled 0 - 40k, rather than 0-40000 using the labels argument in scale_y_continuous.\n\ndf %&gt;%\n  ggplot(aes(x = Calories, y = TotalSteps, size = VeryActiveMinutes)) +\n  geom_point(colour = \"blue\", alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  easy_remove_legend() +\n  scale_y_continuous(limits = c(0,40000), labels = c(\"0\", \"10k\", \"20k\", \"30k\", \"40k\"))"
  },
  {
    "objectID": "posts/2022-07-13-analysing-smartwatch-data/index.html#the-goal-1",
    "href": "posts/2022-07-13-analysing-smartwatch-data/index.html#the-goal-1",
    "title": "analysing smartwatch data",
    "section": "the goal",
    "text": "the goal\n\nThe next graph in the blog post is a pie chart plotting the total active minutes in the 4 categories (inactive, lightly active, very active and fairly active). First I need to replicate these values. Luckily they are in the descriptives, so I am just going to select and filter everything else out of that dataframe.\n\ntam &lt;- descriptives %&gt;%\n  select(skim_variable, numeric.mean) %&gt;%\n  filter(skim_variable %in% c( \"SedentaryMinutes\", \"LightlyActiveMinutes\" , \"FairlyActiveMinutes\", \"VeryActiveMinutes\")) \n\ngt(tam)\n\n\n\n\n\n\n\nskim_variable\nnumeric.mean\n\n\n\n\nVeryActiveMinutes\n21.16489\n\n\nFairlyActiveMinutes\n13.56489\n\n\nLightlyActiveMinutes\n192.81277\n\n\nSedentaryMinutes\n991.21064\n\n\n\n\n\n\n\nOK first thing to “fix” are the labels on these categories. Inactive seems like a better label than Sedentary. Make the skim variable a factor first. Then use levels() to check that there are now levels. Then use fct_recode() to change the labels on the factor levels manually.\n\nglimpse(tam)\n\nRows: 4\nColumns: 2\n$ skim_variable &lt;chr&gt; \"VeryActiveMinutes\", \"FairlyActiveMinutes\", \"LightlyActi…\n$ numeric.mean  &lt;dbl&gt; 21.16489, 13.56489, 192.81277, 991.21064\n\ntam &lt;- tam %&gt;%\n  mutate(skim_variable = as_factor(skim_variable))\n\nlevels(tam$skim_variable)\n\n[1] \"VeryActiveMinutes\"    \"FairlyActiveMinutes\"  \"LightlyActiveMinutes\"\n[4] \"SedentaryMinutes\"    \n\ntam &lt;- tam %&gt;%\n  mutate(skim_variable = fct_recode(skim_variable, \n                                    \"Very Active Minutes\" =  \"VeryActiveMinutes\", \n                                   \"Fairly Active Minutes\" = \"FairlyActiveMinutes\", \n                                   \"Lightly Active Minutes\" = \"LightlyActiveMinutes\", \n                                    \"Inactive Minutes\" = \"SedentaryMinutes\"))\n\nlevels(tam$skim_variable)\n\n[1] \"Very Active Minutes\"    \"Fairly Active Minutes\"  \"Lightly Active Minutes\"\n[4] \"Inactive Minutes\"      \n\n\nThere isn’t a geom_pie() in ggplot, probably because pie charts are the worst visualisation but you can make one by first making a stacked bar chart using geom_bar() and then adding coord_polar().\nGood instructions available here https://r-graph-gallery.com/piechart-ggplot2.html\nBar graph version…\n\ntam %&gt;%\n  ggplot(aes(x=\"\", y=numeric.mean, fill=skim_variable)) +\n  geom_bar(stat=\"identity\") \n\n\n\n\n\n\n\n\n… add coord_polar()\n\ntam %&gt;%\n  ggplot(aes(x=\"\", y=numeric.mean, fill=skim_variable)) +\n  geom_bar(stat=\"identity\", color=\"white\") +\n  coord_polar(\"y\", start = 0) \n\n\n\n\n\n\n\n\nOK the bones are there but I really don’t want the axis labels or the grey background. Add theme_void() to get rid of those.\n\ntam %&gt;%\n  ggplot(aes(x=\"\", y=numeric.mean, fill=skim_variable)) +\n  geom_bar(stat=\"identity\", color=\"white\") +\n  coord_polar(\"y\", start = 0) +\n  theme_void()\n\n\n\n\n\n\n\n\nAwesome, now in the post they have the legend ordered by the mean (with Inactive at the top). I think you can do that within a mutate, right before your data hits ggplot [see this post] (https://r-graph-gallery.com/267-reorder-a-variable-in-ggplot2.html).\n\n tam %&gt;%\n  mutate(skim_variable = fct_reorder(skim_variable, desc(numeric.mean))) %&gt;%\n  ggplot(aes(x=\"\", y=numeric.mean, fill=skim_variable)) +\n  geom_bar(stat=\"identity\", color=\"white\") +\n  coord_polar(\"y\", start = 0) +\n  theme_void()\n\n\n\n\n\n\n\n\nAnd they have ridiculous number labels… in the spirit of reproducibility, lets do that too!\n\ntam %&gt;%\n  mutate(skim_variable = fct_reorder(skim_variable, desc(numeric.mean))) %&gt;%\n  ggplot(aes(x=\"\", y=numeric.mean, fill=skim_variable, label = numeric.mean)) +\n  geom_bar(stat=\"identity\", color=\"white\") +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  geom_text(angle = 45)\n\n\n\n\n\n\n\n\nHmmmm I have overlapping numbers! I would be great to have more control over where the numbers go… I thought maybe ggannotate would help but it doesn’t work with polar coordinates. So I am stuck with position dodge. Adding a mutate to round the numbers also helps…\n\ntam %&gt;%\n  mutate(skim_variable = fct_reorder(skim_variable, desc(numeric.mean))) %&gt;%\n  mutate(numeric.mean =  round(numeric.mean, 4)) %&gt;%\n  ggplot(aes(x=\"\", y=numeric.mean, fill=skim_variable, label = numeric.mean)) +\n  geom_bar(stat=\"identity\", color=\"white\") +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  geom_text(angle = 45, position = position_dodge(0.5))\n\n\n\n\n\n\n\n\nNot terrible, what about colours?? The original post has blue, pink, yellow and green.\n\nblue 1 0 245 (#0100F5)\npink 246 194 203 (#F6C2CB)\nyellow 249 217 73 (#F9D949)\ngreen 166 236 153 (#A6EC99)\n\nI worked out how to use the Digital Colour Meter from Utilities on my Mac ot get the exact RGB codes for the colours in the graph using this resource.\nThen used this RGB-Hex converter. I wonder if this step is necessary?? does ggplot know RGB codes??\nAhhh maybe not… but there is a rgb function, check this out!\n\nrgb(1,0,245, maxColorValue = 255)\n\n[1] \"#0100F5\"\n\nrgb(246,194,203, maxColorValue = 255)\n\n[1] \"#F6C2CB\"\n\nrgb(249,217,73, maxColorValue = 255)\n\n[1] \"#F9D949\"\n\nrgb(166,236,153, maxColorValue = 255)\n\n[1] \"#A6EC99\"\n\n\nAdding in colours using scale_fill_manual() and removing the legend title with ggeasy.\n\ntam %&gt;%\n  mutate(skim_variable = fct_reorder(skim_variable, desc(numeric.mean))) %&gt;%\n  mutate(numeric.mean =  round(numeric.mean, 4)) %&gt;%\n  ggplot(aes(x=\"\", y=numeric.mean, fill=skim_variable, label = numeric.mean)) +\n  geom_bar(stat=\"identity\", color=\"white\") +\n  scale_fill_manual(values = c(\"#0100F5\",\"#F6C2CB\",\"#F9D949\",\"#A6EC99\")) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  geom_text(angle = 45, position = position_dodge(0.5)) +\n  easy_remove_legend_title()\n\n\n\n\n\n\n\n\nUnder the pie chart there are some summary stats…lets see if we can get those using inline code.\n\ntam_wide &lt;- tam %&gt;%\n  pivot_wider(names_from = skim_variable, values_from = numeric.mean) %&gt;%\n  clean_names() %&gt;%\n  rowwise() %&gt;%\n  mutate(total = very_active_minutes + fairly_active_minutes + lightly_active_minutes + inactive_minutes) %&gt;%\n  pivot_longer(names_to = \"category\", values_to = \"minutes\", very_active_minutes:inactive_minutes) %&gt;%\n  relocate(total, .after = minutes) %&gt;%\n  mutate(percent = (minutes/total)*100) %&gt;%\n  mutate(percent = round(percent, 1)) %&gt;%\n   mutate(minutes = round(minutes, 0))\n\nObservations\n\n81.3 of Total inactive minutes in a day\n15.8 of Lightly active minutes in a day\nOn an average, only 21 (1.7) were very active\nand 1.1 (14) of fairly active minutes in a day"
  },
  {
    "objectID": "posts/2022-07-13-analysing-smartwatch-data/index.html#the-goal-2",
    "href": "posts/2022-07-13-analysing-smartwatch-data/index.html#the-goal-2",
    "title": "analysing smartwatch data",
    "section": "the goal",
    "text": "the goal\n\nNext up there is a column plot that looks at activity by day of the week. The lubridate package makes it easy to pull the day out of a date. I am going back to the original data frame and making a new one that includes just the id and activity date and the activity in minutes.\n\nglimpse(df)\n\nRows: 940\nColumns: 16\n$ Id                       &lt;dbl&gt; 1503960366, 1503960366, 1503960366, 150396036…\n$ ActivityDate             &lt;date&gt; 2016-04-12, 2016-04-13, 2016-04-14, 2016-04-…\n$ TotalSteps               &lt;dbl&gt; 13162, 10735, 10460, 9762, 12669, 9705, 13019…\n$ TotalDistance            &lt;dbl&gt; 8.50, 6.97, 6.74, 6.28, 8.16, 6.48, 8.59, 9.8…\n$ TrackerDistance          &lt;dbl&gt; 8.50, 6.97, 6.74, 6.28, 8.16, 6.48, 8.59, 9.8…\n$ LoggedActivitiesDistance &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ VeryActiveDistance       &lt;dbl&gt; 1.88, 1.57, 2.44, 2.14, 2.71, 3.19, 3.25, 3.5…\n$ ModeratelyActiveDistance &lt;dbl&gt; 0.55, 0.69, 0.40, 1.26, 0.41, 0.78, 0.64, 1.3…\n$ LightActiveDistance      &lt;dbl&gt; 6.06, 4.71, 3.91, 2.83, 5.04, 2.51, 4.71, 5.0…\n$ SedentaryActiveDistance  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ VeryActiveMinutes        &lt;dbl&gt; 25, 21, 30, 29, 36, 38, 42, 50, 28, 19, 66, 4…\n$ FairlyActiveMinutes      &lt;dbl&gt; 13, 19, 11, 34, 10, 20, 16, 31, 12, 8, 27, 21…\n$ LightlyActiveMinutes     &lt;dbl&gt; 328, 217, 181, 209, 221, 164, 233, 264, 205, …\n$ SedentaryMinutes         &lt;dbl&gt; 728, 776, 1218, 726, 773, 539, 1149, 775, 818…\n$ Calories                 &lt;dbl&gt; 1985, 1797, 1776, 1745, 1863, 1728, 1921, 203…\n$ TotalMinutes             &lt;dbl&gt; 1094, 1033, 1440, 998, 1040, 761, 1440, 1120,…\n\nday &lt;- df %&gt;%\n  clean_names() %&gt;%\n  select(id:activity_date, very_active_minutes:calories) %&gt;%\n  mutate(day = wday(activity_date, label = TRUE)) %&gt;%\n  rename(inactive_minutes = sedentary_minutes) %&gt;%\n  pivot_longer(names_to = \"category\", values_to = \"minutes\", very_active_minutes:inactive_minutes) %&gt;%\n  mutate(category = str_sub(category, end = -9)) %&gt;%\n  mutate(category = fct_relevel(category, c(\"very_active\", \"fairly_active\", \"lightly_active\", \"inactive\")))\n\nday %&gt;%\n  filter(category != \"inactive\") %&gt;%\n  group_by(day, category) %&gt;%\n  summarise(activity = sum(minutes)) %&gt;%\n  ggplot(aes(x = day, y = activity, fill = category)) +\n  geom_col(position = \"dodge\") +\n   scale_fill_manual(values = c(\"purple\", \"darkgreen\", \"pink\")) +\n   scale_y_continuous(limits = c(0,40000), labels = c(\"0\", \"10k\", \"20k\", \"30k\", \"40k\")) +\n  easy_remove_legend_title()\n\n`summarise()` has grouped output by 'day'. You can override using the `.groups`\nargument."
  },
  {
    "objectID": "posts/2025-06-03_tt_gutenberg/index.html",
    "href": "posts/2025-06-03_tt_gutenberg/index.html",
    "title": "gutenberg",
    "section": "",
    "text": "The Tidy Tuesday data this week comes from the gutenbergr package, which pulls data about the ebooks and authors from Project Gutenberg."
  },
  {
    "objectID": "posts/2025-06-03_tt_gutenberg/index.html#how-many-different-languages-are-available-in-the-project-gutenberg-collection",
    "href": "posts/2025-06-03_tt_gutenberg/index.html#how-many-different-languages-are-available-in-the-project-gutenberg-collection",
    "title": "gutenberg",
    "section": "1. How many different languages are available in the Project Gutenberg collection?",
    "text": "1. How many different languages are available in the Project Gutenberg collection?\n\n\n\n\n\n\nNote\n\n\n\nReminder: the dplyr::distinct function is useful for getting rid of duplicate rows. The base::unique function is similar.\njanitor::get_dups will pull duplicate entries in a particular variable.\nIf we want to count the number of unique entries in a variable, we need dplyr::n_distinct\n\n\n\n\nCode\nglimpse(languages)\n\n\nRows: 76,205\nColumns: 3\n$ gutenberg_id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ language        &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ total_languages &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\nCode\ndistinct &lt;- n_distinct(languages$language)\n\n\nThere are books in 70 languages represented in the Gutenberg data."
  },
  {
    "objectID": "posts/2025-06-03_tt_gutenberg/index.html#how-many-books-are-available-in-each-language",
    "href": "posts/2025-06-03_tt_gutenberg/index.html#how-many-books-are-available-in-each-language",
    "title": "gutenberg",
    "section": "2. How many books are available in each language?",
    "text": "2. How many books are available in each language?\nThe metadata dataframe contains information about books and what language they are in. tabyl() counts how many books there are in each language; here I am displaying the top10 languages.\n\n\nCode\nbooks_per_language &lt;- metadata %&gt;%\n  tabyl(language) %&gt;%\n  select(language, n) %&gt;%\n  arrange(-n) %&gt;%\n  head(10) \n\ngt::gt(books_per_language)\n\n\n\n\n\n\n\n\nlanguage\nn\n\n\n\n\nen\n63362\n\n\nfr\n4091\n\n\nfi\n3388\n\n\nde\n2365\n\n\nnl\n1186\n\n\nit\n1077\n\n\nes\n909\n\n\npt\n671\n\n\nhu\n616\n\n\nzh\n443\n\n\n\n\n\n\n\nI don’t know what many of language codes in that table are, but I found a table of codes here. I used datapasta to get the codes into R and then join them to the books_per_language dataframe.\n\n\nCode\nbook_codes &lt;- left_join(books_per_language, codes, by = \"language\") %&gt;%\n  rename(code = language, language = Name) %&gt;%\n  mutate(language = as.factor(language)) \n\n\ngt::gt(book_codes)\n\n\n\n\n\n\n\n\ncode\nn\nlanguage\n\n\n\n\nen\n63362\nEnglish\n\n\nfr\n4091\nFrench\n\n\nfi\n3388\nFinnish\n\n\nde\n2365\nGerman\n\n\nnl\n1186\nDutch\n\n\nit\n1077\nItalian\n\n\nes\n909\nSpanish\n\n\npt\n671\nPortuguese\n\n\nhu\n616\nHungarian\n\n\nzh\n443\nChinese\n\n\n\n\n\n\n\n\nplot\nThere are so may more books in English relative to other languages, perhaps a treemap plot would work here. Referring back to my 30 Day chart challenge code using the treemapify package.\n\n\nCode\npalette &lt;- c(\"#59C7EBFF\", \"#CCEEF9FF\", \"#FFB8ACFF\", \"#FEE2DDFF\", \"#0AA398FF\", \"#71D1CCFF\", \"#ECA0B2FF\", \"#F3BFCBFF\", \"#B8BCC1FF\", \"#E1E2E5FF\")\n\n\n  \nbook_codes %&gt;% \n  ggplot(aes(area = n, fill = language, label = paste(language, n, sep = \"\\n\"))) +\n  geom_treemap(colour = \"white\") +\n scale_fill_manual(values = palette) +\n  geom_treemap_text(colour = \"black\",\n                    place = \"topleft\",\n                    size = 5, \n                    grow = FALSE) + # option from ggfittext to NOT make font fit box\n  easy_remove_legend() +\n  labs(title = \"Number of books in the Gutenberg database by language\", \n       subtitle = \"Top 10 languages\", \n       caption = \"Data source `gutenbergr` package Tidy Tuesday\") +\n   theme(text = element_text(family = \"Karla\"), \n         plot.background = element_rect(\"antiquewhite\")) +\n  easy_caption_size(8)"
  },
  {
    "objectID": "posts/2025-06-03_tt_gutenberg/index.html#do-any-authors-appear-under-more-than-one-gutenberg_author_id",
    "href": "posts/2025-06-03_tt_gutenberg/index.html#do-any-authors-appear-under-more-than-one-gutenberg_author_id",
    "title": "gutenberg",
    "section": "3. Do any authors appear under more than one gutenberg_author_id?",
    "text": "3. Do any authors appear under more than one gutenberg_author_id?\n\n\nCode\ndups &lt;- authors %&gt;%\n  get_dupes(author)\n\ndup_authors &lt;- n_distinct(dups$author)\n\n\nThere are 119 authors in the dataset who are under more than one author_id."
  },
  {
    "objectID": "posts/2025-06-03_tt_gutenberg/index.html#when-were-most-of-the-gutenberg-books-written",
    "href": "posts/2025-06-03_tt_gutenberg/index.html#when-were-most-of-the-gutenberg-books-written",
    "title": "gutenberg",
    "section": "4. When were most of the gutenberg books written?",
    "text": "4. When were most of the gutenberg books written?\nHere I am joining the authors dataframe to the metadata to add the author birthdate and deathdate. I am adding new columns to distinguish between authors who lived in the time of the printing press (post 1500) vs. earlier.\n\n\nCode\nmeta_authors &lt;- left_join(metadata, authors, by = \"gutenberg_author_id\")\n\nmeta_authors &lt;- meta_authors %&gt;%\n  select(gutenberg_id, title, author = author.x, gutenberg_author_id, alias, birthdate, deathdate, language, wikipedia, gutenberg_bookshelf, rights, has_text) %&gt;%\n  mutate(timepoint= case_when(birthdate &lt; 1500 ~ \"ancient\", \n                               birthdate &gt;= 1500 ~ \"modern\"))\n\n\n\nplot\n\n\nCode\na &lt;- meta_authors %&gt;%\n  filter(timepoint == \"ancient\") %&gt;%\n  ggplot(aes(x = birthdate)) +\n  geom_histogram(binwidth = 100) +\n  theme_minimal() +\n  scale_y_continuous(limits = c(0,150)) +\n  labs(subtitle = \"Pre-Modern\", y = \"Number of books\", x = \"Author birthdate\") \n\n\n\nm &lt;- meta_authors %&gt;%\n  filter(timepoint == \"modern\") %&gt;%\n  ggplot(aes(x = birthdate)) +\n  geom_histogram(binwidth = 20) +\n  theme_minimal() +\n  scale_y_continuous(limits = c(0,20000)) +\n  labs(subtitle = \"Modern\", y = \"Number of books\", x = \"Author birthdate\") \n\na + m +\n  plot_annotation(title = \"Project Gutenberg: Books by historial period\", subtitle = \"The majority of books in the Gutenberg database were written by authors during in \\nthe 19th century, however, there are also books from Ancient Greek and Roman \\nliterature and the Medieval period\")"
  },
  {
    "objectID": "posts/2025-05-21_annotations/index.html",
    "href": "posts/2025-05-21_annotations/index.html",
    "title": "selective annotations",
    "section": "",
    "text": "My friend Lisa called me with a ggplot challenge the other day. She was trying to work out how to have only some of her points labelled with annotations. Here are some notes about how you can label some points but not others.\n\nload packages/make dataframe\n\n\nCode\nlibrary(tidyverse)\n\n## make up some data\n\ndf &lt;- data.frame(\n             level = as.factor(c(\"Level A\", \"Level A\", \n                                 \"Level B\", \"Level B\", \n                                 \"Level C\", \"Level C\", \n                                 \"Level D\", \"Level D\",\n                                 \"Level E\", \"Level E\")),\n              gender = as.factor(c(\"Female\",\"Male\",\n                                   \"Female\",\"Male\",\n                                   \"Female\",\"Male\",\n                                   \"Female\",\"Male\", \n                                   \"Female\",\"Male\")),\n             score = c(12L, 23L, 13L, 20L, 16L, 18L, 15L, 20L, 11L, 22L)\n      )\n\nglimpse(df)\n\n\nRows: 10\nColumns: 3\n$ level  &lt;fct&gt; Level A, Level A, Level B, Level B, Level C, Level C, Level D, …\n$ gender &lt;fct&gt; Female, Male, Female, Male, Female, Male, Female, Male, Female,…\n$ score  &lt;int&gt; 12, 23, 13, 20, 16, 18, 15, 20, 11, 22\n\n\n\n\ndata setup\nFirst, check that levels are behaving and then create new variables that contain just the values you want to print on your plot.\n\n\nCode\nlevels(df$level)\n\n\n[1] \"Level A\" \"Level B\" \"Level C\" \"Level D\" \"Level E\"\n\n\nCode\ndf &lt;- df %&gt;%\n  mutate(f_values = case_when(gender == \"Female\" ~ score), \n         m_values = case_when(gender == \"Male\" ~ score)) \n\nglimpse(df)\n\n\nRows: 10\nColumns: 5\n$ level    &lt;fct&gt; Level A, Level A, Level B, Level B, Level C, Level C, Level D…\n$ gender   &lt;fct&gt; Female, Male, Female, Male, Female, Male, Female, Male, Femal…\n$ score    &lt;int&gt; 12, 23, 13, 20, 16, 18, 15, 20, 11, 22\n$ f_values &lt;int&gt; 12, NA, 13, NA, 16, NA, 15, NA, 11, NA\n$ m_values &lt;int&gt; NA, 23, NA, 20, NA, 18, NA, 20, NA, 22\n\n\n\n\nbasic plot\n\n\nCode\ndf %&gt;%\n  ggplot(aes(x = level, y = score, \n             colour = gender, group = gender)) +\n  geom_point() + \n  geom_line() +\n  scale_y_continuous(limits = c(0,30)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nplot with selective annotations\nHere we have added two geom_text() calls, one for m_values and one for f_values. Unfortunately the values end up on top of the points.\n\n\nCode\ndf %&gt;%\n  ggplot(aes(x = level, y = score, \n             colour = gender, group = gender)) +\n  geom_point() + \n  geom_line() +\n  scale_y_continuous(limits = c(0,30)) +\n  theme_minimal() +\n  geom_text(aes(label = m_values), size = 3) +\n  geom_text(aes(label = f_values), size = 3) \n\n\n\n\n\n\n\n\n\nAdd a little vjust to shift the points up and down a little from the points. Don’t love what has happened to the legend though…\n\n\nCode\ndf %&gt;%\n  ggplot(aes(x = level, y = score, \n             colour = gender, group = gender)) +\n  geom_point() + \n  geom_line() +\n  scale_y_continuous(limits = c(0,30)) +\n  theme_minimal() +\n  geom_text(aes(label = m_values), vjust = -2, size = 3) +\n  geom_text(aes(label = f_values), vjust = 2, size = 3) \n\n\n\n\n\n\n\n\n\nAdding show.legend = FALSE to let R know that I don’t want the text labels to appear in the legend.\n\n\nCode\ndf %&gt;%\n  ggplot(aes(x = level, y = score, colour = gender, group = gender)) +\n  geom_point() + \n  geom_line() +\n  scale_y_continuous(limits = c(0,30)) +\n  theme_minimal() +\n  geom_text(aes(label = m_values), vjust = -2, size = 3, show.legend = FALSE) +\n  geom_text(aes(label = f_values), vjust = 2, size = 3, show.legend = FALSE) \n\n\n\n\n\n\n\n\n\nSUCCESS!"
  },
  {
    "objectID": "posts/2025-05-14_cool-stuff-tidytuesdayR/index.html",
    "href": "posts/2025-05-14_cool-stuff-tidytuesdayR/index.html",
    "title": "cool stuff from tidytuesdayR",
    "section": "",
    "text": "Have you tried #TidyTuesday? It is a weekly data challenge where the team from the Data Science Learning Community curate and post a dataset to their github repository, then data nerds from all over the world have a go at making a cool visualisation with it and everyone shares what they came up with on social media via the hashtag #tidytuesday.\nData organising within the #rstats community takes work; TidyTuesday doesn’t just happen. Ted Laderas has talked a lot about burnout among organisers. It is common in volunteer settings for 20% of the people to do 80% of the work, but this can lead to unsustainable communities. Ted points to importance of expanding your core group of organisers and making it easy for people to contribute, as ways to make a data initiative work better for everyone; many hands make light work.\nRecently the Jon Harmon and the #tidytuesday team have put this philosophy into action by creating some functions within the tidytuesdayR package that make it easy to curate and contribute a dataset to the challenge.\n\nHow to use tidytuesdayR as a participant\nThe package has a number of functions that make it super easy for you get the data into RStudio. No need to download the csv and read it back in. Just use the tt_load() with the date or year and week.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n# install.packages(\"tidytuesdayR\")\nlibrary(tidytuesdayR)\n\ntues_data &lt;- tidytuesdayR::tt_load(\"2025-04-29\") \n\n\n---- Compiling #TidyTuesday Information for 2025-04-29 ----\n--- There is 1 file available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 1: \"user2025.csv\"\n\n\nCode\n# OR use year and week\n# tues_data &lt;- tidytuesdayR::tt_load(2025, week = 17) \n\n\nYour tues_data object will be a list that will sometimes contain more than one table so using str() will give you an idea of which dataframe might be of interest.\n\nstr(tues_data)\n\nList of 1\n $ user2025: spc_tbl_ [128 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ id             : num [1:128] 170 79 30 31 39 169 94 163 13 51 ...\n  ..$ session        : chr [1:128] \"Virtual\" \"Virtual\" \"Virtual\" \"Virtual\" ...\n  ..$ date           : Date[1:128], format: \"2025-08-01\" \"2025-08-01\" ...\n  ..$ time           : chr [1:128] \"TBD\" \"TBD\" \"TBD\" \"TBD\" ...\n  ..$ room           : chr [1:128] \"Online\" \"Online\" \"Online\" \"Online\" ...\n  ..$ title          : chr [1:128] \"A Robust and Informative Application for viewing the dataframes in R\" \"A first look at Positron\" \"Analyzing Census Data in R: Techniques and Applications\" \"Automating workflows with webhooks and plumber in R\" ...\n  ..$ content        : chr [1:128] \"In R programming, the View() function from the Utils package provides a basic interface for viewing the datafra\"| __truncated__ \"Positron is a next generation data science IDE built by the creators of RStudio. It has been available for beta\"| __truncated__ \"This talk provides an introduction to working with IPUMS Census American Community Survey (ACS) data in R, focu\"| __truncated__ \"Webhooks have brought to us new possibilities for automating workflows. With such, we can eliminate the need fo\"| __truncated__ ...\n  ..$ video_recording: chr [1:128] \"✅\" \"✅\" \"✅\" \"✅\" ...\n  ..$ keywords       : chr [1:128] \"statistical programming, clinical trials data, dataset interface, workflow\" \"ide, workflow, tooling\" \"demography, frameworks, census data, equity ml/ai, anti-discrimination in ml/ai\" \"automation, event-driven workflows, plumber api, github webhooks\" ...\n  ..$ speakers       : chr [1:128] \"Madhan Kumar Nagaraji\" \"Julia Silge (Posit PBC)\" \"Joanne Rodrigues\" \"CLINTON DAVID\" ...\n  ..$ co_authors     : chr [1:128] NA NA NA NA ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   id = col_double(),\n  .. ..   session = col_character(),\n  .. ..   date = col_date(format = \"\"),\n  .. ..   time = col_character(),\n  .. ..   room = col_character(),\n  .. ..   title = col_character(),\n  .. ..   content = col_character(),\n  .. ..   video_recording = col_character(),\n  .. ..   keywords = col_character(),\n  .. ..   speakers = col_character(),\n  .. ..   co_authors = col_character()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n - attr(*, \".tt\")= 'tt' chr \"user2025.csv\"\n  ..- attr(*, \".files\")='data.frame':   1 obs. of  3 variables:\n  .. ..$ data_files: chr \"user2025.csv\"\n  .. ..$ data_type : chr \"csv\"\n  .. ..$ delim     : chr \",\"\n  ..- attr(*, \".readme\")=List of 2\n  .. ..$ node:&lt;externalptr&gt; \n  .. ..$ doc :&lt;externalptr&gt; \n  .. ..- attr(*, \"class\")= chr [1:2] \"xml_document\" \"xml_node\"\n  ..- attr(*, \".date\")= Date[1:1], format: \"2025-04-29\"\n - attr(*, \"class\")= chr \"tt_data\"\n\n\nIn this case the data is about the userR2025 conference schedule. We can pull out the dataframe from the list using list$dataframe to get started.\n\nuser25 &lt;- tues_data$user2025\n\nglimpse(user25)\n\nRows: 128\nColumns: 11\n$ id              &lt;dbl&gt; 170, 79, 30, 31, 39, 169, 94, 163, 13, 51, 144, 145, 1…\n$ session         &lt;chr&gt; \"Virtual\", \"Virtual\", \"Virtual\", \"Virtual\", \"Virtual\",…\n$ date            &lt;date&gt; 2025-08-01, 2025-08-01, 2025-08-01, 2025-08-01, 2025-…\n$ time            &lt;chr&gt; \"TBD\", \"TBD\", \"TBD\", \"TBD\", \"TBD\", \"TBD\", \"TBD\", \"TBD\"…\n$ room            &lt;chr&gt; \"Online\", \"Online\", \"Online\", \"Online\", \"Online\", \"Onl…\n$ title           &lt;chr&gt; \"A Robust and Informative Application for viewing the …\n$ content         &lt;chr&gt; \"In R programming, the View() function from the Utils …\n$ video_recording &lt;chr&gt; \"✅\", \"✅\", \"✅\", \"✅\", \"✅\", \"✅\", \"✅\", \"✅\", \"✅\", \"✅\", \"✅\",…\n$ keywords        &lt;chr&gt; \"statistical programming, clinical trials data, datase…\n$ speakers        &lt;chr&gt; \"Madhan Kumar Nagaraji\", \"Julia Silge (Posit PBC)\", \"J…\n$ co_authors      &lt;chr&gt; NA, NA, NA, NA, \"Abbie Brookes (Data Scientist @ Datac…\n\n\n\n\nHow to use tidytuesdayR as a contributor\nDo you have an idea of a dataset that might be of interest to other tidytuesdayers? Great! The new functions in tidytuesdayR make it super easy to contribute a dataset.\nAll you have to do is follow these instructions.\nHere are some notes I made for myself while I was curating the week 20 water quality dataset. Make sure you have the tidytuesdayR installed and loaded, and a github account sorted before you begin.\n\nStep 1: write a cleaning script\nThis function opens a cleaning.R script that you can use to write the code you need to get your data file from its raw state into a state that other people can use.\n\ntt_clean()\n\n\n\nStep 2: save your clean data to .csv\nOnce you have written your cleaning.R script and checked that it produces clean dataframes, you can save your datafile. This function will save your dataframes as .csv in your submission folder. It will also open a .md file with a table that you can complete that describes each of the variables in your dataset.\n\ntt_save_dataset(nameofyourdf)\n\n\n\nStep 3: introduce your data\nThis function opens another .md file that you can use to write an introduction to your dataset. You can describe the data, where it comes from and suggest some questions that people might like to explore. You also want to think about an image that can go along with the data.\n\ntt_intro()\n\n\n\nStep 4: meta data\nThis bit was cool: this function walks you through a question and answer session, filling in all the details needed for the meta data. Once you are done answering the questions, it creates a meta.yaml file.\n\ntt_meta()\n\n\n\nStep 5: submit\nThis bit creates a pull request (i.e. a request that the tidytuesday team pull in your curated dataset to their repo). Make sure you have a github account sorted before embarking on this last step.\n\ntt_submit()\n\n\n\nDONE!"
  },
  {
    "objectID": "posts/2025-05-09-what_happened_what_now/index.html",
    "href": "posts/2025-05-09-what_happened_what_now/index.html",
    "title": "what happened? what now?",
    "section": "",
    "text": "Image generated with the help of ChatGPT and DALL·E by OpenAI\nI think about academia like a train that I jumped on way back when I was a baby scientist. The first time one of your professors sees potential in you, they invite you to come on an exclusive train journey. While the train winds its way through rugged mountain terrain, with steep cliffs on either side of the tracks, there are really no decisions to be made. There are hurdles to jump along the way and sometimes those hurdles result in people being ejected from the train. Do you gain entry to the PhD program? Are you successful on the faculty job market? Do you get grants? Provided you jump each of these hurdles without being thrown from the train, you are allowed to continue on the journey without ever really being required to make a conscious decision about whether you still want to go to the destination."
  },
  {
    "objectID": "posts/2025-05-09-what_happened_what_now/index.html#my-train-journey",
    "href": "posts/2025-05-09-what_happened_what_now/index.html#my-train-journey",
    "title": "what happened? what now?",
    "section": "my train journey",
    "text": "my train journey\nI got on the “academic train” when I was invited to join the honours program in psychology after my 1st year at Otago.  Honours students were allowed to take 3rd year courses in their 2nd year and do a special research-based course in their 3rd year. Provided students continued to do well, they could do a 4th year honours project and graduate with extra letters after their name. I chose to take a 3rd year Neuro course in my 2nd year, learning about all the strange and interesting things that happen when different parts of the brain are damaged and getting to dissect sheep brains (only in NZ!) in neuroanatomy lab.Isn’t it interesting that no one is thrown from the train for being a crappy teacher?\nInspired by that course, for my 3rd year project, I worked in Mike Colombo’s lab. I got to work really closely with the PhD student who had been my lab demonstrator in 1st year, Nic Broadbent, who was studying the effect of hippocampal lesions on learning in pigeons, testing the assumption that the hippocampus in birds does the same thing as the hippocampus in mammals. I studied autoshaping behavior, trying to work out why the lesioned birds learned to peck the key in the operant chamber more slowly than the control birds. I got a vacation research scholarship and I spent the summer between 3rd and 4th year in a dark room, coding videos of pigeons inside the operant chamber, astounded that someone would pay me money to work out this mystery. I wrote up that work and it was published in a peer-reviewed journal (2002).\n\nRichmond, J, and M. Colombo. 2002. “Hippocampal Lesions, Contextual Retrieval, and Autoshaping in Pigeons.” Brain Research 928 (1-2): 60–68.\nI had jumped the first hurdle 🚆 🚧 ✅ and it felt good. Turns out, the lesioned birds stumbled around inside the chamber, just moving through space with less accuracy than controls\nAfter Honours, I switched labs (the pigeons were gross and I didn’t like surgery), and got a prestigious government scholarship (they even called it the Bright Future Scholarship ) which funded my PhD work. My project used eyetracking to dig into the mechanisms underlining visual preferences, which are commonly used to measure learning in infants. I happened to finish my PhD just as Prof. Chuck Nelson was looking for a postdoc researcher to work on grant studying what is going on in the brain when infants display novelty preferences and moved to the Institute of Child Development at the University of Minnesota to learn how to test babies and measure brain activity . I moved with him when the lab moved to Harvard Medical School in Boston, setting up a new research facility at Boston Children’s Hospital. And then, after a couple of years on the academic job market, got what I considered to be my dream job."
  },
  {
    "objectID": "posts/2025-05-09-what_happened_what_now/index.html#a-dream-job",
    "href": "posts/2025-05-09-what_happened_what_now/index.html#a-dream-job",
    "title": "what happened? what now?",
    "section": "a dream job",
    "text": "a dream job\nThere were lots of great things about this job. The location was great; who wouldn’t want to live in the eastern suburbs of Sydney, and it was the three-hour flight home to family in New Zealand. Staff had reasonable teaching loads, courses were team-taught and the honours program allowed you to mentor young research students. There was lots of infrastructure support and friendly supportive colleagues who were keen to share their experience and give you feedback.\nI was successful in gaining an ARC Discovery Project in my 2nd year on faculty and recruited 2 PhD students early on, but I found it hard to convince students to do research with infants. I thought being the only developmentalist on staff would be an advantage in attracting research students, however, most students wanted to do a PhD as a sideline to their clinical training; combining clinical placement and running studies with cranky babies was not an attractive proposition. Over time, my research became what I did in order to provide students with good training, rather than what I really wanted to do.\nAs my research success dwindled, I turned to other things that would “look good” on my next promotion application, trying to bulk up my teaching and leadership contributions to make up for an average research story. On the education front, I designed a research internship course , reproducing my undergraduate experience at Otago. I joined one of the UNSW Human Research Ethics Committees, and then when it was in a leadership crisis accepted an invitation to chair the committee . When I desperately wanted out of that, I agreed to take on the Director of Academic Programs role within the School, right before the pandemic, when it became my job to lead the School through the shift to online and back again.\nWhile the crisis management portion of role consumed most of my research time, I managed to pivot my research program toward the scholarship of teaching and learning, publishing work related to embedding self-management resources in the curriculum (2022) and integrating generative AI into assessment practice (2024). I led the School through external accreditation and internal program review processes and my leadership in the education space was acknowledged with prizes at both the faculty and university level. While I was excelling in my role and on the outside looking like I was holding it all together, even the easy parts of my job started to make me feel nauseous.\n\nRichmond, J, and J. Cranney. 2022. “Curricular Approaches to Supporting University Student Academic Success and Wellbeing.” Psychology Learning & Teaching 21 (3): 254–63.\n\nRichmond, J, and K Nicholls. 2024. “Using Generative AI to Promote Psychological, Feedback, and Artificial Intelligence Literacies in Undergraduate Psychology.” Teaching of Psychology, 00986283241287203.\nScience journalist Ed Yong spoke about burnout in a New York Times podcast and his definition really resonated with me. Burnout isn’t about how hard or stressful your job is or whether you can handle it. It isn’t even really about workload. In relation to healthcare workers and his own experience writing about COVID across the pandemic he said:\n\n\n\nScreenshot from NYT article by Ed Yong re burnout\n\n\nThis is where I got to at the end of last year.\nI could no longer handle not being able to do my job to the standard that I held for myself."
  },
  {
    "objectID": "posts/2025-05-09-what_happened_what_now/index.html#how-the-sausage-is-made",
    "href": "posts/2025-05-09-what_happened_what_now/index.html#how-the-sausage-is-made",
    "title": "what happened? what now?",
    "section": "how the sausage is made",
    "text": "how the sausage is made\nIn retrospect, I think that it was seeing “how the sausage is made” that set me on a course for burnout. In my role as HREC chair, I saw way too many examples of extremely prolific and well-respected researchers who cut corners. Researchers who have lost sight of the details, who have teams that are so big that they no longer see the work on the ground and no longer understand (or even seem to care) about the potential risk to participants.\nIn the education space, the message was “do more, with less, but better”. The obsession with improving the quality of assessment and feedback while simultaneously slashing budgets for casual marking. The lip service paid to student wellbeing, while changing late penalty policies to make them so harsh that it is impossible for a student to pass if anything out of the ordinary is happening in their life. The push to develop cash-cow programs that promise students accelerated entry to professional training, with little concern for the level of support that those students would require or how realistic the promised path even is.\nThe more I understood about the internal workings of the sausage-making machine, the more I realised that the incentives driving the machine are all wrong. The idea of getting on with my day-to-day work and being a cog in the machine made me feel nauseous.\nOf course my real job was to lead the School in initiatives designed to address the big problems we are facing in higher education, and those just felt insurmountable.\n\nWhat are we going to do about assessment?\nHow do we deal with AI?\nHow do we fix feedback?\nWhat do we do about student engagement?\n\nWhat was so confusing to me at the time, is that I like wicked problems. Put me in a room with like-minded people and a difficult task and I’m in my element. But the culture I was working in made progress on any of those wicked problems impossible."
  },
  {
    "objectID": "posts/2025-05-09-what_happened_what_now/index.html#change-fatigue-status-quo-bias",
    "href": "posts/2025-05-09-what_happened_what_now/index.html#change-fatigue-status-quo-bias",
    "title": "what happened? what now?",
    "section": "change fatigue & status quo bias",
    "text": "change fatigue & status quo bias\nChange in universities is notoriously poorly managed and leaders in higher education underestimate how damaging change fatigue is to organisational culture. Those in the Chancellery roll out new calendars, learning management systems, digital assessment tools, assessment implementation procedures, governance record platforms, course outline systems, and program review processes with little consultation or thought given to the workload implications on the ground. Staff are only just keeping their head above water, swimming in an ocean of administration and governance updates. As a result, the majority of academics literally do not have the capacity to make meaningful changes to their education practice that would be impactful to students, because their appetite for change has been wasted and their cynicism has been stoked.\nThe culture of higher education is riddled with status quo bias. When we are all back on campus, but students don’t show up to class, staff complain but never question whether they should change what they are offering. When generative AI tools pose a massive threat to the integrity of the assessments they have assigned forever, they deny there is anything wrong with their assignments. When opportunities like programmatic assessment or competency-based grading are floated as ideas for solving assessment and feedback challenges they are quick to say no. The status quo bias, in combination with a subtle but pervasive attitude that research is more important than education, made higher ed no longer feel like a place in which I do my job.\nI had hung on to my train, navigated the hurdles, never questioning whether the destination was going to be a “good place”. I got to the destination (A/Prof at a GO8 university, award-winning leader in education) and it ate me alive."
  },
  {
    "objectID": "posts/2025-05-09-what_happened_what_now/index.html#what-now",
    "href": "posts/2025-05-09-what_happened_what_now/index.html#what-now",
    "title": "what happened? what now?",
    "section": "what now?",
    "text": "what now?\nAnd so I quit.\nPeople didn’t believe me at first. Why don’t you just take some leave, regroup, and see how you feel about it, they said. What they didn’t realize is that I had found myself in a place where I felt like I didn’t belong. I was languishing, and when I finally took sick leave, I actually got really sick.\nNow that I have resigned, I’m working on undoing the conditioning of the academic train. What do I want work to feel like? What do I WANT to spend my time doing? Not what do I think I SHOULD be doing or what would look good for the next step, but what do I want? In the last few weeks I’m slightly closer to an idea of what the next steps might be. I am fighting the languishing and working out what is next by seeking flow.\n\nprioritising flow\nWhen was the last time you lost track of time? Were in “the zone”? Forgot to eat lunch?\nWhat were you doing?\nIn his 3-2-1 newsletter a few weeks ago, James Clear asked what would you do if your job involved only the activity that most frequently resulted in a sense of flow.\n\n\n\nquestion about flow from James Clear 3-2-1 newsletter (2025-02-27)\n\n\nAdam Grant has also talked about flow as an antidote to languishing, arguing that a sense of flow is key to wellbeing and depends on mastery, mindfulness, and mattering.\nPlaying with data ticks these boxes for me. In the last few weeks, I revamped the RLadies Sydney website, moving the #RYouWithMe materials into Quarto. I’ve done the same with my own website and made a data viz portfolio via the #30DayChartChallenge. The only thing motivating me to write a resume is that I know there’s going to be a cool way to convert my CV into a fancy output using R.\nI know I would be sad if my next step didn’t involve data. I like creating things, sharing knowledge, seeing a gap and working out what I could make to fill it. I want to spend time telling stories with data and making things, but that is as far as I have got.\n\n\nrebuilding identity\nBut the shame is real. People don’t just get off the train. Sometimes they are thrown against their will, but it’s really uncommon for academics to say “stop the train please, I would like to get off”.\nBrene Brown talks about shame only surviving in the presence of secrecy, silence and judgement. It dies when someone shines light on it. So here we are, learning out loud and shining a light.\nI am constantly reminding myself that it is ok to not know what is next. I do know that the transition will involve rebuilding my identity. The train has drilled into me that “I am what I do” for a long time so it is not surprising that I don’t really know who I am if not an academic. In talking about identity with my coach last week, she asked which components of your identity do you want to swap out for new things?\nThe only component I could think of that I want to swap was people pleaser. The other pieces (the critical analyser, thought leader, data whiz, community builder, communicator, synthesis queen) - those components can all stay. But I need to take apart the jigsaw, reshape each piece, and slot them back all together again. That will take time, but I know it will slot back together stronger.\nThanks for reading. I would love to hear from you if data -&gt; flow for you too.\n\n\n\nrebuilding my identity"
  },
  {
    "objectID": "posts/2021-06-22-pat-for-github/index.html",
    "href": "posts/2021-06-22-pat-for-github/index.html",
    "title": "PAT for GitHub",
    "section": "",
    "text": "I have been avoiding setting up a Personal Access Token to authenticate my github account for a while because it seemed complicated. Never fear Happy Git with R and usethis() to the rescue.\n\ncreate a token\nIn the RStudio console, install the usethis package if you don’t already have it and use it to create a github token.\n\n#install.packages(\"usethis\")\n\nlibrary(usethis)\n\nusethis::create_github_token()\n\nThis will take you to github and make a token for you. Accept the defaults and copy the token.\nGo back to the RStudio console. Install the gitcreds package, if you don’t already have it and then use the gitcreds_set function to tell R you want to set github credentials.\n\n\nstore it in your credentials\n\n# install.packages(\"gitcreds\")\n\nlibrary(gitcreds)\n\ngitcreds_set(url = \"https://github.com\")\n\nThen it will ask you to Enter password or token…paste into your console.\n\n\ncheck that it works\nCheck that it works, using the gitcreds_get() function.\n\ngitcreds_get()"
  },
  {
    "objectID": "posts/2022-01-16-useful-bash-commands/index.html",
    "href": "posts/2022-01-16-useful-bash-commands/index.html",
    "title": "useful bash commands",
    "section": "",
    "text": "I remember working in a cafe in Melbourne with Charles Gray a couple of years ago and watching in awe as she navigated her way around our project files using the terminal and command line. I thought, wow… I’d like to learn how to do that. Some time later, I took a Software Carpentry Unix course at ResBaz and none of it sunk in. Much like learning other programming concepts, I think you need to have a reason.\nI created a reason for myself this week when I discovered that in briefly putting my blog project in my icloud, I had inadvertently created all kinds of hidden files (i.e. files that start with a dot) and files with weird .icloud extensions that wouldn’t show up or open in RStudio.\n\nTAKE HOME- your blog is on github, it isn’t going to get lost. There is no need to have it live in another cloud based place (icloud, onedrive, dropbox).\n\nIn trying to rename these files in bulk, I learned some useful bash commands. You can use these in the terminal within RStudio, or in a Terminal window via Utilities.\nWorking in the terminal depends heavily on knowing where you are on your computer. Once you know which directory you are in, you can use cd commands to navigate.\n\npwd # print working directory\n\ncd folder/subfolder # change directory from where you are\n\ncd ~ # navigate back to home directory\n\ncd .. # navigate up a folder level\n \ncd - # navigate to previous directory\n\nTo print all of the folders/files in a directory, use ls commands.\n\nls # print list of subdirectories/files\nls -a # print everything, even hidden files\n\nTo remove files, use rm commands\n\nrm nameoffile.txt # remove a file\nrm -v # remove a directory\n\nYou can find files that meet certain criteria too. The dot stands in for this directory. Here I am looking for all files with the ext .icloud\nstackoverflow thread\n\nfind . -name \"*.icloud\"\n\nThe rename package seems to be something that you need to use brew to install, but once you have it, you can easily rename things. Here we are renaming all .jpeg files with .jpg.\nhowtogeek post\n\nrename .jpeg .jpg *.jpeg\n\nYou can also rename other parts of filenames with this syntax. I had a lot of hidden files that started with a dot. This renames the .index.Rmd files as index.Rmd\n\nrename 's/.index/index' *.Rmd\n\nI also had lots of hidden files with .Rmd.icloud extensions. Here we are looking for files that start with .index and replacing the .icloud ext with noting (i.e. blank second //)\n\nrename 's/.icloud//' .index*\n\nFixing rid of hidden files starting with, - this resource was useful\nTHIS prints what it will do…\n-depth -name \".*\" -exec rename -n 's|(.*/)\\.(.*)|$1$2|' {} +\nRemove the -n to get it to run\n-depth -name \".*\" -exec rename 's|(.*/)\\.(.*)|$1$2|' {} +\nUltimately these bash did what I wanted (rename the hidden files and remove the weird extensions) but it didn’t solve my problem. I managed to rename the files and then they still wouldn’t open, so had to copy them back in from another back up… but it was a useful bash exploration anyway!"
  },
  {
    "objectID": "posts/2022-09-28-git-hints/index.html",
    "href": "posts/2022-09-28-git-hints/index.html",
    "title": "git hints",
    "section": "",
    "text": "I am getting pretty good at avoiding git merge conflicts by always remembering to pull before I push when using github. But the terminal in RStudio has been giving me this hint for a while and I have been ignoring it.\n\nI think this hint is trying to help me but I don’t know what rebase or fast-forward means… to google…\nThis post suggests that I should only use git pull -ff, going as far as setting -ff as a global config setting.\nApparently git pull is actually a two step thing (git fetch + git merge). First, it fetches the content from the remote and then it merges that content into your local. That is USUALLY fine… especially if you are working alone and nothing has changed about your remote repo since the last time you pulled. In this case, your local will be ahead of your remote and git pull will be fine.\nBut if you are using git to collaborate and your team member has pushed some changes to the files you are working on, you can find yourself with a merge conflict.\nThe git pull –ff-only command will fetch and merge content from the remote repo, only if it can be done be done by “fast-fowarding”- that is, without creating new commits. This will only happen if the remote and local haven’t diverged. If they have it will give you a warning.\nMy Twitter friends think that in my use case, when I am mostly working with myself and the remote is unlikely to have diverged from the local, the default option will work fine.\nYay for #rstats twitter!\n\n\nAnyone #rstats know of a helpful rebase vs. fastforward explainer? Which of these options I should choose? My use case is simpler than most. I am not using branches or worried about other people pushing to my repo– just blog/quarto slides to github… thanks #rstats! pic.twitter.com/FAaBLJKAqw\n\n— Jen Richmond @jenrichmondPhD@mastodon.soc (@JenRichmondPhD) October 3, 2022"
  },
  {
    "objectID": "posts/2022-06-27-using-lists-in-r/index.html",
    "href": "posts/2022-06-27-using-lists-in-r/index.html",
    "title": "using lists in R",
    "section": "",
    "text": "One of my goals while on long service leave is to learn some new R things that have been on my radar for a while… the first of these is purrr. The purrr package allows you to iterate a function across different elements in a list or dataframe.\nI have started to try and learn purr before (see a list of resources here). I have copied other people’s purrr code a couple of times too…\n\nhow to use map to read in a LOT of .csv files and\nusing pwalk to output plots in different colour schemes\n\n… but when you copy purrr code from someone else and adjust it to suit you own problem… you can’t really say you know how to use purrr.\nThe first thing I think I need to get my head around in order to understand purrr is lists. Dataframes are the bread and butter of the tidyverse and up until now I have avoided them, or tried desperately to use as.dataframe() or unnest() to turn them into a data structure that i understand. Lists allow you to bundle together different kinds of data elements together, so now is the time to get my head around them.\n\nhow to make a list\nThis is a example I copied from a tutorial\nmyfirstlist &lt;-  list(2, \n                     \"hello\", \n                     c(3,5,4), \n                     1:5, \n                     list(FALSE, \n                          c(\"this\", \"is\",\"a\",\"list\"),\n                          c(FALSE,TRUE,TRUE,TRUE,FALSE))) \nThe list() function lets you put elements of all different types (and lengths) into a listy bundle; characters and numbers and logicals. One of the elements in this list of 5 items when others have 3 items or even 1. Another item is ANOTHER list made up of 3 items. Lists within lists— eeeekk. We can use the class() function to check that our list is a list and the str() function to get our head around what we are dealing with.\nclass(myfirstlist)\n## [1] \"list\"\nstr(myfirstlist)\n## List of 5\n##  $ : num 2\n##  $ : chr \"hello\"\n##  $ : num [1:3] 3 5 4\n##  $ : int [1:5] 1 2 3 4 5\n##  $ :List of 3\n##   ..$ : logi FALSE\n##   ..$ : chr [1:4] \"this\" \"is\" \"a\" \"list\"\n##   ..$ : logi [1:5] FALSE TRUE TRUE TRUE FALSE\nThis iconic image (which I think comes from a Jenny Bryan talk) is supposed to help me understand how to access elements of a list.\nLets see if I can unpack it.\n TBC…"
  },
  {
    "objectID": "posts/2025-04-18_closeread/index.html",
    "href": "posts/2025-04-18_closeread/index.html",
    "title": "Pūteketeke Pandemonium",
    "section": "",
    "text": "This post is my first attempt at a closeread scrollytelling story. It tells the story of the Pūteketeke controversy that plagued the Forest and Bird NZ Bird of the Century competition in 2023.\nThe closeread extension makes it easy to turn a Quarto document into a scrollytelling story like this one. The closeread documentation and this tutorial by Gaston Sanchez were helpful in making this story.\n\n\n\n\nThis bird is called a Pūteketeke\n\n\n\n\nIt is also known as the Australasian Crested Grebe.\n\n\n\n\nThere are fewer than 1000 Pūteketeke in New Zealand.\n\n\n\n\nThey are known for unique courtship behaviour…\n\n\n\n\n… and for being good parents\n\n\n\n\nIn my hometown, Wanaka NZ, the humans make the Pūteketeke floating platforms to build their nests on. Read about the Wanaka Grebe Project here.\n\n\n\n\nIn 2023, the Pūteketeke went global.\n\n\n\n\nBillboards appeared in cities across the world.\n\n\n\n\nFrom Wellington NZ …\n\n\n\n\n… to Paris, France\n\n\n\n\n… to Manitowoc, Wisconsin\n\n\n\n\nHow did the Pūteketeke get this kind of international attention??\n\n\n\n\nIn 2023 Forest and Bird New Zealand was celebrating its centenary and had dubbed its annual competition “Bird of the Century”.\n\n\n\n\nEach year Forest and Bird invites New Zealanders to vote for their favourite native bird.\n\n\n\n\nAnyone with a valid email address can enter.\n\n\n\n\nIn 2023, comedian John Oliver heard about the competition and threw his resources behind a campaign in support of the Pūteketeke.\n\n\n\n\nJohn Oliver himself described the campaign as “alarmingly aggressive”\n\n\n\n\nJohn even appeared on Jimmy Fallon’s show in a Pūteketeke costume\n\n\n\n\nThis is not the first time that the Forest and Bird competition has attracted controversy.\n\n\n\n\nIn 2021, the Long-tailed bat (Pekapeka-tou-roa) won the Bird of the Year, even though it is definitely not a bird.\n\n\n\n\nThe Long-tailed bat (Pekapeka-tou-roa) is New Zealand’s only native land mammal and caused a bit of a stir when it was voted as New Zealand’s favourite bird in 2021.\n\n\n\n\nThe results in 2022 caused less furor.\n\n\n\n\nWhile the Little Penguin / Kororā technically received the most #1 votes…\n\n\n\n\n…it was pipped at the post by the New Zealand Rock Wren / Pīwauwau…\n\n\n\n\n…a tiny alpine bird that lives in the mountains of the South Island.\n\n\n\n\nIn 2023,the result of the John Oliver’s campaign was a resounding win for the Pūteketeke (more than 290K votes!) and a boost in donations to Forest and Bird.\n\n\n\n\nInterest in the Forest and Bird competition carried over into 2024, with vote rates higher than average.\n\n\n\n\nLast year, the Yellow-eyed Penguin / Hoiho prevailed.\n\n\n\n\nCheck out the Forest and Bird website to read about past winners, donate to conservation efforts, and vote for your favourite New Zealand bird in the 2025 competition (voting opens in September).\n\n\n\n\n\n\n\n\n\nPūteketeke\n\n\n\n\n\n\n\nPūteketeke courtship behaviour\n\n\n\n\n\n\n\nPūteketeke parent and chick\n\n\n\n\n\n\n\nPūteketeke platform\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nPekapeka-tou-roa, Bird of the Year 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKororā, Bird of the Year runner up 2022\n\n\n\n\n\n\n\nPīwauwau, Bird of the Year 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHoiho, Bird of the Year 2024\n\n\n\n\n\n\n\nMade with Quarto"
  },
  {
    "objectID": "posts/2022-12-15-colours-that-r-knows/index.html",
    "href": "posts/2022-12-15-colours-that-r-knows/index.html",
    "title": "colours that R knows",
    "section": "",
    "text": "I have been working through the ggplot R Advent calendar by Kiirsti Owen with some lovely RLadies friends and we got up to Day 15 where we started controlling colours in ggplot with scale_fill_manual(). Our immediate question was “how to you know what the names of the colours that R knows are?”\nThis is a “I don’t have to google” post about finding the colours that R knows about.\n\nread the data\n\nlibrary(tidyverse)\n\ntrees &lt;-read_csv(\"https://raw.githubusercontent.com/kiirsti/ggplot_adventcalendaR/main/xmas.trees.csv\")\n\n\n\nmake a plot\n\ntrees %&gt;%\nggplot(aes(x=type, y=height))+\n  geom_boxplot(aes(fill=type), colour=\"black\")+\n  theme_classic()+\n  scale_fill_manual(values=c(\"darkgreen\", \"firebrick2\", \"mediumseagreen\"))\n\n\n\n\n\n\n\n\nYou can have R list the names of all the colours it knows (there are 657 of them) using the colours() function, but that is not so useful if you want to see the difference between aquamarine1 and aquamarine2.\n\n# list the first 20 colours that R knows \n\nhead(colours(), 20)\n\n [1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n [5] \"antiquewhite2\" \"antiquewhite3\" \"antiquewhite4\" \"aquamarine\"   \n [9] \"aquamarine1\"   \"aquamarine2\"   \"aquamarine3\"   \"aquamarine4\"  \n[13] \"azure\"         \"azure1\"        \"azure2\"        \"azure3\"       \n[17] \"azure4\"        \"beige\"         \"bisque\"        \"bisque1\"      \n\n\nWe eventually found a function in the epitools package that will display all the colours and allow you to point a click the ones you want! It doesn’t work so well in an Rmd chunk- you are best to try it in the console.\n\n# install.packages(\"epitools\")\n\nlibrary(epitools)\n\ncolors.plot(locator = TRUE)\n\n\n\n\n\n\n\n\n[1] color.names\n&lt;0 rows&gt; (or 0-length row.names)\n\n\nLoad the epitools package and then use the colors.plot() function in the console, setting locator = TRUE. A matrix will appear in your Plots tab. You can use your mouse to pick the colours you want and then click Finish to have R print the names of those colours to your console. Watch me do it in the screencast below.\n\n\nYou can then use those names to revise your plot colours.\n\ntrees %&gt;%\nggplot(aes(x=type, y=height))+\n  geom_boxplot(aes(fill=type), colour=\"black\")+\n  theme_classic()+\n  scale_fill_manual(values=c(\"seagreen\", \"maroon2\", \"dodgerblue2\"))"
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html",
    "title": "my favourite things about R",
    "section": "",
    "text": "I am prepping a talk for R-Ladies Sydney about my favourite R things, including the packages and functions that end up in every script I write.\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(ggeasy)\nlibrary(palmerpenguins)\nlibrary(naniar)\nlibrary(gt)"
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html#herehere",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html#herehere",
    "title": "my favourite things about R",
    "section": "here::here()",
    "text": "here::here()\nThe here package makes dealing with file paths and telling R where your work lives really easy. If you work within a R project (always recommended), here::here() defaults to the top level of your project folder. You can refer to everything relative to there and use quotes to specify folder levels. Today I am writing in my website project, so here tells me that I am currently…\n\nhere::here()\n\n[1] \"/Users/jenrichmond/Dropbox/ALL_R_stuff/2. WEBSITES/jenrichmond.github.io\"\n\n\nWhen I want to read in some data, I can refer to the location of that data relative to this starting point. In my blogdown site, I’ve put the data in the folder for this particular post (within content/blog/). The nice thing about referring to the location of things relative to the top level of your project, is that it doesn’t matter if you are working in Rmd or R script, on the computer that you wrote the code on or another one, the path doesn’t change.\n\npractice_penguins &lt;- read_csv(here(\"posts\", \"2022-01-17-my-favourite-things-about-r\", \"practice_penguins.csv\"))"
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html#janitorclean_names",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html#janitorclean_names",
    "title": "my favourite things about R",
    "section": "janitor::clean_names()",
    "text": "janitor::clean_names()\nI messed up the penguin data to make the variable names a bit ugly so I could demo my favourite function, clean_names(). So often little thought is put into naming conventions at the time of data entry and it is really common to be given a dataset that has really longwinded and inconsistently formatted variable names.\n\nnames(practice_penguins)\n\n[1] \"Species\"        \"island\"         \"Bill length\"    \"bill_depth\"    \n[5] \"flipper length\" \"Body_Mass\"      \"Sex\"            \"year\"          \n\n\nIn this case there is a mix of upper and lower case, some gaps between words, some underscores. When you are coding, you need to type the names of variables a lot, so it can save you lots of time to make the variable names consistent… enter clean_names()\n\nclean_penguins &lt;- practice_penguins %&gt;%\n  clean_names()\n\nnames(clean_penguins)\n\n[1] \"species\"        \"island\"         \"bill_length\"    \"bill_depth\"    \n[5] \"flipper_length\" \"body_mass\"      \"sex\"            \"year\"          \n\n\nIn one line of code, everything is lower case with underscores in the gaps (aka snake case)."
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html#janitortabyl",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html#janitortabyl",
    "title": "my favourite things about R",
    "section": "janitor::tabyl()",
    "text": "janitor::tabyl()\nOften the first thing you want to do in R is count how many observations you have of different type. The tabyl() function from janitor works much like the count() function, but the output is more concise and user friendly and includes percentages automatically.\n\nclean_penguins %&gt;%\n  tabyl(species) \n\n   species   n   percent\n    Adelie 152 0.4418605\n Chinstrap  68 0.1976744\n    Gentoo 124 0.3604651\n\n\nYou can count just one variable, or get something a bit like a cross tab with two. There are a series of adorn_ functions that also allow you to add totals.\n\nclean_penguins %&gt;%\n  tabyl(species, sex) %&gt;%\n  adorn_totals() \n\n   species female male NA_\n    Adelie     73   73   6\n Chinstrap     34   34   0\n    Gentoo     58   61   5\n     Total    165  168  11\n\n\nYou can assign the output to a dataframe or pipe into gt() to get a nice looking rendered output.\n\nclean_penguins %&gt;%\n  tabyl(species, sex) %&gt;%\n  adorn_totals() %&gt;%\n  gt()\n\n\n\n\n\n\n\nspecies\nfemale\nmale\nNA_\n\n\n\n\nAdelie\n73\n73\n6\n\n\nChinstrap\n34\n34\n0\n\n\nGentoo\n58\n61\n5\n\n\nTotal\n165\n168\n11"
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html#naniarvis_miss",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html#naniarvis_miss",
    "title": "my favourite things about R",
    "section": "naniar::vis_miss",
    "text": "naniar::vis_miss\nSometimes you know there is missing data but it can be difficult to know where it is or what to do about it. The vis_miss() function from the naniar` package helps you see where the missing values are so you can better decide what to do with them.\n\nnaniar::vis_miss(clean_penguins)"
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html#tidyrpivot_longer",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html#tidyrpivot_longer",
    "title": "my favourite things about R",
    "section": "tidyr::pivot_longer()",
    "text": "tidyr::pivot_longer()\nWhen we enter data it is usually in wide format. This is problematic when you want to use ggplot, which expects your data to be long. The new pivot functions from tidyr make it really easy to switch your data from wide to long (and back again if you need). Here I am selecting just species and the two variables that start with “bill” to make a smaller demo dataset.\n\npenguin_bill &lt;- clean_penguins %&gt;%\n  select(species, starts_with(\"bill\"))\n\nglimpse(penguin_bill)\n\nRows: 344\nColumns: 3\n$ species     &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\"…\n$ bill_length &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, …\n$ bill_depth  &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, …\n\n\nTechnically this bill data is in wide format (it is not the best example but lets run with it). The two columns contain bill measurements, about two different parts of the penguin bill. We could represent this data in long format by making a new column that contained info about which part of the bill we were measuring, and another column with the measurement value.\nThe pivot_longer() function asks you to specify what you want to call the column that will contain what is currently in the variable names (i.e. names_to), what you want to call the column that will contain the values (i.e. values to) and the range of columns that are currently wide that you want to be long.\n\nlong_bill &lt;- penguin_bill %&gt;%\n  pivot_longer(names_to = \"bill_part\", \n               values_to = \"measurement\",   bill_length:bill_depth)\n\nhead(long_bill)\n\n# A tibble: 6 × 3\n  species bill_part   measurement\n  &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt;\n1 Adelie  bill_length        39.1\n2 Adelie  bill_depth         18.7\n3 Adelie  bill_length        39.5\n4 Adelie  bill_depth         17.4\n5 Adelie  bill_length        40.3\n6 Adelie  bill_depth         18"
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html#ggeasyeasy_remove_legend",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html#ggeasyeasy_remove_legend",
    "title": "my favourite things about R",
    "section": "ggeasy::easy_remove_legend()",
    "text": "ggeasy::easy_remove_legend()\nOnce you have your head around how to construct figures in ggplot, you can spend a lot of time googling how to customise it. The ggeasy package contains a whole lot of easy to use wrappers for really common ggplot adjustments. Like removing the legend…the code to remove the legend is p + theme(legend.position = \"none\") … or you can use ggeasy::easy_remove_legend()\n\nlong_bill %&gt;% \n  ggplot(aes(x = species, y = measurement, colour = species)) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  facet_wrap(~ bill_part) +\n  easy_remove_legend()"
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html#dplyrcase_when",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html#dplyrcase_when",
    "title": "my favourite things about R",
    "section": "dplyr::case_when()",
    "text": "dplyr::case_when()\nSometimes you need to compute a new variable based on values in other variables, case_when() is your friend. Lets say we were interested in which penguins have extremely long or short bills. Here I am filtering for just the Gentoo penguins, and calculating the mean and sd for bill length. Then I am using mutate() to make a new variable and case_when() to flag values of bill length than are more than 2sd greater than the mean as “long” and values of bill length that are more than 2sd below the mean as short. The TRUE ~ “ordinary”, puts ordinary in the cells that don’t meet those criteria.\nThen we can use tabyl() to count how many penguins have extraordinarily long or short bills.\n\ngentoo &lt;- clean_penguins %&gt;%\n  filter(species == \"Gentoo\") %&gt;%\n  select(species, bill_length, sex)\n\nmean_length &lt;- mean(gentoo$bill_length, na.rm = TRUE)\nsd_length &lt;- sd(gentoo$bill_length, na.rm = TRUE)\n\ngentoo &lt;- gentoo %&gt;%\n  mutate(long_short = case_when(bill_length &gt; mean_length + 2*sd_length ~ \"long\", \n                         bill_length &lt; mean_length - 2*sd_length ~ \"short\",\n                         TRUE ~ \"ordinary\"))\n\ngentoo %&gt;% tabyl(long_short)\n\n long_short   n     percent\n       long   4 0.032258065\n   ordinary 119 0.959677419\n      short   1 0.008064516"
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html#dplyrrelocate",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html#dplyrrelocate",
    "title": "my favourite things about R",
    "section": "dplyr::relocate()",
    "text": "dplyr::relocate()\nWhen using mutate() to make a new variable, the default is to add it to the right side of the dataframe. With small datasets that is ok, but when you have lots of variables and you want to check whether the mutate has done what you want, it can be annoying. There is a relatively new function in dplyr that allows you to relocate a variable. Here I am moving the long_short variable we just made to the position after bill_length.\n\ngentoo &lt;- gentoo %&gt;%\n  relocate(long_short, .after = bill_length)\n\nglimpse(gentoo)\n\nRows: 124\nColumns: 4\n$ species     &lt;chr&gt; \"Gentoo\", \"Gentoo\", \"Gentoo\", \"Gentoo\", \"Gentoo\", \"Gentoo\"…\n$ bill_length &lt;dbl&gt; 46.1, 50.0, 48.7, 50.0, 47.6, 46.5, 45.4, 46.7, 43.3, 46.8…\n$ long_short  &lt;chr&gt; \"ordinary\", \"ordinary\", \"ordinary\", \"ordinary\", \"ordinary\"…\n$ sex         &lt;chr&gt; \"female\", \"male\", \"female\", \"male\", \"male\", \"female\", \"fem…\n\n\nLets make a plot to illustrate the variability in Gentoo penguins bill length.\n\ngentoo %&gt;%\n  ggplot(aes(x = sex, y = bill_length, colour = long_short)) + \n  geom_jitter(width = 0.2)"
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html#knitr-options-fig.path",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html#knitr-options-fig.path",
    "title": "my favourite things about R",
    "section": "knitr options fig.path =",
    "text": "knitr options fig.path =\nYou might have noticed that in the default Rmd template there is a chunk at the top that controls how your document knits. The default knit settings have echo = TRUE which makes your code appear in your knitted document along with your output. But you can add other knit settings.\n\n\n\n\n\n\n\n\n\nYou can add fig.width, fig.height, and fig.path to control how big your plots appear in your knitted document. You can also add fig.path to have your plots be rendered in png format to a folder within your project. And if you want all the ggplots in your document to be the same theme, you can add that as a default."
  },
  {
    "objectID": "posts/2022-01-17-my-favourite-things-about-r/index.html#readrwrite_csv",
    "href": "posts/2022-01-17-my-favourite-things-about-r/index.html#readrwrite_csv",
    "title": "my favourite things about R",
    "section": "readr::write_csv()",
    "text": "readr::write_csv()\nMy data analysis process often involves reading in raw data, cleaning it up, and then writing it out to csv so that you can read the clean data in to another process (visualisation, modelling). I use write_csv() and here::here() to write out a csv that can then be used in a different script.\n\ngentoo %&gt;%\n  write_csv(\"clean_gentoo.csv\")"
  },
  {
    "objectID": "posts/2025-05-27_tt_monsters/index.html",
    "href": "posts/2025-05-27_tt_monsters/index.html",
    "title": "dungeons and dragons monsters",
    "section": "",
    "text": "This week for TidyTuesday I decided to look at dragons and practice writing a ggplot function that makes it easy to create the same plot for each level of a variable. In this case, I wanted a function that would spit me out a boxplot comparing the different dragon types on each of the key measures (strength, intelligence, wisdom, charisma, dexerity, and constitution)."
  },
  {
    "objectID": "posts/2025-05-27_tt_monsters/index.html#theme_set",
    "href": "posts/2025-05-27_tt_monsters/index.html#theme_set",
    "title": "dungeons and dragons monsters",
    "section": "1. theme_set()",
    "text": "1. theme_set()\nYou can make a custom ggplot theme and then automatically use it for all the plots in your Quarto Document using theme_set(). In this case, I called my theme theme_dragons()\n\ntheme_set(theme_dragons())"
  },
  {
    "objectID": "posts/2025-05-27_tt_monsters/index.html#plot_annotation",
    "href": "posts/2025-05-27_tt_monsters/index.html#plot_annotation",
    "title": "dungeons and dragons monsters",
    "section": "2. plot_annotation",
    "text": "2. plot_annotation\nWhen using the patchwork package to combine plots you can use plot_annotation() to add titles etc to your combined plot. You can also use theme() functions, in much the same way you would for ggplot to control your title, margins or background. See patchwork vignette.\n\nplot1 + plot2 +\n    plot_annotation(title = \"My title\",\n              theme = \n                theme(plot.background = element_rect(fill = \"ghostwhite\"),\n                      plot.margin = margin(20, 20, 20, 20),  # T, R, B, L \n                      plot.title = element_text(hjust = 0, # align left\n                                  size = 16,  color = \"black\",  \n                      margin = margin(b = 5))  # spacing below title\n    )\n  )"
  },
  {
    "objectID": "posts/2025-05-27_tt_monsters/index.html#how-to-reorder-x-axis-in-ggplot",
    "href": "posts/2025-05-27_tt_monsters/index.html#how-to-reorder-x-axis-in-ggplot",
    "title": "dungeons and dragons monsters",
    "section": "3. how to reorder x axis in ggplot",
    "text": "3. how to reorder x axis in ggplot\ncoorid_flip() is handy when you want x axis labels to display more clearly, but it is a bit aanoying that the order L-R gets translated B-T. I played for a little bit working out how to use fct_rev() to make the dragons display in alphabetical order from top to bottom, before deciding that ordering the bars by the mean_score would make for a more meaningful plot. Here is the code for each of those options in a panel-tabset.\n\ndefaultcoord_flipfct_revreorder\n\n\n\ndragons_long %&gt;% \n  filter(index == \"str\") %&gt;%\n    ggplot(aes(x = category, y = score)) +\n   geom_boxplot(fill = \"lavender\") \n\n\n\n\n\n\n\n\n\n\n\ndragons_long %&gt;% \n  filter(index == \"str\") %&gt;%\n    ggplot(aes(x = category, y = score)) +\n   geom_boxplot(fill = \"lavender\")  +\n  coord_flip() \n\n\n\n\n\n\n\n\n\n\n\ndragons_long %&gt;% \n  filter(index == \"str\") %&gt;%\n    ggplot(aes(x = fct_rev(category), y = score)) +\n  geom_boxplot(fill = \"lavender\") +\n    coord_flip() \n\n\n\n\n\n\n\n\n\n\n\ndragons_long %&gt;% \n  filter(index == \"str\") %&gt;%\n    ggplot(aes(x = reorder(category,mean_score), y = score)) +\n geom_boxplot(fill = \"lavender\") +    \n  coord_flip()"
  },
  {
    "objectID": "posts/2021-08-31-parameterised-penguins/index.html",
    "href": "posts/2021-08-31-parameterised-penguins/index.html",
    "title": "parameterised penguins",
    "section": "",
    "text": "I am beginning a project where we want to create a distill site that has lots of profile pages that all have the same format but pull different data from a dataframe. Here I am learning how to use set up parameters within RMarkdown so that I can use purrr to render many reports at once.\n\nhow to customise your yaml\nyaml stands for “yet another markup language” and it is the code that you see at the top of your RMarkdown document that is between the sets of dashes.\nWhen you open a new Rmd document, the yaml just contains fields to insert the title, author, and date that your document was created and some info about what kind of output your want your Rmd to knit to, but you can customise the yaml to specify lots of different characteristics of your Rmd document.\n\n\n\n\n\n\n\n\n\nIn this yaml, I have set the output to be html and added the hpstr theme from the prettydoc package. I have also set how big I want the figures to be in my knitted document. Other options include adding a table of contents and allowing readers to show and hide code see details in the Yihui’s rmarkdown book here.\nThe new piece here is adding “params” (more about Rmd/params here. Eventually I want to create a separate report for each type of penguin species so I have set the parameter I am interested in as my_species. As a first run, I am interested in Adelie penguins.\n\n\nhow to write a basic report\nBefore I can render lots of reports, I need to write the code and make it run on the Adelie species. The first chunk of code in my report loads the packages needed, sets my default ggplot theme and loads in the penguin data. Then I use the species params that I set in the yaml to filter the penguins for only Adelie, creating a new species dataframe.\n\n\n\n\n\n\n\n\n\nThe next chunk uses that species dataframe to make a plot of body mass by flipper length. I can also refer to the params I set to make the level 1 header and subtitle of my plot reflect which species of penguin are being plotted.\n\n\n\n\n\n\n\n\n\nWhen I knit the document I end up with a pretty nice looking report about Adelie penguins see RPubs here.\n\nhow to render many reports\nNow that my report is working for one level of species and I have set params within my yaml, I can use purrr to automagically render a separate report for each species in the penguins dataset. I start by opening a new .R script and making a tibble that has three variables…\n\nspecies (the unique values of species from the penguin data)\nfilename (a combo of penguin-report- and each of the species with .html extension)\nparams (map across each levels of species)\n\n\n\n\n\n\n\n\n\n\nThen I take that tibble and pipe it into a select() function, selecting just the filename (which is renamed as output_file at the same time) and params. Pipe those two variables into pwalk(), which renders the parameterised_penguins.Rmd for each level of the params. When I run the code in my knit_many_reports.R script, I end up with 3 x .html reports, one for each species, in my files tab. Magic!!\n\n\n\n\n\n\n\n\n\n\nnote: I am still getting my head around how purrr works, but my understanding is that walk() is a lot like map(), in that it iterates over a list of things, but it is useful when you want to keep the output of the code you are iterating (i.e. generating reports). I have used it before here to test out different ggplot palettes. pwalk() is like walk() except that there are multiple arguments, in this case both the output_file and each of the params."
  },
  {
    "objectID": "posts/2021-01-08-my-first-pull-request/index.html",
    "href": "posts/2021-01-08-my-first-pull-request/index.html",
    "title": "my first pull request",
    "section": "",
    "text": "There are so many lovely packages for colour palettes (see links at the bottom of this post for more).\nThis new one called feathers by Shandiya Balasubramanium is inspired by Australian birds and is delightful.\nThe vignette doesn’t have any pictures of the birds though, so I thought it might be a good opportunity to help Shandiya out and try my first pull request. I am documenting the process here, so I can refer back to it for next time."
  },
  {
    "objectID": "posts/2021-01-08-my-first-pull-request/index.html#eastern-rosella",
    "href": "posts/2021-01-08-my-first-pull-request/index.html#eastern-rosella",
    "title": "my first pull request",
    "section": "Eastern Rosella",
    "text": "Eastern Rosella\n\n\n\n\n\n\n\n\n\nImage credit: JJ Harrison. This file is licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license. Image source: Wikimedia Commons."
  },
  {
    "objectID": "posts/2021-01-08-my-first-pull-request/index.html#plains-wanderer",
    "href": "posts/2021-01-08-my-first-pull-request/index.html#plains-wanderer",
    "title": "my first pull request",
    "section": "Plains wanderer",
    "text": "Plains wanderer\n\n\n\n\n\n\n\n\n\nImage credit: Patrick_K59. This file is licensed under the CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=34831381. Image source: Wikimedia Commons."
  },
  {
    "objectID": "posts/2021-01-08-my-first-pull-request/index.html#rose-crowned-fruit-dove",
    "href": "posts/2021-01-08-my-first-pull-request/index.html#rose-crowned-fruit-dove",
    "title": "my first pull request",
    "section": "Rose crowned fruit dove",
    "text": "Rose crowned fruit dove\n\n\n\n\n\n\n\n\n\nImage credit: Bjørn Christian Tørrissen, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=6874281"
  },
  {
    "objectID": "posts/2021-01-08-my-first-pull-request/index.html#bee-eater",
    "href": "posts/2021-01-08-my-first-pull-request/index.html#bee-eater",
    "title": "my first pull request",
    "section": "Bee eater",
    "text": "Bee eater\n\n\n\n\n\n\n\n\n\nImage credit: JJ Harrison. This file is licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license. Image source: Wikimedia Commons."
  },
  {
    "objectID": "posts/2021-01-08-my-first-pull-request/index.html#my-favourite-colour-palettes",
    "href": "posts/2021-01-08-my-first-pull-request/index.html#my-favourite-colour-palettes",
    "title": "my first pull request",
    "section": "my favourite colour palettes",
    "text": "my favourite colour palettes\n\nManu (NZ birds)\nHarry Potter\nwesanderson\nPNWColors\nfeathers (Oz birds)\n\nAnd many many more to be found in Emil’s list"
  },
  {
    "objectID": "posts/2024-07-04-learning_python/index.html",
    "href": "posts/2024-07-04-learning_python/index.html",
    "title": "learning python",
    "section": "",
    "text": "When you are exploring a far off land and only know a tiny bit of the language they speak there, you always carry a little dictionary with commonly used phrases translated from the language you speak into the other language. It is important to know how to ask someone where the toilets are while you are travelling!\nI have just started learning Python with Posit Academy in the lead up to #positconf2024 and I am trying to approach in the same way I would if I was learning French. A dictionary that helps me link functions I know in R to new functions I am learning in Python could be handy.\nLinking new concepts to old concepts is also a useful learning strategy. Research in Psychology tells us that memory is relational; the brain represents memories as networks of representations. If you can link something that you are trying to learn to something you already know, you are much more likely to remember that new thing into the long term.\n\n\n\n\n\nArt credit: China Blue Art, Memory Network I\n\n\n\n\nOf course, there are probably a million R to Python dictionaries on the internet; why am I creating a new one for me?\nThat’s because we also know that generative learning and retrieval practice are more effective strategies for remembering things into the long term, than are learning strategies that involve passive review of materials that were created by someone else.\nTo create a dictionary of functions as I learn new things in Python, I need to retrieve the equivalent function in R from memory and actively evaluate the similarities and differences between the Python and R versions. That process of retrieving and using the concepts I already know, strengthens both my knowledge of R, and links my new Python understanding to it.\nSo I am starting a googlesheet to keep track of new things I am learning how to do in Python and their R equivalents. Maybe I can read that googlesheet in here using the googlesheets4 package and make it display in a searchable table using gt.\n\ncheat_sheet %&gt;%\n  gt() %&gt;%\n  tab_header(\"My R vs Python cheatsheet\") %&gt;%\n  opt_interactive(use_search = TRUE)\n\n\n\n\nMy R vs Python cheatsheet"
  },
  {
    "objectID": "posts/2020-11-18-digging-into-the-janitor-package/index.html",
    "href": "posts/2020-11-18-digging-into-the-janitor-package/index.html",
    "title": "cleaning penguins with the janitor package",
    "section": "",
    "text": "The janitor package by Sam Firke contains probably my FAVOURITE R function: clean_names().\nBy default when I am reading data into R, I always pipe clean_names() onto the end of my read_csv(). That way, I never have to look at inconsistently formatted variable names. But janitor package includes lots of other useful functions that make it easier to deal with dirty data and count stuff.\n\nExploring package functions\nAre you keen to dig into the little known functions of a package that you use all the time? Here is a tip: in console type the name of the package with a double colon (i.e. janitor::), all the functions in the package will pop up and you can explore them by scrolling up and down the list.\nAlternatively you can load the package in the console and then use ls(package:packagename) to get a list of all the objects in the package.\n\nlibrary(janitor)\n\nls(package:janitor)\n\n [1] \"%&gt;%\"                   \"add_totals_col\"        \"add_totals_row\"       \n [4] \"adorn_crosstab\"        \"adorn_ns\"              \"adorn_pct_formatting\" \n [7] \"adorn_percentages\"     \"adorn_rounding\"        \"adorn_title\"          \n[10] \"adorn_totals\"          \"as_tabyl\"              \"chisq.test\"           \n[13] \"clean_names\"           \"compare_df_cols\"       \"compare_df_cols_same\" \n[16] \"convert_to_date\"       \"convert_to_datetime\"   \"convert_to_NA\"        \n[19] \"crosstab\"              \"describe_class\"        \"excel_numeric_to_date\"\n[22] \"find_header\"           \"fisher.test\"           \"get_dupes\"            \n[25] \"get_one_to_one\"        \"make_clean_names\"      \"remove_constant\"      \n[28] \"remove_empty\"          \"remove_empty_cols\"     \"remove_empty_rows\"    \n[31] \"round_half_up\"         \"round_to_fraction\"     \"row_to_names\"         \n[34] \"sas_numeric_to_date\"   \"signif_half_up\"        \"single_value\"         \n[37] \"tabyl\"                 \"top_levels\"            \"untabyl\"              \n[40] \"use_first_valid_of\"   \n\n\nLets try a few these functions.\n\n\nread some dirty data\nThe penguin data isn’t very dirty out of the package, but I added some funky things for illustrative purposes.\n\ndirty &lt;- read_csv(\"penguin_raw_dirty.csv\")\n\n\n\n1. clean_names()\nThe penguin variable names are not great. A nasty mix of capital and little letters, gaps and brackets- not fun to type over and over again.\n\nnames(dirty)\n\n [1] \"studyName\"           \"Sample Number\"       \"Species\"            \n [4] \"Region\"              \"Island\"              \"Stage\"              \n [7] \"Individual ID\"       \"Clutch Completion\"   \"Empty Column\"       \n[10] \"Date Egg\"            \"Culmen Length (mm)\"  \"Culmen Depth (mm)\"  \n[13] \"Flipper Length (mm)\" \"Body Mass (g)\"       \"Sex\"                \n[16] \"Delta 15 N (o/oo)\"   \"Delta 13 C (o/oo)\"   \"Comments\"           \n\n\nclean_names() will take all the variable names and make them lower case and replace gaps/brackets with underscores.\n\nclean &lt;- dirty %&gt;%\n  clean_names()\n\nnames(clean)\n\n [1] \"study_name\"        \"sample_number\"     \"species\"          \n [4] \"region\"            \"island\"            \"stage\"            \n [7] \"individual_id\"     \"clutch_completion\" \"empty_column\"     \n[10] \"date_egg\"          \"culmen_length_mm\"  \"culmen_depth_mm\"  \n[13] \"flipper_length_mm\" \"body_mass_g\"       \"sex\"              \n[16] \"delta_15_n_o_oo\"   \"delta_13_c_o_oo\"   \"comments\"         \n\n\n\n\n2. remove_empty()\nSometimes dirty data contains whole rows or columns that are empty. You can quickly remove them with remove_empty(). By default it is a “quiet” function, but specify quiet = FALSE and it will give you a little feedback about what it has done.\n\nempty &lt;- clean %&gt;%\n  remove_empty(which = c(\"rows\", \"cols\"), quiet = FALSE)\n\nRemoving 1 empty rows of 347 rows total (0.288%).\n\n\nRemoving 1 empty columns of 18 columns total (Removed: empty_column).\n\n\n\n\n3. get dupes()\nLets imagine an RA made a mistake and entered the data for a couple of penguins twice. The get_dupes() function will tell you if there are duplicate entries in your dataset.\n\nempty %&gt;%\n  get_dupes(sample_number, species)\n\n# A tibble: 4 × 18\n  sample_number species  dupe_count study_name region island stage individual_id\n          &lt;dbl&gt; &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;        \n1            18 Adelie …          2 PAL0708    Anvers Torge… Adul… N9A2         \n2            18 Adelie …          2 PAL0708    Anvers Torge… Adul… N9A2         \n3            60 Chinstr…          2 PAL0910    Anvers Dream  Adul… N95A2        \n4            60 Chinstr…          2 PAL0910    Anvers Dream  Adul… N95A2        \n# ℹ 10 more variables: clutch_completion &lt;chr&gt;, date_egg &lt;chr&gt;,\n#   culmen_length_mm &lt;dbl&gt;, culmen_depth_mm &lt;dbl&gt;, flipper_length_mm &lt;dbl&gt;,\n#   body_mass_g &lt;dbl&gt;, sex &lt;chr&gt;, delta_15_n_o_oo &lt;dbl&gt;, delta_13_c_o_oo &lt;dbl&gt;,\n#   comments &lt;chr&gt;\n\n\nThen you can remove the duplicates with the distinct() function from dplyr. It only keeps distinct observations.\n\ndupes_gone &lt;- empty %&gt;%\n  distinct()\n\ndupes_gone %&gt;%\n  get_dupes(sample_number, species)\n\nNo duplicate combinations found of: sample_number, species\n\n\n# A tibble: 0 × 18\n# ℹ 18 variables: sample_number &lt;dbl&gt;, species &lt;chr&gt;, dupe_count &lt;int&gt;,\n#   study_name &lt;chr&gt;, region &lt;chr&gt;, island &lt;chr&gt;, stage &lt;chr&gt;,\n#   individual_id &lt;chr&gt;, clutch_completion &lt;chr&gt;, date_egg &lt;chr&gt;,\n#   culmen_length_mm &lt;dbl&gt;, culmen_depth_mm &lt;dbl&gt;, flipper_length_mm &lt;dbl&gt;,\n#   body_mass_g &lt;dbl&gt;, sex &lt;chr&gt;, delta_15_n_o_oo &lt;dbl&gt;, delta_13_c_o_oo &lt;dbl&gt;,\n#   comments &lt;chr&gt;\n\n\n\n\n4. tabyl()\nBut perhaps the most surprisingly awesome function in the janitor package is tabyl(). Counting things in R is surprisingly hard see post, but tabyl() is a huge help.\nAdvantages…\n\nworks with %&gt;%\ntakes a dataframe\noutputs a dataframe\nis compatible with other table packages like kableExtra and gt\n\n\none variable (gets you %)\n\ndupes_gone %&gt;%\n  tabyl(species) \n\n                                   species   n   percent\n       Adelie Penguin (Pygoscelis adeliae) 152 0.4418605\n Chinstrap penguin (Pygoscelis antarctica)  68 0.1976744\n         Gentoo penguin (Pygoscelis papua) 124 0.3604651\n\n\n\n\ntwo variables\n\ndupes_gone %&gt;%\n  tabyl(species, sex)\n\n                                   species . FEMALE MALE NA_\n       Adelie Penguin (Pygoscelis adeliae) 0     73   73   6\n Chinstrap penguin (Pygoscelis antarctica) 0     34   34   0\n         Gentoo penguin (Pygoscelis papua) 1     58   61   4\n\n\nHmmmm why is there a “.” column? Turns out for one penguin their sex is entered as “.” instead of NA.\nUse na_if() from dplyr to convert pesky values to NA.\n\ndupes_gone$sex &lt;-  na_if(dupes_gone$sex, \".\")\n\ndupes_gone %&gt;%\n  tabyl(species, sex)\n\n                                   species FEMALE MALE NA_\n       Adelie Penguin (Pygoscelis adeliae)     73   73   6\n Chinstrap penguin (Pygoscelis antarctica)     34   34   0\n         Gentoo penguin (Pygoscelis papua)     58   61   5\n\n\n\n\nthree variables\n\ndupes_gone %&gt;%\n  tabyl(species, sex, island) \n\n$Biscoe\n                                   species FEMALE MALE NA_\n       Adelie Penguin (Pygoscelis adeliae)     22   22   0\n Chinstrap penguin (Pygoscelis antarctica)      0    0   0\n         Gentoo penguin (Pygoscelis papua)     58   61   5\n\n$Dream\n                                   species FEMALE MALE NA_\n       Adelie Penguin (Pygoscelis adeliae)     27   28   1\n Chinstrap penguin (Pygoscelis antarctica)     34   34   0\n         Gentoo penguin (Pygoscelis papua)      0    0   0\n\n$Torgersen\n                                   species FEMALE MALE NA_\n       Adelie Penguin (Pygoscelis adeliae)     24   23   5\n Chinstrap penguin (Pygoscelis antarctica)      0    0   0\n         Gentoo penguin (Pygoscelis papua)      0    0   0\n\n\n\n\nadorn_ ing things\n\ntotals by row and/or col\n\ndupes_gone %&gt;%\n  tabyl(species, sex) %&gt;%\n  adorn_totals(c(\"row\", \"col\"))\n\n                                   species FEMALE MALE NA_ Total\n       Adelie Penguin (Pygoscelis adeliae)     73   73   6   152\n Chinstrap penguin (Pygoscelis antarctica)     34   34   0    68\n         Gentoo penguin (Pygoscelis papua)     58   61   5   124\n                                     Total    165  168  11   344\n\n\n\n\npercentages\n\ndupes_gone %&gt;%\n  tabyl(species, sex) %&gt;%\n  adorn_percentages(\"row\")\n\n                                   species    FEMALE      MALE        NA_\n       Adelie Penguin (Pygoscelis adeliae) 0.4802632 0.4802632 0.03947368\n Chinstrap penguin (Pygoscelis antarctica) 0.5000000 0.5000000 0.00000000\n         Gentoo penguin (Pygoscelis papua) 0.4677419 0.4919355 0.04032258\n\n\n\n\npercent formatting\n\n  dupes_gone %&gt;%\n  tabyl(species, sex) %&gt;%\n  adorn_percentages(\"row\") %&gt;%\n  adorn_pct_formatting()\n\n                                   species FEMALE  MALE  NA_\n       Adelie Penguin (Pygoscelis adeliae)  48.0% 48.0% 3.9%\n Chinstrap penguin (Pygoscelis antarctica)  50.0% 50.0% 0.0%\n         Gentoo penguin (Pygoscelis papua)  46.8% 49.2% 4.0%\n\n\n\n\n\n\n5. tabyl + other nice tables (kableExtra, gt)\nThe nice thing is that the output of tabyl() can be assigned as a dataframe object in your environment OR you can pipe on a kable()…\n\ndupes_gone %&gt;%\n  tabyl(species, sex) %&gt;%\n  adorn_percentages(\"row\") %&gt;%\n  adorn_pct_formatting() %&gt;%\n  kable() \n\n\n\n\nspecies\nFEMALE\nMALE\nNA_\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n48.0%\n48.0%\n3.9%\n\n\nChinstrap penguin (Pygoscelis antarctica)\n50.0%\n50.0%\n0.0%\n\n\nGentoo penguin (Pygoscelis papua)\n46.8%\n49.2%\n4.0%\n\n\n\n\n\n… or a gt() to get a really nicely formatted summary table\n\ndupes_gone %&gt;%\n  tabyl(species, sex) %&gt;%\n  adorn_percentages(\"row\") %&gt;%\n  adorn_pct_formatting() %&gt;%\n  gt()\n\n\n\n\n\n\n\nspecies\nFEMALE\nMALE\nNA_\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n48.0%\n48.0%\n3.9%\n\n\nChinstrap penguin (Pygoscelis antarctica)\n50.0%\n50.0%\n0.0%\n\n\nGentoo penguin (Pygoscelis papua)\n46.8%\n49.2%\n4.0%"
  },
  {
    "objectID": "posts/2025-05-06_tt_nsf/index.html",
    "href": "posts/2025-05-06_tt_nsf/index.html",
    "title": "nsf grants terminated",
    "section": "",
    "text": "The TidyTuesday challenge this week comes from a crowdsourced dataset put together by GrantWatch about the National Science Foundation grants that have been terminated this year by the Trump administration. The data includes information about the state from which the researchers were operating, as well as the award type and directorate, which gives a sense of the kind of research that is being targeted.\nI am particularly interested in the amount of grant funding that has been lost by state and the kinds of research grants that are being targeted by the administration."
  },
  {
    "objectID": "posts/2025-05-06_tt_nsf/index.html#readclean-data",
    "href": "posts/2025-05-06_tt_nsf/index.html#readclean-data",
    "title": "nsf grants terminated",
    "section": "read/clean data",
    "text": "read/clean data\n\n\nCode\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(usmap)\nlibrary(gt)\nlibrary(ggeasy)\nlibrary(patchwork)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 18)\n\nnsf &lt;- tuesdata$nsf_terminations\n\n\nstate_dir &lt;- nsf %&gt;%\n  select(state = org_state, usaspending_obligated, directorate_abbrev, directorate) %&gt;%\n  mutate(directorate = str_remove_all(directorate, '[\"]'))"
  },
  {
    "objectID": "posts/2025-05-06_tt_nsf/index.html#stem-sbe-by-state",
    "href": "posts/2025-05-06_tt_nsf/index.html#stem-sbe-by-state",
    "title": "nsf grants terminated",
    "section": "STEM & SBE by state",
    "text": "STEM & SBE by state\nI am interested in the state distribution of funding loss in STEM Education and Social, Behavioral, and Economic Sciences. This plot includes states that lost more than 10 million USD in STEM and SBE grants.\n\n\nCode\namount_dir_state &lt;- state_dir %&gt;%\n  group_by(state, directorate) %&gt;%\n  summarise(amount = sum(usaspending_obligated, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n\nstem_sbe &lt;- amount_dir_state %&gt;%\n  filter(directorate %in% c(\"STEM Education\", \"Social, Behavioral and Economic Sciences\")) %&gt;%\n  group_by(state) %&gt;%\n  mutate(total_stem_sbe = sum(amount)) %&gt;%\n  arrange(-total_stem_sbe) \n\n\nstem_sbe %&gt;%\n  filter(total_stem_sbe &gt; 10000000) %&gt;%\n  ggplot(aes(x = reorder(state, -total_stem_sbe), y = amount, fill = directorate)) +\n  geom_col() +\n  scale_y_continuous(limits = c(0,60000000), labels = unit_format(unit = \"M\", scale = 1e-6)) +\ntheme_minimal() +\n  easy_add_legend_title(\"Directorate\") +\n  easy_move_legend(to = c(\"bottom\")) +\n    scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"State\", y = \"Funding $\", \n       title = \"Value of STEM Education & Social, Behavioral, and Economic Sciences funding \\nterminated by state\", subtitle = \"States that have lost more than 10 million in STEM & SBE grants\")"
  },
  {
    "objectID": "posts/2025-06-10_tt_judges/index.html",
    "href": "posts/2025-06-10_tt_judges/index.html",
    "title": "historydata: judges",
    "section": "",
    "text": "The Tidy Tuesday data today comes from the historydata R package. The today is about US judges. I am particularly interested in the US Supreme Court and how long justices have remained in the role across history.\n\nload packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(janitor)\nlibrary(ggeasy)\nlibrary(ggtext)\n\n# adjust year/week values here\nyear = 2025\nweek = 23\n\n\n\n\nget the data\nReading the data and dealing with some date formatting.\n\n\nCode\ntt &lt;- tt_load(year, week)\n\n\n---- Compiling #TidyTuesday Information for 2025-06-10 ----\n--- There are 2 files available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 2: \"judges_appointments.csv\"\n  2 of 2: \"judges_people.csv\"\n\n\nCode\nappt &lt;- tt[[1]]\n\nappt &lt;- appt  %&gt;%\n  mutate(across(ends_with(\"_date\"), ~ mdy(.))) %&gt;%\n  mutate(retirement_from_active_service =  mdy(retirement_from_active_service))\n\npeople &lt;- tt[[2]]\n\npeople &lt;- people %&gt;%\n  mutate(age_at_death = death_date - birth_date)\n\n\n\n\nwrangling\nI start by joining the people and appt dataframes and for ease of use, create some new variables that pull year values out of dates. I filter for just the supreme court and select relevant columns, before sorting out some duplicate name issues.\n\n\n\n\n\n\nNote\n\n\n\nDid you know that there were two justices named John Harlan? One born in 1833 and his grandson born in 1899.\n\n\nI create a new variable that codes dates as pre vs post 1900 and a couple that allow me to set the colour of the dot by a different coloured point depending on whether the justice retired or their term ended (because they died).\n\n\nCode\njoined &lt;- left_join(people, appt, by = \"judge_id\") %&gt;%\n  arrange(judge_id) %&gt;%\n  mutate(year_confirmed = year(senate_confirmation_date), \n         year_retired = year(retirement_from_active_service), \n         year_terminated = year(termination_date))\n\nussc &lt;- joined %&gt;%\n  filter(court_type == \"USSC\") %&gt;%\n  filter(!is.na(year_confirmed)) %&gt;%\n  select(1:6, 26:31) %&gt;%\n  mutate(length_of_term = year_terminated - year_confirmed) %&gt;%\n  mutate(last_first = paste0(name_last, \", \" , name_first)) %&gt;%\n  mutate(last_first = case_when(name_last == \"Harlan\" & birth_date == \"1833\" ~ \"I Harlan, John\", \n                                name_last == \"Harlan\" & birth_date == \"1899\" ~ \"II Harlan, John\", \n                                TRUE ~ last_first)) %&gt;%\n    distinct(last_first, .keep_all = TRUE) %&gt;%\n  mutate(period = case_when(year_confirmed &lt; 1900 ~ \"pre1900\", \n                             year_confirmed &gt;= 1900 ~ \"post1900\", \n                            )) %&gt;%\n  mutate(endpoint = case_when(!is.na(year_terminated) ~ year_terminated, \n                              !is.na(year_retired) ~ year_retired,  \n                               TRUE ~ NA_real_))  %&gt;%                \n  mutate(status = case_when(!is.na(year_terminated) ~ \"Term end\",\n                            !is.na(year_retired) ~ \"Retired\", \n                       TRUE ~ \"Current\"))\n\n# order names by year confirmed\nussc$last_first &lt;- fct_reorder(ussc$last_first, ussc$year_confirmed) \n\n# select just plot variables\nussc_plot &lt;- ussc %&gt;%\n  select(period, last_first, year_confirmed, year_retired, year_terminated, endpoint, status)\n\n\n\n\nplot\nThis plot looks ok. It took me ages to work out that the easiest way to make a taller plot that gives each justice more vertical room is to set the r chunk options {r fig.width=6, fig.height=12, fig.dpi=300}.\nUnfortunately this data isn’t up to date for the most recent justices. Scalia and Ginsburg have died, Kennedy and Breyer have retired and four new justices have joined the court.\n\n\nCode\nussc_plot %&gt;%\n  filter(period == \"post1900\") %&gt;%\nggplot() +\n  geom_point(aes(x = last_first, y = year_confirmed), \n             colour = \"red\", size = 1.5) +\n  geom_point(aes(x = last_first, y = endpoint, colour = status), \n             size = 1.5, na.rm = TRUE) +\n  geom_segment(aes(x = last_first, \n                   xend= last_first, \n                   y = year_confirmed, yend = endpoint), \n               colour = \"grey\", na.rm = TRUE) +\n coord_flip() +\n   scale_y_continuous(limits = c(1900, 2020), breaks = seq(1900,2020,30)) +\n  theme_minimal() +\n  labs(y = \"Year\", x = \"Justice\", \n       title = \"Length of Supreme Court Justice Terms\", \n       subtitle = \"Since 1900 &lt;span style='color:red'&gt;● Confirmed&lt;/span&gt; \n                    &lt;span style='color:purple'&gt;● Retired&lt;/span&gt;\n                  &lt;span style='color:blue'&gt;● Term end&lt;/span&gt;\") +\n    theme(plot.title.position = \"plot\", \n        plot.subtitle = element_markdown(),\n        panel.grid.major.y = element_line(linewidth = 0.3, color = \"grey90\"),\n        panel.grid.minor.y = element_line(linewidth = 0.1, color = \"grey95\"),\n        axis.text.y = element_text(size = 10),\n        axis.text.x = element_text(size = 10), \n          plot.title = element_text(size = 12)) +\n  scale_colour_manual(values = c(\"Term end\" = \"blue\", \n                                 \"Retired\" = \"purple\")) +\n  easy_remove_legend()\n\n\n\n\n\n\n\n\n\n\n\nadding new data for recent appointments\nThis chunk creates a new dataframe for the four most recent justices and updates values for those that have died and retired.\n\n\nCode\n# get just those appointed since 1980\n\nussc80 &lt;- ussc_plot %&gt;%\n  filter(year_confirmed &gt; 1980) %&gt;%\n  arrange(year_confirmed)\n\n# make new df with most recent 4 appointees\n\nnew_justice &lt;- tibble(\n  period = c(\"post1900\", \"post1900\",\"post1900\",\"post1900\"),\n  last_first = c(\"Gorsuch, Neil\", \"Kavanaugh, Brett\", \"Coney Barrett, Amy\", \"Brown Jackson, Ketanji\"),\n  year_confirmed = c(2017, 2018, 2020, 2022),\n  year_retired = c(NA, NA, NA, NA), \n  year_terminated = c(NA, NA, NA, NA),\n  endpoint = c(NA, NA, NA, NA),\n  status = c(\"Current\", \"Current\",\"Current\",\"Current\")\n)\n\n# join since 1980 and new justices, correcting values for those no longer serving\n\nussc_recent &lt;- rbind(ussc80, new_justice) %&gt;%\n  mutate(\n    year_retired = replace(year_retired, last_first == \"Kennedy, Anthony\", 2018), \n    year_retired = replace(year_retired, last_first == \"Breyer, Stephen\", 2022), \n    year_terminated = replace(year_terminated, last_first == \"Scalia, Antonin\", 2016),\n    year_terminated = replace(year_terminated, last_first == \"Ginsburg, Ruth\", 2020)) %&gt;%\n  mutate(endpoint = case_when(!is.na(year_terminated) ~ year_terminated, \n                              !is.na(year_retired) ~ year_retired,  \n                               TRUE ~ NA_real_))  %&gt;%                \nmutate(status = case_when(!is.na(year_terminated) ~ \"Term end\",\n                            !is.na(year_retired) ~ \"Retired\", \n                       TRUE ~ \"Current\"))\n\n# bind updated ussc data with old df, keeping only distinct names.\n\nupdated_ussc &lt;- rbind(ussc_recent, ussc_plot) %&gt;%\n  distinct(last_first, .keep_all = TRUE) %&gt;%\n  mutate(term_length = case_when(year_terminated - year_confirmed &gt; 30 ~ \"long\", \n                                 year_terminated - year_confirmed &lt;= 30 ~ \"short\", \n                                 ))\n\n\n\n\nupdated plot\n\n\nCode\nupdated_ussc %&gt;%\n  filter(period == \"post1900\") %&gt;%\nggplot() +\n  geom_point(aes(x = last_first, y = year_confirmed), \n             colour = \"red\", size = 1.5) +\n  geom_point(aes(x = last_first, y = endpoint, colour = status), \n             size = 1.5, na.rm = TRUE) +\n  geom_segment(aes(x = last_first, \n                   xend= last_first, \n                   y = year_confirmed, yend = endpoint), colour = \"grey\", \n               na.rm = TRUE) +\n coord_flip() +\n   scale_y_continuous(limits = c(1900, 2022), breaks = seq(1900,2020,30), \n                      sec.axis= dup_axis()) +\n  theme_minimal() +\n  labs(y = \"Year\", x = \"Justice\", \n       title = \"Length of Supreme Court Justice Terms\", \n       subtitle = \"Since 1900 &lt;span style='color:red'&gt;● Confirmed&lt;/span&gt; \n                    &lt;span style='color:purple'&gt;● Retired&lt;/span&gt;\n                  &lt;span style='color:blue'&gt;● Term end&lt;/span&gt;\") +\n    theme(plot.title.position = \"plot\", \n        plot.subtitle = element_markdown(),\n        panel.grid.major.y = element_line(linewidth = 0.3, color = \"grey90\"),\n        panel.grid.minor.y = element_line(linewidth = 0.1, color = \"grey95\"),\n        axis.text.y = element_text(size = 10),\n        axis.text.x = element_text(size = 10), \n          plot.title = element_text(size = 12)) +\n  scale_colour_manual(values = c(\"Term end\" = \"blue\", \n                                 \"Retired\" = \"purple\")) +\n  easy_remove_legend() +\n  easy_remove_x_axis(what = c(\"title\"))"
  },
  {
    "objectID": "posts/2025-05-20_tt_beaches/index.html",
    "href": "posts/2025-05-20_tt_beaches/index.html",
    "title": "sydney beaches",
    "section": "",
    "text": "My first TidyTuesday data curation!! This is an updated version of the dataset we used for #RYouWithMe, a series of online modules designed to get beginners into #rstats. I am excited to dig in and learn something new about what is going on with poo at Sydney beaches.\nIn poking about the BeachWatch website, I noticed that they have a key that gives each beach a star rating according to the bacteria levels and the risk to swimmers.\nI thought it might be interesting to look at different beach sites across the city and see how frequently each beach is rated as 4 stars “Good”.\nI am most interested in data in the last 5 years so have filtered to only include reports from 2020 until 2025."
  },
  {
    "objectID": "posts/2025-05-20_tt_beaches/index.html#how-many-days-are-good-for-swimming",
    "href": "posts/2025-05-20_tt_beaches/index.html#how-many-days-are-good-for-swimming",
    "title": "sydney beaches",
    "section": "how many days are good for swimming",
    "text": "how many days are good for swimming\n\ncity beaches\nIn the #RYouWithMe modules, we used data from the Sydney City beaches, so I am going to start with those because I am a bit more familiar with them.\n\nraw count\nHere I am filtering the water data to include only beaches that are within the Sydney city region and using the tabyl function from the janitor package to count how often each site was rated Good, Fair, Poor, and Bad. Then I make that data long and fix the order of the rating levels, before plotting.\n\n\nCode\ncity &lt;-  water %&gt;%\n  filter(region == \"Sydney City\") \n\ngood_days_city &lt;- city %&gt;%\n tabyl(swim_site, stars) %&gt;%\n  select(swim_site, Good, Fair, Poor, Bad) \n\n\ngood_days_city_long &lt;- good_days_city %&gt;%\n  pivot_longer(names_to = \"rating\", values_to = \"count\", Good:Bad) %&gt;%\n  mutate(rating = fct_relevel(rating, c(\"Bad\", \"Poor\", \"Fair\", \"Good\"))) \n\n\ngood_days_city_long %&gt;%\n    ggplot(aes(x = reorder(swim_site, count), y = count, fill = rating)) +\n  geom_col() +\n   scale_fill_manual(values = my_colours) +\n  coord_flip() +\n  labs(x = \"Beach\", y = \"Count of ratings\", title = \"Water quality at Sydney City Beaches\", \n        subtitle= \"Data from BeachWatch NSW 2020-2025\") +\n  theme_beaches()\n\n\n\n\n\n\n\n\n\n\n\npercent\nAcross 2020-2025, there is a small amount of variability in the number of readings that each beach site has so I am going to tweak this to plot percent rather than raw data. I have also ordered the bars so that the beaches that have the highest percent Good ratings are at the top.\n\n\nCode\ngood_days_city_long_percent &lt;- good_days_city %&gt;%\n  pivot_longer(names_to = \"rating\", values_to = \"count\", Good:Bad) %&gt;%\n  mutate(rating = fct_relevel(rating, c(\"Bad\", \"Poor\", \"Fair\", \"Good\"))) %&gt;%\n  group_by(swim_site) %&gt;%\n  mutate(total_days = sum(count)) %&gt;%\n  rowwise() %&gt;%\n  mutate(percent = count/total_days) %&gt;%\n   group_by(swim_site) %&gt;%\n  mutate(good_percent = percent[rating == \"Good\"]) %&gt;% #pull good % for use in reorder\n  ungroup()\n\n\ngood_days_city_long_percent %&gt;%\n    ggplot(aes(x = reorder(swim_site, good_percent), y = percent, fill = rating)) +\n  geom_col() +\n   scale_fill_manual(values = my_colours) +\n  coord_flip() +\n  labs(x = \"Beach\", y = \"Percent of ratings\", title = \"Water quality at Sydney City Beaches\", \n       subtitle= \"Data from BeachWatch NSW 2020-2025\") +\n    theme_beaches()\n\n\n\n\n\n\n\n\n\nIt is a bit disconcerting that Coogee beach (where I used to live) is only rated “Good” 75% of the time.\n\n\n\nall regions\nNow I would like to get a plot like this for all the other regions. It is interesting that there are so many data points for Northern Sydney and hardly any for Western Sydney, so that might be something to dig into too.\n\nwater %&gt;%\n  tabyl(region) %&gt;%\n  gt()\n\n\n\n\n\n\n\nregion\nn\npercent\n\n\n\n\nNorthern Sydney\n6711\n0.35386238\n\n\nSouthern Sydney\n2456\n0.12950171\n\n\nSydney City\n3436\n0.18117585\n\n\nSydney Harbour\n5712\n0.30118640\n\n\nWestern Sydney\n650\n0.03427366\n\n\n\n\n\n\n\nI would like to count the number of times each swim site gets each star rating but keep information about which region the beach belongs too so I can compare regions.\nIf I use water %&gt;% tabyl(swim_site, stars) the output drops information about the region.\nBut if I add region as an argument to the tabyl() function, the output ends up being a list.\n\ngood_days_region &lt;- water %&gt;%\n tabyl(swim_site, stars, region)\n\n\nclass(good_days_region)\n\n[1] \"list\"\n\n\nMaybe I write a function that will apply the cleaning process and then make a plot for each region in the list.\nMy function takes two arguments: the data and names of the regions. It has a processing step which filters out sites that have missing data across the board, selects just relevant variables, makes the data long, and fixes the levels of the quality. The output of that step (processed_df) is fed into the plot.\n\n\nCode\nplot_region_data_raw &lt;- function(df, region_name) {\n  \n  processed_df &lt;- df %&gt;%\n    filter(Bad &gt; 0 | Fair &gt; 0 | Good &gt; 0 | Poor &gt; 0) %&gt;%\n    select(swim_site, Good, Fair, Poor, Bad) %&gt;%\n    pivot_longer(names_to = \"rating\", values_to = \"count\", Good:Bad) %&gt;%\n    mutate(rating = fct_relevel(rating, c(\"Bad\", \"Poor\", \"Fair\", \"Good\"))) \n    \n   \n  plot &lt;- ggplot(processed_df, \n                 aes(x = reorder(swim_site, count), \n                     y = count, fill = rating)) +\n    geom_col() +\n     scale_fill_manual(values = my_colours) +\n    coord_flip() +\n    labs(\n      title = paste(\"Water Quality Ratings in\", region_name),\n      subtitle= \"Data from BeachWatch NSW 2020-2025\",\n      x = \"Beach\",\n      y = \"Count of ratings\",\n      fill = \"Rating\"\n    ) +\n    theme_beaches() \n  \n  return(plot)\n}\n\n\nOnce I have the function in my enviornment, I can use map2() from purrr to run the processing/plot function on each of the regions in the list and output a list of plots.\n\n\nCode\nregion_plots_raw &lt;- map2(good_days_region,  # the list \n  names(good_days_region),  # the names of each list element\n  plot_region_data_raw # function to map \n)\n\n\n\nWater Quality by region: Count of ratings\n\nNorthern SydneySouthern SydneySydney CitySydney HarbourWestern Sydney\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is curious that Western Sydney has so many fewer data points than other regions, but it also makes it worth repeating the plots with percents.\n\n\nCode\nplot_region_data_percent &lt;- function(df, region_name) {\n  processed_df &lt;- df %&gt;%\n    filter(Bad &gt; 0 | Fair &gt; 0 | Good &gt; 0 | Poor &gt; 0) %&gt;%\n    select(swim_site, Good, Fair, Poor, Bad) %&gt;%\n    pivot_longer(names_to = \"rating\", values_to = \"count\", Good:Bad) %&gt;%\n    mutate(rating = fct_relevel(rating, c(\"Bad\", \"Poor\", \"Fair\", \"Good\"))) %&gt;%\n     group_by(swim_site) %&gt;%\n  mutate(total_days = sum(count)) %&gt;%\n  rowwise() %&gt;%\n  mutate(percent = count/total_days) %&gt;%\n   group_by(swim_site) %&gt;%\n  mutate(good_percent = percent[rating == \"Good\"]) %&gt;% #pull good % for use in reorder\n  ungroup()\n  \n  plot &lt;- ggplot(processed_df, \n                 aes(x = reorder(swim_site, good_percent), y = percent, fill = rating)) +\n    geom_col() +\n     scale_fill_manual(values = my_colours) +\n    coord_flip() +\n    labs(\n      title = paste(\"Water Quality Ratings in\", region_name),\n      subtitle= \"Data from BeachWatch NSW 2020-2025\",\n      x = \"Beach\",\n      y = \"Percent of ratings\",\n      fill = \"Rating\"\n    ) +\n    theme_beaches() \n  \n  return(plot)\n}\n\n\nregion_plots_percent &lt;- map2(good_days_region,  # list of data frames\n  names(good_days_region),  # Names of each list element\n  plot_region_data_percent # function to map \n)\n\n\n\n\nWater Quality by region: Percent of ratings\n\nNorthern SydneySouthern SydneySydney CitySydney HarbourWestern Sydney"
  },
  {
    "objectID": "posts/2025-05-20_tt_beaches/index.html#what-is-going-on-in-western-sydney",
    "href": "posts/2025-05-20_tt_beaches/index.html#what-is-going-on-in-western-sydney",
    "title": "sydney beaches",
    "section": "what is going on in Western Sydney?",
    "text": "what is going on in Western Sydney?\nThe data presented above show that Western Sydney beaches have many fewer data points than beaches in other regions.\nIt got me thinking about potential differences in testing frequency. Here I am computing a new variable that calcuates the time between sucessive data points using the difftime() function.\n\n\nCode\nfreq &lt;- water %&gt;%\n  select(region, swim_site, bugs, stars, date) %&gt;%\n  arrange(swim_site, date) %&gt;%\n mutate(testing_gap = difftime(date, lag(date), units = \"days\")) %&gt;%\n  mutate(testing_gap = as.numeric(testing_gap)) %&gt;%\n  filter(testing_gap &gt; 0)\n\nfreq %&gt;%\n  group_by(region) %&gt;%\n  summarise(mean_time_lapsed = mean(testing_gap, na.rm= TRUE)) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nregion\nmean_time_lapsed\n\n\n\n\nNorthern Sydney\n6.332735\n\n\nSouthern Sydney\n6.316993\n\n\nSydney City\n6.187500\n\n\nSydney Harbour\n9.577256\n\n\nWestern Sydney\n18.598131\n\n\n\n\n\n\n\nIt is clear that while the water quality in most regions is tested about once a week, in Western Sydney data points come in on average every 18 days. Is that because the frequency of testing is genuinely lower in Western Sydney, or are there gaps in the data?\nThis plot would suggest the latter is true.\n\n\nCode\nws &lt;- freq %&gt;%\n  filter(region == \"Western Sydney\") \n\n\nws %&gt;%\n  ggplot(aes(x = date, y= testing_gap)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~swim_site) +\n  theme_minimal() +\n  labs(title = \"Days between data points in Western Sydney\", x = \"Date\", y = \"Testing gap\")\n\n\n\n\n\n\n\n\n\nIn fact, if we look at the number of data points in the dataset per month, it is clear that in Western Sydney, at some beach sites, testing is seasonal and only happens in the summer.\n\n\nCode\nws_monthly_count &lt;- ws %&gt;%\n  mutate(month = month(date, label = TRUE)) %&gt;%\n  group_by(month, swim_site) %&gt;%\n  summarise(count = n()) \n\n\nws_monthly_count %&gt;%\n  ggplot(aes(x = month, y = count)) +\n  geom_point() +\n  facet_wrap(~swim_site) +\n  theme_minimal() +\n  labs(title = \"Water quality testing is seasonal in Western Sydney\", x = \"Month\", y = \"Count\")"
  },
  {
    "objectID": "posts/2023-05-24-rowwise-mean/index.html",
    "href": "posts/2023-05-24-rowwise-mean/index.html",
    "title": "rowwise %>% mean",
    "section": "",
    "text": "When you have data from a survey, the responses for each item are most often listed in different variables. Generally you have to average across the items to get a mean value for that scale for each participant. But dealing with calculations across rows is sometimes difficult in R.\n\nPhoto by Travis Essinger on Unsplash\n\nload packages + make some data\n\nlibrary(tidyverse)\n\n\npID &lt;- c(\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\")\nitem1 = sample(1:7, 6, replace=T)\nitem2 = sample(1:7, 6, replace=T)\nitem3 = sample(1:7, 6, replace=T)\nitem4 = sample(1:7, 6, replace=T)\nitem5 = sample(1:7, 6, replace=T)\n\nsurvey &lt;- data.frame(pID, item1, item2, item3, item4, item5)\n\nglimpse(survey)\n\nRows: 6\nColumns: 6\n$ pID   &lt;chr&gt; \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\"\n$ item1 &lt;int&gt; 1, 4, 5, 5, 2, 6\n$ item2 &lt;int&gt; 3, 6, 5, 3, 3, 4\n$ item3 &lt;int&gt; 7, 6, 2, 4, 6, 1\n$ item4 &lt;int&gt; 5, 3, 3, 5, 2, 3\n$ item5 &lt;int&gt; 3, 4, 5, 6, 7, 7\n\n\n\n\nbase R rowMeans\nThe rowMeans() function works, but why the x and what do the dots mean??\n\nsurvey_means_base &lt;- survey %&gt;%\n  mutate(item_mean = rowMeans(x = select(.data = . , starts_with(match = \"item\"))))\n\n\n\ntidyverse rowwise\nThe tidyverse version involves using rowwise() to tell R that you would like a mean calculated for each row in the dataset. Use c() to tell R which columns to average across.\nWithout rowwise(), R will calculate the mean of all rows/columns and put that in the new variable. You will end up with the same value for each row.\n\nsurvey_means_norowwise &lt;- survey %&gt;%\n  mutate(item_mean = mean(c(item1, item2, item3, item4, item5))) \n\nglimpse(survey_means_norowwise)\n\nRows: 6\nColumns: 7\n$ pID       &lt;chr&gt; \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\"\n$ item1     &lt;int&gt; 1, 4, 5, 5, 2, 6\n$ item2     &lt;int&gt; 3, 6, 5, 3, 3, 4\n$ item3     &lt;int&gt; 7, 6, 2, 4, 6, 1\n$ item4     &lt;int&gt; 5, 3, 3, 5, 2, 3\n$ item5     &lt;int&gt; 3, 4, 5, 6, 7, 7\n$ item_mean &lt;dbl&gt; 4.2, 4.2, 4.2, 4.2, 4.2, 4.2\n\n\nWith rowwise(), it calculates across the rows, separately for each participant.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to get into the habit of adding ungroup() after a rowwise() in the same way as you would after a group_by() because the dataframe becomes grouped by row, which can mess with calcuations further down the pipeline.\n\n\n\nsurvey_means_rowwise &lt;- survey %&gt;%\n  rowwise() %&gt;%\n  mutate(item_mean = mean(c(item1, item2, item3, item4, item5))) %&gt;%\n  ungroup()\n\nglimpse(survey_means_rowwise)\n\nRows: 6\nColumns: 7\n$ pID       &lt;chr&gt; \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\"\n$ item1     &lt;int&gt; 1, 4, 5, 5, 2, 6\n$ item2     &lt;int&gt; 3, 6, 5, 3, 3, 4\n$ item3     &lt;int&gt; 7, 6, 2, 4, 6, 1\n$ item4     &lt;int&gt; 5, 3, 3, 5, 2, 3\n$ item5     &lt;int&gt; 3, 4, 5, 6, 7, 7\n$ item_mean &lt;dbl&gt; 3.8, 4.6, 4.0, 4.6, 4.0, 4.2\n\n\nIf there are a lot of columns to average across, you can avoid typing all of the names using c_across().\n\nsurvey_means_rowwise_across &lt;- survey %&gt;%\n  rowwise() %&gt;%\n  mutate(item_mean = mean(c_across(item1:item5))) %&gt;%\n  ungroup()\n\nglimpse(survey_means_rowwise_across)\n\nRows: 6\nColumns: 7\n$ pID       &lt;chr&gt; \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\"\n$ item1     &lt;int&gt; 1, 4, 5, 5, 2, 6\n$ item2     &lt;int&gt; 3, 6, 5, 3, 3, 4\n$ item3     &lt;int&gt; 7, 6, 2, 4, 6, 1\n$ item4     &lt;int&gt; 5, 3, 3, 5, 2, 3\n$ item5     &lt;int&gt; 3, 4, 5, 6, 7, 7\n$ item_mean &lt;dbl&gt; 3.8, 4.6, 4.0, 4.6, 4.0, 4.2"
  },
  {
    "objectID": "projects/2025-05-13_tipstricks/index.html",
    "href": "projects/2025-05-13_tipstricks/index.html",
    "title": "tips and tricks",
    "section": "",
    "text": "I love learning new #rstats things and sharing what I learn with the R community. This YouTube series captures quick tips and tricks to make your coding life more fun."
  },
  {
    "objectID": "projects/2025-05-13_tipstricks/index.html#customise-your-rstudio",
    "href": "projects/2025-05-13_tipstricks/index.html#customise-your-rstudio",
    "title": "tips and tricks",
    "section": "customise your RStudio",
    "text": "customise your RStudio"
  },
  {
    "objectID": "projects/2025-05-13_tipstricks/index.html#find-fun-data",
    "href": "projects/2025-05-13_tipstricks/index.html#find-fun-data",
    "title": "tips and tricks",
    "section": "find fun data",
    "text": "find fun data"
  },
  {
    "objectID": "projects/2025-05-13_tipstricks/index.html#clean-up-with-janitor",
    "href": "projects/2025-05-13_tipstricks/index.html#clean-up-with-janitor",
    "title": "tips and tricks",
    "section": "clean up with janitor",
    "text": "clean up with janitor"
  },
  {
    "objectID": "projects/2025-05-13_tipstricks/index.html#making-ggplot-easy",
    "href": "projects/2025-05-13_tipstricks/index.html#making-ggplot-easy",
    "title": "tips and tricks",
    "section": "making ggplot easy",
    "text": "making ggplot easy"
  },
  {
    "objectID": "projects/2025-05-13_tipstricks/index.html#make-datapasta",
    "href": "projects/2025-05-13_tipstricks/index.html#make-datapasta",
    "title": "tips and tricks",
    "section": "make datapasta",
    "text": "make datapasta"
  },
  {
    "objectID": "projects/2025-03-02_RYouWithMe/index.html",
    "href": "projects/2025-03-02_RYouWithMe/index.html",
    "title": "RYouWithMe",
    "section": "",
    "text": "I co-founded RLadies Sydney in 2018 and grew the community to more than 900 members over 4 years.\nI also created the #RYouWithMe course, a set of modules by beginners for beginners to get users started with R.\n\nThe course is highlighted on the RStudio Education website as a recommended learning resource. The YouTube videos have been by more the 40K new R learners.\n\nThe #RYouWithMe modules got a Quarto update in Feb 2025, you can find the new structure on the RLadies Sydney website and the videos are all on YouTube, start with Basic Basics here"
  },
  {
    "objectID": "projects/2024-10-30_alumni_gateway/index.html",
    "href": "projects/2024-10-30_alumni_gateway/index.html",
    "title": "careers",
    "section": "",
    "text": "In my work with Psychology students, I led several projects related to career preparation and work integrated learning (WIL).\nI led a Student Experience grant that culminated in the Psychology Alumni gateway, a site that showcases the diverse career paths that Psychology students pursue following graduation.\nThe site has interviews with recent alumni explaining how they use their Psychology skills in their chosen career.\nVideo"
  },
  {
    "objectID": "charts/2025-04-01_fractions/index.html",
    "href": "charts/2025-04-01_fractions/index.html",
    "title": "day 1 fractions",
    "section": "",
    "text": "This plot from Our World in Data illustrates the proportion of people who are willing to donate 1% of their income to climate change efforts as a function of people’s estimate of how many people in their country would be willing to donate.\nAs it turns out, people have a rather dim view of the likelihood that others would commit to supporting climate action. Lets see if we can reproduce the chart using ggplot.\n\n\n\nload packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(scales)\nlibrary(ggeasy)\nlibrary(ggrepel)\nlibrary(plotly)\n\n\n\nread in the data\nHere I am using the owidapi package to read in the data and reordering/renaming some variables before sorting the data by country and year.\n\nwillingness &lt;- read_csv(\"https://ourworldindata.org/grapher/willingness-climate-action.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names() %&gt;%\n  select(entity, code, owid_region, year, \n         prediction_others_willingness = willingness_contribute_1pct_climate_others, \n         self_willingness = willingness_contribute_pct_climate) %&gt;%\n   arrange(entity, year)\n\nRows: 397 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Entity, Code, owid_region\ndbl (3): Year, willingness_contribute_1pct_climate_others, willingness_contr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(willingness)\n\nRows: 397\nColumns: 6\n$ entity                        &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Aland Isl…\n$ code                          &lt;chr&gt; \"AFG\", \"AFG\", \"ALA\", \"ALB\", \"ALB\", \"DZA\"…\n$ owid_region                   &lt;chr&gt; \"Asia\", NA, \"Europe\", \"Europe\", NA, \"Afr…\n$ year                          &lt;dbl&gt; 2023, 2024, 2023, 2023, 2024, 2023, 2024…\n$ prediction_others_willingness &lt;dbl&gt; NA, 40.5, NA, NA, 43.8, NA, 40.0, NA, NA…\n$ self_willingness              &lt;dbl&gt; NA, 82.0, NA, NA, 71.3, NA, 54.6, NA, NA…\n\n\n\n\nclean up the data\nLooks like the plot only includes 2024 data, so lets filter first.\n\nwillingness2024 &lt;- willingness %&gt;%\n  filter(year == 2024) \n \n\nglimpse(willingness2024)\n\nRows: 126\nColumns: 6\n$ entity                        &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Ar…\n$ code                          &lt;chr&gt; \"AFG\", \"ALB\", \"DZA\", \"ARG\", \"ARM\", \"AUS\"…\n$ owid_region                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ year                          &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024…\n$ prediction_others_willingness &lt;dbl&gt; 40.5, 43.8, 40.0, 38.1, 40.7, 38.6, 37.9…\n$ self_willingness              &lt;dbl&gt; 82.0, 71.3, 54.6, 62.2, 75.4, 56.1, 72.7…\n\n\nUnfortunately in filtering out the 2023 data, I have also lost information about the owid region, which is used in colouring the dots in the plot. I need to go back to the dataset that has regions for 2023 and have them propogate into the 2024 rows as well.\nThis chunk groups the data by country and then mutates a new column to contain new region values. It looks just at 2023 rows and pulls the first value of region from those. Then we can filter for just 2024 values.\n\nwillingness2024 &lt;- willingness %&gt;%\n  group_by(entity) %&gt;%\n  mutate(region = first(owid_region[year == 2023])) %&gt;%\n  select(starts_with(\"entity\"), region, year, ends_with(\"willingness\")) %&gt;%\n  filter(year == 2024) \n \nglimpse(willingness2024)\n\nRows: 126\nColumns: 5\nGroups: entity [126]\n$ entity                        &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Ar…\n$ region                        &lt;chr&gt; \"Asia\", \"Europe\", \"Africa\", \"South Ameri…\n$ year                          &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024…\n$ prediction_others_willingness &lt;dbl&gt; 40.5, 43.8, 40.0, 38.1, 40.7, 38.6, 37.9…\n$ self_willingness              &lt;dbl&gt; 82.0, 71.3, 54.6, 62.2, 75.4, 56.1, 72.7…\n\n\n\n\nmake a basic plot\n\nwillingness2024 %&gt;%\n  ggplot(aes(y =  prediction_others_willingness, x = self_willingness)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0,100)) +\n    scale_x_continuous(limits = c(0,100)) +\n  labs(y = \"Predicted share willing to give\", \n       x = \"Actual share who said they were willing to give 1% of their income\") \n\n\n\n\n\n\n\n\nOK, basic plot success… things that I need to change\n\naxis values in 20% increments with %\ntheme needs to be white background with dotted gridlines on 20% values\ndiagonal dotted line\ncolour points by region\ninclude text of selection of countries\ncaption re data source\ntitle and subtitle\nplotly interactivity\n\n\n\ngridlines and axes\nHere I am using the expand and breaks arguments within scale_y_continuous and scale_x_continuous to make the axis start at 0,0 and have 20% increments. I get % labels on each access using the percent_format() function from the scales package (accuracy = 1 tells R I want it to display whole numbers and scale = 1 tells R my numbers are already percentages and don’t need to be multiplied by 100).\nI removed the minor gridlines using easy_remove_gridlines() from the ggeasy package. I also added a diagonal line usinggeom_abline().\n\nwillingness2024 %&gt;%\n  ggplot(aes(y =  prediction_others_willingness, x = self_willingness)) +\n  geom_point() +\n  scale_y_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                      limits = c(0,100), expand = c(0,0), breaks = seq(0,100,20)) +\n    scale_x_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                       limits = c(0,100), expand = c(0,0), breaks = seq(0,100,20)) +\n  labs(y = \"Predicted share willing to give\", \n       x = \"Actual share who said they were willing to give 1% of their income\") +\n  theme_bw()  +\n  easy_remove_gridlines(axis = \"both\", minor = TRUE, major = FALSE) +\n  geom_abline(\n    slope = 1, \n    intercept = 0, \n    color = \"grey\", \n    linetype = \"dotted\")\n\n\n\n\n\n\n\n\n\n\ncolours and text\nAdding colour by region makes me realise that there is a row that entity == World, which doesn’t have a region. I need to filtered the World out to avoid an NA category.\nI want to use the same colours as the OWID plot. The ColorZilla web extension allows you to use a dropper to get the specific # codes for each colour on any plot on the internet- handy!\nI have used scale_colour_manual() to set the colours and added a title and subtitle using labs and removed the legend title using a function from the ggeasy package. I have added some country labels, while trying to avoid overlapping the point using the geom_text_repel() function from the ggrepel package. It does give me a warning that there are too many overlapping points though.\n\nwillingness2024 %&gt;%\n  filter(entity != \"World\") %&gt;%\n  ggplot(aes(y =  prediction_others_willingness, x = self_willingness, colour = region)) +\n  geom_point() +\n  scale_y_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                      limits = c(0,100), expand = c(0,0), breaks = seq(0,100,20)) +\n    scale_x_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                       limits = c(0,100), expand = c(0,0), breaks = seq(0,100,20)) +\n  labs(y = \"Predicted share willing to give\", \n       x = \"Actual share who said they were willing to give 1% of their income\", \n       title = \"People underestimate others' willingness to take climate action\", \n       subtitle = \"Participants were asked if they would contribute 1% of their income to tackle \\nclimate change. \\nThe share that answered 'yes' is shown on the horizontal axis. \\nThe share of the population in their country that people think would be willing \\nis shown on the vertical axis.\") +\n  theme_bw()  +\n  easy_remove_gridlines(axis = \"both\", minor = TRUE, major = FALSE) +\n  geom_abline(\n    slope = 1, \n    intercept = 0, \n    color = \"grey\", \n    linetype = \"dotted\") +\n  scale_colour_manual(values = c(\"#a2559b\", \"#00847d\", \"#4b6a9c\", \"#e56e59\", \"#38aaba\", \"#883039\")) +\n  easy_remove_legend_title() +\n  geom_text_repel(aes(label = entity), size = 3, max.overlaps = 20)\n\n\n\n\n\n\n\n\n\n\ninteractivity\nThe plotly packages allows for much of the same kind of interactivity that the grapher tool allows on Our World in Data. Here I have added an extra text argument to the ggplot aes to make the hover option display the country associated with each point, as well as the variables that were mapped. By assigning the ggplot to an object and then using the ggplotly() function, we can produce an interactive plot.\n\nplot &lt;- willingness2024 %&gt;%\n  filter(entity != \"World\") %&gt;%\n  ggplot(aes(y =  prediction_others_willingness, x = self_willingness, \n             colour = region, text = paste(\"country:\", entity))) +\n  geom_point() +\n  scale_y_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                      limits = c(0,100), expand = c(0,0), breaks = seq(0,100,20)) +\n    scale_x_continuous(labels = percent_format(accuracy = 1, scale = 1), \n                       limits = c(0,100), expand = c(0,0), breaks = seq(0,100,20)) +\n  labs(y = \"Predicted share willing to give\", \n       x = \"Actual share who said they were willing to give 1% of their income\", \n       title = \"People underestimate others' willingness to take climate action\", \n       subtitle = \"Participants were asked if they would contribute 1% of their income to tackle climate change. \\nThe share that answered 'yes' is shown on the horizontal axis. The share of the population \\nin their country that people think would be willing is shown on the vertical axis.\") +\n  theme_bw()  +\n  theme(panel.grid.minor = element_blank()) +\n  geom_abline(\n    slope = 1, \n    intercept = 0, \n    color = \"grey\", \n    linetype = \"dotted\") +\n  scale_colour_manual(values = c(\"#a2559b\", \"#00847d\", \"#4b6a9c\", \"#e56e59\", \"#38aaba\", \"#883039\")) +\n  easy_remove_legend_title() \n\ninteractive_plot &lt;- ggplotly(plot)\n\ninteractive_plot"
  },
  {
    "objectID": "charts/2025-04-16_negative/index.html",
    "href": "charts/2025-04-16_negative/index.html",
    "title": "day 16_negative",
    "section": "",
    "text": "Exploring OurWorldinData this morning looking for plots that might fit the negative theme for today and this article caught my eye.\nApparently we vastly underestimate how happy other people are. Much like my Day 1 plot re climate action, when asked how happy they are and how happy they think other people in their country are, we judge others negatively, rating them to be much less happy than we are ourselves.\nSadly I couldn’t find the data from that plot to reproduce it, but in exploring the IPSOS website, I found this article about what worries the world in March 2025. The data was downloadable and I think probably better represented as stacked bars rather than pie charts, so that is my challenge for today."
  },
  {
    "objectID": "charts/2025-04-16_negative/index.html#get-the-data",
    "href": "charts/2025-04-16_negative/index.html#get-the-data",
    "title": "day 16_negative",
    "section": "get the data",
    "text": "get the data\nHere I am reading the data from csv and making it into long format.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(ggeasy)\nlibrary(plotly)\n\nright_wrong &lt;- read_csv(here(\"charts\", \"2025-04-16_negative\", \"ipsos_direction.csv\")) %&gt;%\n  rename(track = Country) \n\n\nright_wrong_long &lt;- right_wrong %&gt;%\n  pivot_longer(names_to = \"country\", values_to = \"percent\", World:Peru) %&gt;%\n  arrange(country, track) \n\nglimpse(right_wrong_long)\n\nRows: 60\nColumns: 3\n$ track   &lt;chr&gt; \"Right Track\", \"Wrong Track\", \"Right Track\", \"Wrong Track\", \"R…\n$ country &lt;chr&gt; \"Argentina\", \"Argentina\", \"Australia\", \"Australia\", \"Belgium\",…\n$ percent &lt;dbl&gt; 59, 41, 48, 52, 36, 64, 35, 65, 31, 69, 27, 73, 33, 67, 15, 58…"
  },
  {
    "objectID": "charts/2025-04-16_negative/index.html#plot",
    "href": "charts/2025-04-16_negative/index.html#plot",
    "title": "day 16_negative",
    "section": "plot",
    "text": "plot\n\nright_wrong_long %&gt;%\n  ggplot(aes(x = country, y = percent, fill = track)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\nBasic plot check! Things I would like to change…\n\nsomething weird is going on with France, that is obviously a data entry error- exclude France\nI would like to order the bars from highest right track to lowest\nfix the theme and colours, add titles and data annotations\n\n\nbar order\nI need to turn country into a factor based on the percent values highest to lowest, but I am not sure how to do that with the data long, because for some countries the max percent value is the Right Track and for other countries the max is the Wrong Track value.\nMaybe I need to make the data wide, with Right and Wrong track in separate columns first??\n\nright_wrong_wide &lt;- right_wrong_long %&gt;%\n  pivot_wider(names_from = track, values_from = percent) %&gt;%\n  clean_names()\n\nglimpse(right_wrong_wide)\n\nRows: 30\nColumns: 3\n$ country     &lt;chr&gt; \"Argentina\", \"Australia\", \"Belgium\", \"Brazil\", \"Canada\", \"…\n$ right_track &lt;dbl&gt; 59, 48, 36, 35, 31, 27, 33, 15, 21, 29, 23, 62, 63, 31, 28…\n$ wrong_track &lt;dbl&gt; 41, 52, 64, 65, 69, 73, 67, 58, 79, 71, 77, 38, 37, 69, 72…\n\n\nNow R thinks that country is characters so lets make it a factor with the levels based on values of right track.\n\nright_wrong_wide &lt;- right_wrong_wide %&gt;%\n  mutate(country = fct_reorder(country, right_track)) \n\nlevels(right_wrong_wide$country)\n\n [1] \"Peru\"          \"France\"        \"South Korea\"   \"Germany\"      \n [5] \"Japan\"         \"Hungary\"       \"Türkiye\"       \"Chile\"        \n [9] \"Netherlands\"   \"Sweden\"        \"Italy\"         \"South Africa\" \n[13] \"Great Britain\" \"Canada\"        \"Israel\"        \"Spain\"        \n[17] \"Colombia\"      \"Brazil\"        \"Belgium\"       \"World\"        \n[21] \"Poland\"        \"US\"            \"Australia\"     \"Thailand\"     \n[25] \"Mexico\"        \"Argentina\"     \"India\"         \"Indonesia\"    \n[29] \"Malaysia\"      \"Singapore\"    \n\n\nGreat! Peru has the lowest value and Singapore has the highest value. Now what happens to the country factor when I make the data long again? Does the order stick around??\n\nright_wrong_long_new &lt;- right_wrong_wide %&gt;%\n  pivot_longer(names_to = \"track\", values_to = \"percent\", right_track:wrong_track) \n\nlevels(right_wrong_long_new$country)\n\n [1] \"Peru\"          \"France\"        \"South Korea\"   \"Germany\"      \n [5] \"Japan\"         \"Hungary\"       \"Türkiye\"       \"Chile\"        \n [9] \"Netherlands\"   \"Sweden\"        \"Italy\"         \"South Africa\" \n[13] \"Great Britain\" \"Canada\"        \"Israel\"        \"Spain\"        \n[17] \"Colombia\"      \"Brazil\"        \"Belgium\"       \"World\"        \n[21] \"Poland\"        \"US\"            \"Australia\"     \"Thailand\"     \n[25] \"Mexico\"        \"Argentina\"     \"India\"         \"Indonesia\"    \n[29] \"Malaysia\"      \"Singapore\"    \n\n\nSUCCESS!!\n\nright_wrong_long_new &lt;- right_wrong_long_new %&gt;%\n  mutate(track = fct_relevel(track, c(\"wrong_track\", \"right_track\")))\n\n\n\nlevels(right_wrong_long_new$track)\n\n[1] \"wrong_track\" \"right_track\"\n\n\n\n\nreplot with new order\n\nright_wrong_long_new %&gt;%\n  filter(country != \"France\") %&gt;%\n  ggplot(aes(x = country, y = percent, fill = track)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\ncolours etc\nI learned a few things about legend control here. I didn’t know previously that you could change the legend title and labels within the scale_fill_manual() function. I also used guides(fill = guide_legend(reverse = TRUE) to reverse the order of the legend display (so that Right appeared above Wrong), even when that order was the opposite of the factor levels.\n\npalette &lt;- c (\"#e94554\", \"#009d9c\")\n\np &lt;- right_wrong_long_new %&gt;%\n  filter(country != \"France\") %&gt;%\n  ggplot(aes(x = country, y = percent, fill = track)) +\n  scale_fill_manual(values = palette, name = \"Direction\", labels = c(\"Wrong\", \"Right\")) +\n  geom_col() +\n  coord_flip() +\n  theme_minimal() +\n  easy_remove_gridlines() +\n  scale_y_continuous(expand = c(0,0)) +\n  labs(y = \"Country\", x = \"Percent of people\", \n       title = \"Percent of the public who rate their country as heading in \\nright direction vs the wrong direction\", \n       caption = \"Source: Ipsos: What Worries The World? March 2025 • \\nBase: Representative sample of 25,746 adults aged 16-74 in 29 participating countries, \\nFebruary 21 2025 - March 7 2025.\") +\n  guides(fill = guide_legend(reverse = TRUE))\n  \np"
  },
  {
    "objectID": "charts/2025-04-16_negative/index.html#add-interactive-with-plotly",
    "href": "charts/2025-04-16_negative/index.html#add-interactive-with-plotly",
    "title": "day 16_negative",
    "section": "add interactive with plotly",
    "text": "add interactive with plotly"
  },
  {
    "objectID": "charts/2025-04-09_diverging/index.html",
    "href": "charts/2025-04-09_diverging/index.html",
    "title": "day 9_diverging",
    "section": "",
    "text": "Our World in Data has a series of plots that report data from the Integrated Values survey, asking people across the world how important they consider a range of life areas (family, friends,work, leisure time, politics, religion. For the Day 6 prompt, I am wondering whether our views on the relative importance of family and work have shifted over time.\n\n\n\nload packages\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggeasy)\nlibrary(RColorBrewer)\n\n\n\nread/combine data\nA bit of wrangling required for this one, because the data for each life area was in a different csv file. In this rather long code chunk, I am reading each csv in, cleaning up names, selecting relavant columns, adding a new column for life area, making the data long, and changing the string values in the ratings column to make them consistent (i.e. instead of very_important_in_life_family -&gt; very_important_in_life).\n\n\nCode\nfamily &lt;- read_csv(\"https://ourworldindata.org/grapher/how-important-family-is-to-people-in-life.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names() %&gt;%\n  select(country = entity, year, ends_with(\"family\")) %&gt;%\n  mutate(life_area = \"family\") %&gt;%\n  pivot_longer(names_to = \"rating\", values_to = \"score\", very_important_in_life_family:no_answer_important_in_life_family) %&gt;%\n  mutate(rating = str_sub(rating, 1, -8)) # edit string, start at pos1, end 8 chars back\n\nfriends &lt;-  read_csv(\"https://ourworldindata.org/grapher/how-important-friends-are-to-people-in-life.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names() %&gt;%\n  select(country = entity, year, ends_with(\"friends\")) %&gt;%\n  mutate(life_area = \"friends\") %&gt;%\n  pivot_longer(names_to = \"rating\", values_to = \"score\", very_important_in_life_friends:no_answer_important_in_life_friends) %&gt;%\n  mutate(rating = str_sub(rating, 1, -9)) \n  \nleisure &lt;- read_csv(\"https://ourworldindata.org/grapher/how-important-leisure-is-to-people-in-life.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names() %&gt;%\n  select(country = entity, year, ends_with(\"leisure_time\")) %&gt;%\n  mutate(life_area = \"leisure_time\") %&gt;%\n  pivot_longer(names_to = \"rating\", values_to = \"score\", very_important_in_life_leisure_time:no_answer_important_in_life_leisure_time) %&gt;%\n  mutate(rating = str_sub(rating, 1, -14)) \n  \npolitics &lt;- read_csv(\"https://ourworldindata.org/grapher/how-important-politics-is-in-your-life.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names() %&gt;%\n  select(country = entity, year, ends_with(\"politics\")) %&gt;%\n  mutate(life_area = \"politics\") %&gt;%\n  pivot_longer(names_to = \"rating\", values_to = \"score\", very_important_in_life_politics:no_answer_important_in_life_politics) %&gt;%\n  mutate(rating = str_sub(rating, 1, -10)) \n\nreligion &lt;-read_csv(\"https://ourworldindata.org/grapher/how-important-religion-is-in-your-life.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names() %&gt;%\n  select(country = entity, year, ends_with(\"religion\")) %&gt;%\n  mutate(life_area = \"religion\") %&gt;%\n  pivot_longer(names_to = \"rating\", values_to = \"score\", very_important_in_life_religion:no_answer_important_in_life_religion) %&gt;%\n  mutate(rating = str_sub(rating, 1, -10)) \n  \nwork &lt;- read_csv(\"https://ourworldindata.org/grapher/how-important-work-is-to-people-in-life.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names() %&gt;%\n  select(country = entity, year, ends_with(\"work\")) %&gt;%\n  mutate(life_area = \"work\") %&gt;%\n  pivot_longer(names_to = \"rating\", values_to = \"score\", very_important_in_life_work:no_answer_important_in_life_work) %&gt;%\n  mutate(rating = str_sub(rating, 1, -6)) \n\n\nOnce I have separate dataframes for each life area that are all structured in the same way, I can use rbind() to join them all togeher.\n\nlife &lt;- rbind(family, friends, leisure, work, politics, religion)\n\n\n\nclean it up\nA little more string editing, this time chopping the _in_life piece of the end of each of the rating strings and converting both rating and life area to factors.\n\nlife_ratings &lt;- life %&gt;%\n  mutate(rating = str_sub(rating, 1, -9)) %&gt;%\n  mutate(rating = as_factor(rating)) %&gt;%\n  mutate(life_area = as_factor(life_area))\n\nglimpse(life_ratings)\n\nRows: 14,328\nColumns: 5\n$ country   &lt;chr&gt; \"Albania\", \"Albania\", \"Albania\", \"Albania\", \"Albania\", \"Alba…\n$ year      &lt;dbl&gt; 1998, 1998, 1998, 1998, 1998, 1998, 2004, 2004, 2004, 2004, …\n$ life_area &lt;fct&gt; family, family, family, family, family, family, family, fami…\n$ rating    &lt;fct&gt; very_important, rather_important, not_very_important, notata…\n$ score     &lt;dbl&gt; 95.7958000, 3.2032030, 0.3003003, 0.2002002, 0.5005005, 0.00…\n\n\nI am using the same selection of countries that I plotted in my sex ratio chart, for no good reason really. I am interested in just data for the family and work life areas and want to sum the proportion of people that have “very important” and “rather important” ratings so group the data by year, country and life area and sum the scores.\n\ncountries &lt;- c(\"China\", \"India\", \"South Korea\", \"Australia\", \"United States\", \"United Kingdom\")\nf_w &lt;- c(\"family\", \"work\")\nimpt &lt;- c(\"very_important\", \"rather_important\")\n\nwork_family &lt;- life_ratings %&gt;%\n  filter(life_area %in% f_w) %&gt;%\n  filter(country %in% countries) %&gt;%\n  filter(rating  %in% impt) %&gt;%\n  group_by(year, country, life_area) %&gt;%\n  summarise(important = sum(score)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'year', 'country'. You can override using\nthe `.groups` argument.\n\n\n\n\nplot family and work\n\nwork_family %&gt;%\n  ggplot(aes(x = year, y = important, colour = life_area)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ country) +\n  labs(title = \"Percent of people who rate family and work as `very important` \\nor `rather important`\")\n\n\n\n\n\n\n\n\nBasic plot check! Things I would like to change…\n\ntheme and axis labels\nscale issues\ncolour scheme\nadd subtitle caption\n\n\nwork_family %&gt;%\n  ggplot(aes(x = year, y = important, colour = life_area)) +\n  geom_point(size = 2) +\n  geom_line() +\n  facet_wrap(~ country) +\n  labs(title = \"How important family and work are to people in life\", \n       subtitle = \"Share of survey respondents rating `very important` or `rather important`\", \n       x = \"Year\", y = \"Percent of people\", \n       caption = \"Data source: Integrated Values Surveys (2022)\") +\n  theme_minimal() +\n  scale_y_continuous(limits = c(70,100)) +\n  scale_x_continuous(limits = c(1990, 2022)) +\n  scale_color_brewer(palette = \"Set1\") +\n  easy_add_legend_title(\"Life Area\")"
  },
  {
    "objectID": "charts/2025-04-27_noise/index.html",
    "href": "charts/2025-04-27_noise/index.html",
    "title": "day 27 noise",
    "section": "",
    "text": "The Day 27 prompt is noise and I am interested in finding some data illustrating how noise pollution might impact our health. It was a bit hard to find interesting plots that had open data for this theme, but this Super Bowl noise data from Apple Hearing Study was available on Github.\nI think the plots in this writeup are interesting, both because it seems that game days (red line) are noisier than regular Sundays (dotted black) and that the magnitude of that difference is bigger in game states. Also if you tab across these panels, it seems that games are getting noisier over this period.\nThat said, I really dislike the way the authors have messed with the y axis scale here. It is possible that there has been some smoothing going on too. Lets see what this data looks like without that manipulation."
  },
  {
    "objectID": "charts/2025-04-27_noise/index.html#read-in-data",
    "href": "charts/2025-04-27_noise/index.html#read-in-data",
    "title": "day 27 noise",
    "section": "read in data",
    "text": "read in data\n\n\nCode\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggeasy)\nlibrary(ggannotate)\n\nsb21 &lt;- read_csv(\"https://raw.githubusercontent.com/Yingt2023/Super-Bowl/refs/heads/main/SuperBowl_2021.csv\") %&gt;%\n  clean_names() %&gt;%\n  mutate(year = \"2021\")\n\nsb22 &lt;- read_csv(\"https://raw.githubusercontent.com/Yingt2023/Super-Bowl/refs/heads/main/SuperBowl_2022.csv\") %&gt;%\n   clean_names() %&gt;%\n  mutate(year = \"2022\")\n\nsb23 &lt;- read_csv(\"https://raw.githubusercontent.com/Yingt2023/Super-Bowl/refs/heads/main/SuperBowl_2023.csv\") %&gt;%\n   clean_names() %&gt;%\n  mutate(year = \"2023\")\n\nsb24 &lt;- read_csv(\"https://raw.githubusercontent.com/Yingt2023/Super-Bowl/refs/heads/main/SuperBowl_2024.csv\") %&gt;%\n   clean_names() %&gt;%\n  mutate(year = \"2024\")\n\nsb &lt;- rbind(sb21, sb22, sb23, sb24) %&gt;%\n  select(year, super_bowl_sunday, game_zone, hours_to_game_begin, avg_leq)\n\nglimpse(sb)\n\n\nRows: 512\nColumns: 5\n$ year                &lt;chr&gt; \"2021\", \"2021\", \"2021\", \"2021\", \"2021\", \"2021\", \"2…\n$ super_bowl_sunday   &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\"…\n$ game_zone           &lt;chr&gt; \"Non-game States\", \"Non-game States\", \"Non-game St…\n$ hours_to_game_begin &lt;dbl&gt; -1.5, 6.5, 2.5, 0.5, 0.5, 6.0, -3.0, 4.5, 3.5, 2.0…\n$ avg_leq             &lt;dbl&gt; 66.11693, 54.07987, 65.76910, 65.91371, 67.20883, …\n\n\n\nmake plot\n\nsb %&gt;%\n  ggplot(aes(x = hours_to_game_begin, y = avg_leq, colour = super_bowl_sunday)) +\n  geom_line() +\n  facet_grid(year~game_zone)\n\n\n\n\n\n\n\n\nBasic plot check! Things I would like to change…\n\ntheme and gridlines\ncolours and linetype\nscale (is it possible to reproduce the weird y axis scale?)\ngrey rectangle highlight\nannotations and titles\n\n\n\ntheme/colours\nHere I am getting rid of the grey background with theme_minimal and using ggeasy::easy_remove_gridlines to remove the vertical grid. Took me a while to realise that I had to specify both colour and linetype in my main aesthetic, in order to make one line block and dashed and the other solid red.\n\n\nCode\nsb %&gt;%\n  ggplot(aes(x = hours_to_game_begin, y = avg_leq, \n             colour = super_bowl_sunday, linetype = super_bowl_sunday)) +\n  geom_line() +\n  facet_grid(year~game_zone) +\n  theme_minimal() +\n   easy_remove_gridlines(axis = c(\"x\")) +\n    scale_colour_manual(values = c(\"black\", \"red\")) +\n   scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  easy_remove_legend()\n\n\n\n\n\n\n\n\n\n\n\ngrey box\nAdding a grey box using annotate() to mark the game time period and fixing the x axis scale.\n\n\nCode\nsb %&gt;%\n  ggplot(aes(x = hours_to_game_begin, y = avg_leq, \n             colour = super_bowl_sunday, linetype = super_bowl_sunday)) +\n  geom_line() +\n  facet_grid(year~game_zone) +\n  theme_minimal() +\n   easy_remove_gridlines(axis = c(\"x\")) +\n    scale_colour_manual(values = c(\"black\", \"red\")) +\n   scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  easy_remove_legend() +\n  annotate(\"rect\", xmin = 0, xmax = 3.5, ymin = -Inf, ymax = Inf, alpha = 0.1, fill = \"darkgrey\") +\n    scale_x_continuous(limits = c(-6, 9), breaks= seq(-6, 9, 3)) \n\n\n\n\n\n\n\n\n\n\n\ntitle/annotations\nFixing the axis labels, adding a title and annotations\n\n\nCode\nsb %&gt;%\n  ggplot(aes(x = hours_to_game_begin, y = avg_leq, \n             colour = super_bowl_sunday, linetype = super_bowl_sunday)) +\n  geom_line() +\n  facet_grid(year~game_zone) +\n  theme_minimal() +\n   easy_remove_gridlines(axis = c(\"x\")) +\n    scale_colour_manual(values = c(\"black\", \"red\")) +\n   scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  easy_remove_legend() +\n  annotate(\"rect\", xmin = 0, xmax = 3.5, ymin = -Inf, ymax = Inf, alpha = 0.1, fill = \"darkgrey\") +\n    scale_x_continuous(limits = c(-6, 9), breaks= seq(-6, 9, 3)) +\n   scale_y_continuous(breaks= seq(50, 70, 10)) +\n  labs(title = \"Super Bowl Sunday noise exposure in decibels\", x = \"Hours from start of Super Bowl\", y = \"Average noise exposure (in decibels)\", caption = \"Data from Apple Hearing Study\") +\n  geom_text(data = data.frame(x = -1.3, y = 53, label = \"Start game \\n6:30 PM EST\", game_zone = \"Game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 2.3, y = 53, label = \"End game \\n10:00 PM EST\", game_zone = \"Game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = -1.3, y = 53, label = \"Start game \\n6:30 PM EST\", game_zone = \"Non-game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 2.3, y = 53, label = \"End game \\n10:00 PM EST\", game_zone = \"Non-game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  theme(panel.spacing = unit(1, \"lines\"))\n\n\n\n\n\n\n\n\n\n\n\nbonus\n\nsplit by year and use map() to create separate plots\nI want to create a panel set like the one at the top of the post with my plots across years, so have used this code to map the ggplot code across each year, exporting each plot to png.\n\n\nCode\n# Split the data by year and create separate plots\nyear_plots &lt;- sb %&gt;%\n  group_split(year) %&gt;%\n  map(~{\n    year_val &lt;- unique(.x$year)\n    ggplot(.x, aes(x = hours_to_game_begin, y = avg_leq, \n                   colour = super_bowl_sunday, linetype = super_bowl_sunday)) +\n      geom_line() +\n      facet_grid(. ~ game_zone) +\n      ggtitle(paste(\"Super Bowl Noise -\", year_val)) +\n      theme_minimal() +\n   easy_remove_gridlines(axis = c(\"x\")) +\n    scale_colour_manual(values = c(\"black\", \"red\")) +\n   scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  easy_remove_legend() +\n  annotate(\"rect\", xmin = 0, xmax = 3.5, ymin = -Inf, ymax = Inf, alpha = 0.1, fill = \"darkgrey\") +\n    scale_x_continuous(limits = c(-6, 9), breaks= seq(-6, 9, 3)) +\n   scale_y_continuous(breaks= seq(50, 70, 5)) +\n  labs( x = \"Hours from start of Super Bowl\", y = \"Average noise exposure (in decibels)\", caption = \"Data from Apple Hearing Study\") +\n        ggtitle(paste(\"Super Bowl Noise -\", year_val)) +\n  geom_text(data = data.frame(x = -1.3, y = 53, label = \"Start game \\n6:30 PM EST\", game_zone = \"Game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 2.3, y = 53, label = \"End game \\n10:00 PM EST\", game_zone = \"Game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = -1.3, y = 53, label = \"Start game \\n6:30 PM EST\", game_zone = \"Non-game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 2.3, y = 53, label = \"End game \\n10:00 PM EST\", game_zone = \"Non-game States\", year = \"2024\"),\nmapping = aes(x = x, y = y, label = label),\nsize = 2, inherit.aes = FALSE) +\n  theme(panel.spacing = unit(1, \"lines\"))\n  }) \n\n\n# save all plots\nwalk2(\n  year_plots,\n  unique(sb$year),\n  ~ggsave(filename = paste0(\"superbowl_noise_\", .y, \".png\"), \n          plot = .x, width = 10, height = 6)\n)"
  },
  {
    "objectID": "charts/2025-04-11_stripes/index.html",
    "href": "charts/2025-04-11_stripes/index.html",
    "title": "day 11_stripes",
    "section": "",
    "text": "Departing from Our World in Data today to try and make a “show your stripes” temperature plot.\n\nThis was much easier than I expected because I just followed these beautiful instructions from Dominic Roye.\nData from StatsNZ.\n\nset up\nHere I am loading packages and defining theme_strip (code copied from Dominic’s blog)\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(RColorBrewer)\nlibrary(ggeasy)\n\n\ntheme_strip &lt;- function(){ \n  \n  theme_minimal() %+replace%\n  theme(\n    axis.text.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.title = element_blank(),\n    panel.grid.major = element_blank(),\n    legend.title = element_blank(),\n    axis.text.x = element_text(vjust = 3),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.key.width = unit(.5, \"lines\")\n  )\n}\n\ncol_strip &lt;- brewer.pal(11, \"RdBu\")\n\n\n\nread the data\nHere I am reading the data from Stats NZ and filtering it to only include the site closest to where I live.\nThe dataset had daily temperature values and I really only needed the average temp for each year so I group_by year and summarise the mean temperature.\n\ntemp &lt;- read_csv(here(\"charts\", \"2025-04-11_stripes\", \"daily-temperature-for-30-sites-to-2022-part2.csv\"))\n\nq &lt;- temp %&gt;%\n  filter(site ==  \"Queenstown (Otago)\") %&gt;%\n  mutate(site = str_sub(site, 1, -9)) \n\nqmean &lt;- q %&gt;%\n  filter(statistic == \"Average\") %&gt;%\n  group_by(year(date)) %&gt;%\n  summarise(annual = mean(temperature)) %&gt;%\n  rename(date = `year(date)`)\n\nglimpse(qmean)\n\nRows: 51\nColumns: 2\n$ date   &lt;dbl&gt; 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 198…\n$ annual &lt;dbl&gt; 9.631421, 9.964384, 9.929041, 9.853973, 9.092077, 8.694247, 9.8…\n\n\n\n\nplot\nI hadn’t used geom_tile before. Here I am defining the colour of the tile fill to be the annual average temperature.\n\nqmean %&gt;%\n    ggplot(aes(x = date, y = 1, fill = annual)) +\n  geom_tile() \n\n\n\n\n\n\n\n\nTo get the colour scale to represent how far the annual temperature is from average, this chunk defines the min, max and mean across the whole dataset and then uses scale_fill_gradient() to colour the tiles.\n\nmaxmin &lt;- range(qmean$annual, na.rm = T)\nmd &lt;- mean(qmean$annual, na.rm = T)\n\n\nqmean %&gt;%\n    ggplot(aes(x = date, y = 1, fill = annual)) +\n  geom_tile() +\n  scale_fill_gradientn(colors = rev(col_strip), \n                       values = scales::rescale(c(maxmin[1], md, maxmin[2])), \n                       na.value = \"gray80\") +\n  scale_x_continuous(limits = c(1972, 2022), expand = c(0,0), breaks = seq(1972,2022, 10)) +\n  labs(\n    title = \"Queenstown 1972-2022\",\n    caption = \"Data: Stats NZ\", \n    x = \"Year\") +\n  coord_cartesian(expand = FALSE) +\n  theme_strip() +\n  easy_remove_axes(which = \"x\")\n\n\n\n\n\n\n\n\nToo easy! Thanks Dominic!"
  },
  {
    "objectID": "charts/2025-04-02_slope/index.html",
    "href": "charts/2025-04-02_slope/index.html",
    "title": "day 2 slope",
    "section": "",
    "text": "This plot from Our World in Data illustrates the amount of time that we spend with others and how that changes with age. The slope of line plotting the amount of time that people spend alone is pretty terrifying. Lets see if we can reproduce that plot using ggplot.\n\n\n\nload packages\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(ggeasy)\nlibrary(ggannotate)\n\n# remotes::install_github(\"mattcowgill/ggannotate\")\n\n\n\nread in the data\nHere I am reading the data using the owidapi package and selecting and renaming columns within the same line of code. I also want to make the data long so that the category information appears in a single column and the hours values in another.\nI know that colours are often easier to deal with if we turn character variables into factors so that we can set the order. Here I use fct_relevel() to order the categories so that I can easily line up which colour is associated with each.\n\ntime &lt;- read_csv(\"https://ourworldindata.org/grapher/time-spent-with-relationships-by-age-us.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n          clean_names() %&gt;%\nselect(group = entity, age = year, \n         alone = t_who_category_alone, friend = t_who_category_friend, \n         children = t_who_category_children, family = t_who_category_family, \n         partner = t_who_category_partner, coworker = t_who_category_co_worker) %&gt;%\n  pivot_longer(names_to = \"category\", values_to = \"hours\", alone:coworker)\n\nRows: 201 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Entity\ndbl (7): Year, t__who_category_alone, t__who_category_friend, t__who_categor...\nlgl (1): Code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntime$category &lt;- fct_relevel(time$category, c(\"alone\", \"partner\", \"family\", \"children\", \"friend\", \"coworker\"))\n\nlevels(time$category) #check levels\n\n[1] \"alone\"    \"partner\"  \"family\"   \"children\" \"friend\"   \"coworker\"\n\nglimpse(time)\n\nRows: 1,206\nColumns: 4\n$ group    &lt;chr&gt; \"All people\", \"All people\", \"All people\", \"All people\", \"All …\n$ age      &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 1…\n$ category &lt;fct&gt; alone, friend, children, family, partner, coworker, alone, fr…\n$ hours    &lt;dbl&gt; 3.629829200, 1.584973700, 0.363883400, 4.324233500, 0.0000000…\n\n\n\n\nmake basic plot\nThe dataset contains time use data for Male and Female separate, but lets start with a plot of All people.\n\ntime %&gt;%\n  filter(group == \"All people\") %&gt;%\n  ggplot(aes(x = age, y = hours, colour = category)) +\n  geom_point() + \n  geom_line() \n\n\n\n\n\n\n\n\nBasic plot… check! Here is a list of things I would like to change…\n\ntheme and colours\nadd horizontal gridlines\nfilter out data point more than 80\nstart x axis at 15, 30 then in increments of 10\ny axis add h to numbers\nadd group annotations rather than legend\n\n\n\ntheme and colours\nI started by filtering out data with age values greater than 80. I used the ColorZilla web extension to get the # codes for the colours in the Our World in Data plot and scale_colour_manual() to set the colours.\nI fixed the y axis using scale_y_continuous() breaks and got most of the way there with theme_minimal(). The easy_remove_gridlines() function from ggeasy makes it easy to control which gridlines you want to display. Here I want to remove both major and minor verticial lines but keep the major horizontal lines. I want the gridlines to be dotted and use the theme(panel.grid = element_line() to achieve that.\n\ntime %&gt;%\n  filter(age &lt; 80) %&gt;%\n  filter(group == \"All people\") %&gt;%\n  ggplot(aes(x = age, y = hours, colour = category)) +\n  geom_point(size = 1) + \n  geom_line() +\n  scale_colour_manual(values = c(\"#496899\", \"#6b3d8d\", \"#2b8465\", \"#986d39\", \"#b03508\", \"#883039\")) +\n  scale_y_continuous(expand = c(0,0), breaks = seq(0,8,1)) +\n  theme_minimal() +\n  easy_remove_gridlines(axis = \"x\", major = TRUE, minor = TRUE) +\n  easy_remove_gridlines(axis = \"y\", major = FALSE, minor = TRUE) +\n  theme(panel.grid = element_line(linewidth = 0.4, linetype = 2))\n\n\n\n\n\n\n\n\n\n\naxes\nNow theme_minimal() removes the x and y axis lines and ticks. I want to add the line/ticks back in on just the x axis so use theme(axis.line.x = element_line()and theme(axis.ticks.x = element_line().\n\ntime %&gt;%\n  filter(age &lt; 80) %&gt;%\n  filter(group == \"All people\") %&gt;%\n  ggplot(aes(x = age, y = hours, colour = category)) +\n  geom_point(size = 1) + \n  geom_line() +\n  scale_colour_manual(values = c(\"#496899\", \"#6b3d8d\", \"#2b8465\", \"#986d39\", \"#b03508\", \"#883039\")) +\n  theme_minimal() +\n  scale_y_continuous(expand = c(0,0), limits = c(-0.05,8.1), breaks = seq(0,9,1)) +\n  scale_x_continuous(breaks=c(15,30,40,50,60,70,80)) +\n  easy_remove_gridlines(axis = \"x\") +\n  easy_remove_gridlines(axis = \"y\", major = FALSE, minor = TRUE) +\n  theme(panel.grid = element_line(linewidth = 0.4, linetype = 2)) +\n  theme(axis.line.x = element_line(linewidth = 0.2, colour = \"darkgrey\", linetype=1), \n        axis.ticks.x = element_line(linewidth = 0.5, color=\"darkgrey\")\n        )\n\n\n\n\n\n\n\n\n\n\nannotations\nThe easy_remove_legend() function from the ggeasy package makes it easy to get rid of a legend; getting annotations on the end of the each line is a bit tricker. I’m sure there is a simpler way, but the ggannotate package by Matt Cowgill pulls up a shiny panel that allows you to place annotations where you want them and then copy the geom_text() code into your chunk.\n\n  time %&gt;%\n  filter(age &lt; 80) %&gt;%\n  filter(group == \"All people\") %&gt;%\n  ggplot(aes(x = age, y = hours, colour = category)) +\n  geom_point(size = 1) + \n  geom_line() +\n  scale_colour_manual(values = c(\"#496899\", \"#6b3d8d\", \"#2b8465\", \"#986d39\", \"#b03508\", \"#883039\")) +\n  theme_minimal() +\n  scale_y_continuous(expand = c(0,0), limits = c(-0.05,8.1), breaks = seq(0,9,1)) +\n  scale_x_continuous(breaks=c(15,30,40,50,60,70,80)) +\n  easy_remove_gridlines(axis = \"x\") +\n  easy_remove_gridlines(axis = \"y\", major = FALSE, minor = TRUE) +\n  theme(panel.grid = element_line(linewidth = 0.4, linetype = 2)) +\n  theme(axis.ticks.x =   element_line(linewidth = 0.5, color=\"darkgrey\") , \n        axis.line.x = element_line(linewidth = 0.2, colour = \"darkgrey\", linetype=1)) +\n  easy_remove_legend() +\n  geom_text(data = data.frame(x = 82, y = 8, \n    label = \"Alone\"), aes(x = x, y = y, label = label), size = 3, colour = \"#496899\") +\n  geom_text(data = data.frame(x = 82, y = 4.4, \n    label = \"With \\npartner\"), aes(x = x, y = y, label = label), size = 3,colour = \"#6b3d8d\") +\n  geom_text(data = data.frame(x = 80, y = 1.3, \n    label = \"With family\"),aes(x = x, y = y, label = label), size = 3, colour = \"#2b8465\") +\n  geom_text(data = data.frame(x = 83.5, y = 0.7, \n    label = \"With children\"), aes(x = x, y = y, label = label), size = 2.5,colour = \"#986d39\") +\n geom_text(data = data.frame(x = 83.5, y = 0.5, \n    label = \"With friends\"), aes(x = x, y = y, label = label), size = 2.5,colour = \"#b03508\") +\n  geom_text(data = data.frame(x = 83.5, y = 0.1, \n    label = \"With coworkers\"), aes(x = x, y = y, label = label), size = 2.5, colour = \"#883039\")\n\n\n\n\n\n\n\n\n\n\ntitles and captions\nFinishing touches… title, subtitle and caption. The theme(plot.caption = element_text(hjust = 0)) makes the caption appear on the left side of the plot.\n\ntime %&gt;%\n  filter(age &lt; 80) %&gt;%\n  filter(group == \"All people\") %&gt;%\n  ggplot(aes(x = age, y = hours, colour = category)) +\n  geom_point(size = 1) + \n  geom_line() +\n  scale_colour_manual(values = c(\"#496899\", \"#6b3d8d\", \"#2b8465\", \"#986d39\", \"#b03508\", \"#883039\")) +\n  theme_minimal() +\n  scale_y_continuous(expand = c(0,0), limits = c(-0.05,8.1), breaks = seq(0,9,1)) +\n  scale_x_continuous(breaks=c(15,30,40,50,60,70,80)) +\n  easy_remove_gridlines(axis = \"x\") +\n  easy_remove_gridlines(axis = \"y\", major = FALSE, minor = TRUE) +\n  theme(panel.grid = element_line(linewidth =  0.4, linetype = 2)) +\n  theme(axis.ticks.x =  element_line(linewidth = 0.5, color=\"darkgrey\") , \n        axis.line.x = element_line(linewidth = 0.2, colour = \"darkgrey\", linetype=1)) +\n  easy_remove_legend() +\n  ### the geom_text code below are created using the ggannotate package\n  geom_text(data = data.frame(x = 82, y = 7.8, \n    label = \"Alone\"), aes(x = x, y = y, label = label), size = 3, colour = \"#496899\") +\n  geom_text(data = data.frame(x = 82, y = 4.4, \n    label = \"With \\npartner\"), aes(x = x, y = y, label = label), size = 3,colour = \"#6b3d8d\") +\n  geom_text(data = data.frame(x = 80, y = 1.3, \n    label = \"With family\"),aes(x = x, y = y, label = label), size = 3, colour = \"#2b8465\") +\n  geom_text(data = data.frame(x = 83.5, y = 0.7, \n    label = \"With children\"), aes(x = x, y = y, label = label), size = 2.5,colour = \"#986d39\") +\n geom_text(data = data.frame(x = 83.5, y = 0.5, \n    label = \"With friends\"), aes(x = x, y = y, label = label), size = 2.5,colour = \"#b03508\") +\n  geom_text(data = data.frame(x = 83.5, y = 0.1, \n    label = \"With coworkers\"), aes(x = x, y = y, label = label), size = 2.5, colour = \"#883039\") +\n  labs(title = \"Who Americans spend their time with, by age, All people\", \n       subtitle = \"Measured in hours per day, based on averages from surveys in the United States \\nbetween 2010 and 2023\", \n       x = \"Age\", \n       y = \"Hours\",\n       caption = \"Data source: U.S. Bureau of Labor Statistics (2023). \\nNote: Activities such as sleeping, grooming, and personal care are not included in the data. \\nRelationships used to categorize people are not exhaustive and time spent with multiple people counts toward all \\n(e.g., attending a party with friends and partner counts toward both friends and partner)\") +\n  theme(plot.caption = element_text(hjust = 0)) # make the caption appear on the left\n\n\n\n\n\n\n\n\n\n\nbonus\nAt this point I am wondering whether who people spend their time with differs for men and women. Here I have changed the filter to include both Men and Women and added a facet_wrap(~group) to plot the gender data separately.\n\ntime %&gt;%\n  filter(age &lt; 80) %&gt;%\n  filter(group %in% c(\"Men\", \"Women\")) %&gt;%\n  ggplot(aes(x = age, y = hours, colour = category)) +\n  geom_point(size = .6) + \n  geom_line() +\n  facet_wrap(~ group) +\n  scale_colour_manual(values = c(\"#496899\", \"#6b3d8d\", \"#2b8465\", \"#986d39\", \"#b03508\", \"#883039\")) +\n  theme_minimal() +\n  scale_y_continuous(expand = c(0,0), limits = c(-0.05,10), breaks = seq(0,9,1)) +\n  scale_x_continuous(breaks=c(15,30,40,50,60,70,80)) +\n  easy_remove_gridlines(axis = \"x\") +\n  easy_remove_gridlines(axis = \"y\", major = FALSE, minor = TRUE) +\n  theme(panel.grid = element_line(linewidth =  0.4, linetype = 2)) +\n  theme(axis.ticks.x =  element_line(linewidth = 0.5, color=\"darkgrey\") , \n        axis.line.x = element_line(linewidth = 0.2, colour = \"darkgrey\", linetype=1)) +\n  labs(title = \"Who Americans spend their time with, by age, Men and Women\", \n        caption = \"Data source: U.S. Bureau of Labor Statistics (2023)\", \n              x = \"Age\", \n       y = \"Hours\") +\n    theme(plot.caption = element_text(hjust = 0)) \n\n\n\n\n\n\n\n\nIt seems that women in their later years spend more time that men alone and less time with a partner, presumably because they are living longer than the men in their lives."
  },
  {
    "objectID": "charts/2025-04-17_birds/index.html",
    "href": "charts/2025-04-17_birds/index.html",
    "title": "day 17_birds",
    "section": "",
    "text": "library(tidyverse)\nlibrary(janitor)\nlibrary(ggbump)\nlibrary(ggeasy)\n\nIt is Day 17 and the theme is birds. I am interested in the results of the Forest and Bird New Zealand Bird of the Year competition over the past few years.\nOn Wikipedia I found tables of the 2022-2024 results but I had to get the 2021 data from the press release.\nThe datapasta package from Miles McBain makes copying and pasting data from the wild into RStudio really easy. The cleaning required was a bit unwieldy though, so this separate .R script gets the data in for each year, combines it into a single dataframe and fixes all the annoying inconsistencies in bird naming across years, before writing it to csv.\n\nload packages/read data\n\nlibrary(tidyverse)\nlibrary(datapasta)\nlibrary(janitor)\nlibrary(ggbump)\nlibrary(ggeasy)\n\ntop10_year &lt;- read_csv(here::here(\"charts\", \"2025-04-17_birds\", \"top10.csv\"))\n\nglimpse(top10_year)\n\nRows: 40\nColumns: 5\n$ bird        &lt;chr&gt; \"Pekapeka-tou-roa / Long-tailed bat\", \"Kākāpō\", \"Titipouna…\n$ votes       &lt;dbl&gt; 2894, 3351, 1852, 1594, 1468, 1228, 1302, 1260, 1184, 1477…\n$ year        &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021…\n$ rank_number &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, …\n$ maori_name  &lt;chr&gt; \"Pekapeka-tou-roa\", \"Kākāpō\", \"Titipounamu\", \"Kea\", \"Toroa…\n\n\n\n\nplot ranking\nThis is rank data so I am trying ggbump()again.\n\n\nCode\ntop10_year %&gt;%\nggplot(aes(year, rank_number, color = maori_name)) +\n  geom_point(size = 5) +\n   geom_bump(linewidth = 2, smooth = 8) +\n      scale_y_reverse() +\n  scale_x_continuous(limits = c(2019.8, 2025),\n                     breaks = seq(2021, 2024, 1)) +\n theme_minimal() +\n  easy_remove_legend() +\n  easy_remove_gridlines()  +\n   geom_text(data = top10_year %&gt;% filter(year == min(year)),\n            aes(x = year - .1, label = maori_name), size = 3.5, hjust = 1) +\n  geom_text(data = top10_year %&gt;% filter(year == max(year)),\n            aes(x = year + .1, label = maori_name), size = 3.5, hjust = 0) +\n  scale_y_reverse(breaks = seq(1 , 10, 1), limits= c(10,0)) +\n  geom_text(data = data.frame(x = 2023, y = 0.5, label = \"Pūteketeke\", size = 1.5),\nmapping = aes(x = x, y = y, label = label),\ncolour = \"#00aafd\", inherit.aes = FALSE) +\n  geom_text(data = data.frame(x = 2022, y = 0.5, label = \"Pīwauwau\", size = 1.5),\nmapping = aes(x = x, y = y, label = label),\ncolour = \"#01B7EB\", inherit.aes = FALSE) +\n  labs(x = \"Year\",  y = \"Rank\", title = \"Forest & Bird New Zealand\", subtitle = \"Bird of the Year Ranking: 2021-2024\")\n\n\n\n\n\n\n\n\n\n\n\nControversy\nThe ranking data is interesting but it hides a number of controversies that have plagued the Bird of the year competition in recent years.\n\n2021: not even a bird\n\n\n\nIn 2021, the Long-tailed bat (Pekapeka-tou-roa) won the Bird of the Year, despite not being a bird\n\n\nThe Long-tailed bat (Pekapeka-tou-roa) is New Zealand’s only native land mammal and caused a bit of a stir when it was voted as New Zealand’s favourite bird in 2021.\n\n\nCode\nd21 &lt;- top10_year %&gt;%\n  filter(year == 2021) %&gt;%\n  mutate(maori_name = fct_reorder(maori_name, rank_number))\n\n\nd21 %&gt;%\n  ggplot(aes(x = rank_number, y = votes, fill = maori_name)) +\n  geom_col() +\n  scale_x_continuous(breaks = seq(1,10, 1)) +\n  scale_y_continuous(limits = c(0,5000)) +\n  labs(y = \"Number #1 Votes\", x = \"Rank\", title = \"Bird of the Year 2021\", subtitle= \"Winner = Pekapeka-tou-roa / Long-tailed bat\",\n       caption = \"note: the BOTY competition uses a single transferable vote system; the bird \\nwith the most Number #1 Votes does not necessarily win\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\n\n2022: Rock Wren beats the Little Penguin\n\n\n\nIn 2022, the NZ Rock Wren beat the Little Penguin\n\n\nThe results in 2022 were more typical. While the Little Penguin / Kororā technically received the most #1 votes, it was pipped at the post by the New Zealand Rock Wren / Pīwauwau, a tiny alpine bird that lives in the mountains of the South Island.\n\n\nCode\nd22 &lt;- top10_year %&gt;%\n  filter(year == 2022) %&gt;%\n  mutate(maori_name = fct_reorder(maori_name, rank_number))\n\nd22 %&gt;%\n  ggplot(aes(x = rank_number, y = votes, fill = maori_name)) +\n  geom_col() +\n  scale_x_continuous(breaks = seq(1,10, 1)) +\n   scale_y_continuous(limits = c(0,5000)) +\n  labs(y = \"Number #1 Votes\", x = \"Rank\", title = \"Bird of the Year 2022\", subtitle= \"Winner = Pīwauwau / Rock wren\",\n       caption = \"note: the BOTY competition uses a single transferable vote system; the bird \\nwith the most Number #1 Votes does not necessarily win\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Paired\", name = \"Maori name\")\n\n\n\n\n\n\n\n\n\n\n\n2023: Pūteketeke pandemonium\n\n\n\nThe Puteketeke (Australasian Crested Grebe) won by an impressive margin in 2023.\n\n\nForest and Bird were celebrating their 100 year anniversary in 2023, so dubbed the competition “Bird of the Century” that year. This may have been what attracted international attention and the subsequent pandemonium.\nThe Australasian crested grebe / Pūteketeke received almost 300K votes due to a campaign launched by British/American comedian and late night show host John Oliver.\nThe campaign, described as “alarmingly aggressive” by some, involved placing billboards across the world, from Wellington NZ…\n\n\n\nBillboard in Wellington, NZ\n\n\n… to Manitowoc WI.\n\n\n\nBillboard in Manitowoc, WI\n\n\nJohn even appeared on the Jimmy Fallon show wearing a Pūteketeke costume.\n\n\nThe result of the campaign was a resounding win for the Pūteketeke and an impressive boost in competition interest and donations to Forest and Bird.\n\n\nCode\noptions(scipen = 999)\n\nd23 &lt;- top10_year %&gt;%\n  filter(year == 2023) %&gt;%\n  mutate(maori_name = fct_reorder(maori_name, rank_number))\n\n\n d23 %&gt;%\n  ggplot(aes(x = rank_number, y = votes, fill = maori_name)) +\n   geom_col() +\n  scale_x_continuous(breaks = seq(1,10, 1)) +\n     scale_y_continuous(limits = c(0,350000)) +\n  labs(y = \"Number #1 Votes\", x = \"Rank\", title = \"Bird of the Century 2023\", subtitle= \"Winner = Pūteketeke / Australasian crested grebe\",\n       caption = \"Note: some argue that the 2023 results were \\nimpacted by foreign election interference\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Paired\", name = \"Maori name\")\n\n\n\n\n\n\n\n\n\n\n\n2024: Yellow-eyed Penguin\n\n\n\nYellow-eyed Penguin, Bird of the Year 2024\n\n\n\n\nCode\nd24 &lt;- top10_year %&gt;%\n  filter(year == 2024) %&gt;%\n  mutate(maori_name = fct_reorder(maori_name, rank_number))\n\n  \n d24 %&gt;%\n  ggplot(aes(x = rank_number, y = votes, fill = maori_name)) +\n   geom_col() +\n  scale_x_continuous(breaks = seq(1,10, 1)) +\n     scale_y_continuous(limits = c(0,9000)) +\n  labs(y = \"Number #1 Votes\", x = \"Rank\", title = \"Bird of the Year 2024\", subtitle= \"Winner = Hoiho / Yellow-eyed Penguin\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Paired\", name = \"Maori name\")"
  },
  {
    "objectID": "charts/2025-04-22_stars/index.html",
    "href": "charts/2025-04-22_stars/index.html",
    "title": "day 22 stars",
    "section": "",
    "text": "Day 22 and the prompt is stars. Here I am looking at the age differences between love interests in Hollywood movies. Data found on Kaggle and downloaded from https://hollywoodagegap.com.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(ggrain)\nlibrary(ggeasy)\nlibrary(patchwork)\n\nage_diff &lt;- read_csv(here(\"charts\", \"2025-04-22_stars\", \"hollywood age.csv\")) %&gt;%\n  clean_names() %&gt;%\n  select(movie_name, release_year, age_difference)\n\n\nglimpse(age_diff)\n\nRows: 1,203\nColumns: 3\n$ movie_name     &lt;chr&gt; \"Harold and Maude\", \"Venus\", \"The Quiet American\", \"Sol…\n$ release_year   &lt;dbl&gt; 1971, 2006, 2002, 2009, 1998, 2010, 1992, 2016, 2009, 1…\n$ age_difference &lt;dbl&gt; 52, 50, 49, 45, 45, 43, 42, 41, 40, 39, 38, 38, 36, 36,…\n\n\n\nplot\nThe increase in the number of movies made across this period makes any change in age difference over time difficult to see. Maybe creating a new variable that groups movies into decade will help.\n\nage_diff %&gt;%\n  ggplot(aes(x = release_year, y = age_difference)) +\n  geom_jitter() \n\n\n\n\n\n\n\n\nHere I am making a new decade column using case_when() .\n\nage_diff_decade &lt;- age_diff %&gt;%\n  mutate(decade = case_when(release_year &lt; 1940 ~ \"1930s\", \n                            release_year &gt;= 1940 & release_year &lt; 1950 ~ \"1940s\", \n                            release_year &gt;= 1950 & release_year &lt; 1960 ~ \"1950s\", \n                            release_year &gt;= 1960 & release_year &lt; 1970 ~ \"1960s\", \n                            release_year &gt;= 1970 & release_year &lt; 1980 ~ \"1970s\", \n                            release_year &gt;= 1980 & release_year &lt; 1990 ~ \"1980s\", \n                            release_year &gt;= 1990 & release_year &lt; 2000 ~ \"1990s\", \n                            release_year &gt;= 2000 & release_year &lt; 2010 ~ \"2000s\", \n                            release_year &gt;= 2010 & release_year &lt; 2020 ~ \"2010s\", \n                            release_year &gt;= 2020 & release_year &lt; 2030 ~ \"2020s\"\n                            ))\n\nAnd plotting by decade instead of release year.\n\nage_diff_decade %&gt;%\n  ggplot(aes(x = decade, y = age_difference)) +\n  geom_jitter(width = 0.1, alpha = 0.5) \n\n\n\n\n\n\n\n\nI haven’t tried a raincloud plot in a while- this might be a good use case. Raincloud plot combine raw points, box plot, and half violin to get a good idea of the distribution of the data.\nQuick google and found the ggrain package.\n\n\nCode\np1 &lt;- age_diff_decade %&gt;%\n  filter(release_year &lt; 1980) %&gt;%\n  ggplot(aes(x = decade, y = age_difference, fill = decade)) +\n geom_rain(alpha = .5, \n            boxplot.args.pos = list(\n              width = .1, position = position_nudge(x = 0.2)),\n            violin.args.pos = list(\n              side = \"r\",\n              width = 0.7, position = position_nudge(x = 0.3))) +\n  theme_minimal() +\n  easy_remove_legend() +\n  scale_y_continuous(expand = c(0,0), limits = c(-.2, 55)) +\n  labs(y = \"Age difference\", x = \"Decade\", \n       subtitle = \"1930s - 1970s\")\n\np1\n\n\n\n\n\n\n\n\n\nCode\np2 &lt;- age_diff_decade %&gt;%\n  filter(release_year &gt;= 1980) %&gt;%\n  ggplot(aes(x = decade, y = age_difference, fill = decade)) +\n geom_rain(alpha = .5, \n            boxplot.args.pos = list(\n              width = .1, position = position_nudge(x = 0.2)),\n            violin.args.pos = list(\n              side = \"r\",\n              width = 0.7, position = position_nudge(x = 0.3))) +\n  theme_minimal() +\n  easy_remove_legend() +\n    scale_y_continuous(expand = c(0,0), limits = c(-.2, 55)) +\n  labs(y = \"Age difference\", x = \"Decade\",  subtitle = \"1980s - current\")\n\np2\n\n\n\n\n\n\n\n\n\nHere I am using the patchwork package to combine the plots\n\np1 + \n  labs(title = \"The age difference in years between movie love interests\") +\np2 +\n  labs(caption = \"Data from https://hollywoodagegap.com/\")"
  },
  {
    "objectID": "charts/2025-04-19_smooth/index.html",
    "href": "charts/2025-04-19_smooth/index.html",
    "title": "day 19_smooth",
    "section": "",
    "text": "Egg prices in the USA are high right now, mostly due to supply issues caused by bird flu outbreaks. I was interested in how prices compare to egg prices in NZ, which have been high since battery farms bans went into effect in 2022.\nThe data for these plots came from StatsNZ and the US Bureau of Labor Statistics and required a bit of wrangling to make the formats consistent. You can find the code that I wrote to read in and clean the egg data in this .R script"
  },
  {
    "objectID": "charts/2025-04-19_smooth/index.html#read-datafix-dates",
    "href": "charts/2025-04-19_smooth/index.html#read-datafix-dates",
    "title": "day 19_smooth",
    "section": "read data/fix dates",
    "text": "read data/fix dates\n\nlibrary(tidyverse)\nlibrary(here)\n\neggs &lt;- read_csv(here(\"charts\", \"2025-04-19_smooth\", \"eggsNZUSA.csv\")) \n\n\neggs &lt;- eggs %&gt;%\n    mutate(month_year = paste0(month, \"_\", year)) %&gt;%\n  mutate(date = lubridate::my(month_year))\n\nglimpse(eggs)\n\nRows: 253\nColumns: 6\n$ year       &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015,…\n$ month      &lt;chr&gt; \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Se…\n$ price      &lt;dbl&gt; 3.78, 3.79, 3.64, 3.71, 3.87, 3.83, 3.81, 3.77, 3.74, 3.76,…\n$ currency   &lt;chr&gt; \"NZD\", \"NZD\", \"NZD\", \"NZD\", \"NZD\", \"NZD\", \"NZD\", \"NZD\", \"NZ…\n$ month_year &lt;chr&gt; \"Jan_2015\", \"Feb_2015\", \"Mar_2015\", \"Apr_2015\", \"May_2015\",…\n$ date       &lt;date&gt; 2015-01-01, 2015-02-01, 2015-03-01, 2015-04-01, 2015-05-01…"
  },
  {
    "objectID": "charts/2025-04-06_florence/index.html",
    "href": "charts/2025-04-06_florence/index.html",
    "title": "day 6 florence",
    "section": "",
    "text": "I did not know that Florence Nightingale was also a data visualisation revolutionary, in addition to being a nurse and scientist. She realised that visualisations are important for telling data stories and her rose diagrams convinced Queen Victoria of the impact of sanitation on health for soldiers.\nThe Day 6 theme is Florence Nightingale, so I am on a mission to find an appropriate dataset.\n\n\n\nRose diagram by Florence Nightengale\n\n\nThis plot comparing the proportion of people willing to get the COVID vaccine in Australia and the USA illustrates how differences in public health policy can impact vaccine uptake.\nIt is interesting that across this period the proportion of people who were unvaccinated and unwilling remained pretty constant in the USA, whereas this proportion became smaller and smaller in Australia, as campaigns encouraging people to get vaccinated were rolled out.\nThe vaccination data seems like a good candidate for a rose diagram.\n\n\n\nload packages\n\nlibrary(tidyverse)\nlibrary(owidapi)\nlibrary(scales)\nlibrary(ggeasy)\nlibrary(janitor)\n\n\nread in the data\nHere I am reading the data from Our World in Data and selecting/renaming variables.\n\nvax &lt;- read_csv(\"https://ourworldindata.org/grapher/covid-vaccine-willingness-and-people-vaccinated-by-month.csv?v=1&csvType=full&useColumnShortNames=true\") %&gt;%\n  clean_names() %&gt;%\n  select(country = entity, date = day, unvax_unwilling = unwillingness_covid_vaccinate_this_week_pct_pop, \n         unvax_uncertain =  uncertain_covid_vaccinate_this_week_pct_pop , unvax_willing =  willingness_covid_vaccinate_this_week_pct_pop , vaccinated =  people_vaccinated_per_hundred ) \n\n\n\nclean it up\nI am interested in the comparison between Australia and the US so filter for those two countries and then make the data long, so that the percent values appear in one column, and the status categories in another. I make the status column a factor and pull the month out of the date variable.\n\nvax_long &lt;- vax %&gt;%\n  filter(country %in% c(\"Australia\", \"United States\")) %&gt;%\n  pivot_longer(names_to = \"status\", values_to = \"percent\", unvax_unwilling:vaccinated) %&gt;%\n  mutate(status = as_factor(status)) %&gt;%\n  mutate(month = month(date, label=TRUE))\n\nglimpse(vax_long)\n\nRows: 84\nColumns: 5\n$ country &lt;chr&gt; \"Australia\", \"Australia\", \"Australia\", \"Australia\", \"Australia…\n$ date    &lt;date&gt; 2021-03-15, 2021-03-15, 2021-03-15, 2021-03-15, 2021-04-15, 2…\n$ status  &lt;fct&gt; unvax_unwilling, unvax_uncertain, unvax_willing, vaccinated, u…\n$ percent &lt;dbl&gt; 27.72, 19.80, 51.77, 0.71, 31.32, 20.13, 43.73, 4.82, 31.14, 2…\n$ month   &lt;ord&gt; Mar, Mar, Mar, Mar, Apr, Apr, Apr, Apr, May, May, May, May, Ju…\n\n\n\n\nplot\n\njust Australia\nOK I am going to start by plotting Australia first and getting column graph, before adding the coord_polar() below. I want the plot to start with the March data, so relevel month as a factor.\n\noz &lt;- vax_long %&gt;%\n  filter(country == \"Australia\") \n\noz$month &lt;- fct_relevel(oz$month, \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \n                              \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nlevels(oz$month)\n\n [1] \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\" \"Jan\" \"Feb\"\n\npalette &lt;- c(\"#c5001f\",\"#f6b79b\", \"#a8d0e4\", \"#498dc0\" )\n\noz %&gt;%\n  ggplot(aes(x = month, y = percent, fill = status)) +\n  geom_col() +\n  scale_fill_manual(values = palette)\n\n\n\n\n\n\n\n\nAdding coord_polar() to turn it into a Florence Nightengale plot. Here I have removed the y axis, made the background white and moved the legend to the bottom.\n\noz %&gt;%\n  ggplot(aes(x = month, y = percent, fill = status)) +\n  geom_col() +\n  scale_fill_manual(values = palette) +\n  coord_polar() +\n  easy_remove_axes(which = \"y\") +\n  theme(panel.background = element_rect(fill = 'white', colour = 'white')) +\n  easy_move_legend(to = \"bottom\") \n\n\n\n\n\n\n\n\n\n\nboth Australia and USA\nNow I want to use facet_wrap() to get a plot that compares this pattern for the US and Australia. In looking at the data, the USA data only goes until Oct, so I am filtering so that I have the same months represented for each country.\n\nmonths &lt;- c(\"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \n                              \"Sep\", \"Oct\")\n\nus_oz &lt;- vax_long %&gt;%\n  filter(month %in% months)\n\n\nus_oz %&gt;%\n  ggplot(aes(x = month, y = percent, fill = status)) +\n  geom_col() +\n  scale_fill_manual(values = palette) +\n  coord_polar() +\n  facet_wrap(~country) +\n  easy_remove_axes(which = \"y\") +\n  theme(panel.background = element_rect(fill = 'white', colour = 'white')) +\n  easy_move_legend(to = \"bottom\") +\n  labs(title = \"Willingness to get vaccinated against COVID-19\", \n       subtitle = \"Mar 15, 2021 to Oct 15, 2021\") +\n  easy_remove_axes(which = \"x\", what = \"title\")\n\n\n\n\n\n\n\n\nSuccess- this style of plot really nicely illustrates the lack change in willingness to get the COVID vaccine across this period in the US."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "tips and tricks\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRYouWithMe\n\n\n\n\n\n\n\n\nMar 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPsychEd podcast\n\n\n\n\n\n\n\n\nNov 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\ncareers\n\n\n\n\n\n\n\n\nOct 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nJenny’s (very opinionated) tips for academic writing\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\n\n\n\nNo matching items"
  }
]