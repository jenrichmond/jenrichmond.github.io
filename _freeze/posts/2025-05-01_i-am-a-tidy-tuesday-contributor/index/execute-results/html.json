{
  "hash": "d27be68fd6e716bda69748fe9456095a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"i am a tidy tuesday contributor\"\nauthor:\n  - name: Jen Richmond\n    url: https://jenrichmond.github.io/\ndate: 2025-05-05\nformat: html\neditor: source\ndraft: TRUE\nimage: featured.png\n---\n\n\n\nHave you tried #TidyTuesday? It is a weekly data challenge where the team from the Data Science Learning Community curate and post a dataset to their github repository, then data nerds from all over the world have a go at making a cool visualisation with it and everyone shares what they came up with on social media via the hashtag #tidytuesday. \n\nData organising within the #rstats community takes work; TidyTuesday doesn't just happen. Ted Laderas has [talked a lot about burnout](https://laderast.github.io/meetup_burnout/) among organisers. It is common in volunteer settings for 20% of the people to do 80% of the work but this can lead to unsustainable communities. Ted points to importance of expanding your core group of organisers and making it easy for people to contribute, as ways to make a data initiative work better for everyone; many hands make light work. \n\nRecently the #tidytuesday team have put this philosophy into action by creating some functions within the `tidytuesdayR` package that make it easy to curate and contribute a dataset to the challenge. \n\n# How to use `tidytuesdayR` as a participant\n\nThe package has a number of functions that make it super easy for you get the data into RStudio. No need to download the csv and read it back in. Just use the `tt_load()` with the date or year and week. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"tidytuesdayR\")\n\nlibrary(tidytuesdayR)\n\ntues_data <- tidytuesdayR::tt_load(\"2025-04-29\") \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n---- Compiling #TidyTuesday Information for 2025-04-29 ----\n--- There is 1 file available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 1: \"user2025.csv\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# OR use year and week\n# tues_data <- tidytuesdayR::tt_load(2025, week = 17) \n```\n:::\n\n\n\nYour tues_data object will be a list that will sometimes contain more than one table so looking at the struture of the object that `tt_load()` returns using `str()` will give you an idea of which dataframe might be of interest. In this case the data is about the userR2025 conference schedule.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(tues_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 1\n $ user2025: spc_tbl_ [128 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ id             : num [1:128] 170 79 30 31 39 169 94 163 13 51 ...\n  ..$ session        : chr [1:128] \"Virtual\" \"Virtual\" \"Virtual\" \"Virtual\" ...\n  ..$ date           : Date[1:128], format: \"2025-08-01\" \"2025-08-01\" ...\n  ..$ time           : chr [1:128] \"TBD\" \"TBD\" \"TBD\" \"TBD\" ...\n  ..$ room           : chr [1:128] \"Online\" \"Online\" \"Online\" \"Online\" ...\n  ..$ title          : chr [1:128] \"A Robust and Informative Application for viewing the dataframes in R\" \"A first look at Positron\" \"Analyzing Census Data in R: Techniques and Applications\" \"Automating workflows with webhooks and plumber in R\" ...\n  ..$ content        : chr [1:128] \"In R programming, the View() function from the Utils package provides a basic interface for viewing the datafra\"| __truncated__ \"Positron is a next generation data science IDE built by the creators of RStudio. It has been available for beta\"| __truncated__ \"This talk provides an introduction to working with IPUMS Census American Community Survey (ACS) data in R, focu\"| __truncated__ \"Webhooks have brought to us new possibilities for automating workflows. With such, we can eliminate the need fo\"| __truncated__ ...\n  ..$ video_recording: chr [1:128] \"✅\" \"✅\" \"✅\" \"✅\" ...\n  ..$ keywords       : chr [1:128] \"statistical programming, clinical trials data, dataset interface, workflow\" \"ide, workflow, tooling\" \"demography, frameworks, census data, equity ml/ai, anti-discrimination in ml/ai\" \"automation, event-driven workflows, plumber api, github webhooks\" ...\n  ..$ speakers       : chr [1:128] \"Madhan Kumar Nagaraji\" \"Julia Silge (Posit PBC)\" \"Joanne Rodrigues\" \"CLINTON DAVID\" ...\n  ..$ co_authors     : chr [1:128] NA NA NA NA ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   id = col_double(),\n  .. ..   session = col_character(),\n  .. ..   date = col_date(format = \"\"),\n  .. ..   time = col_character(),\n  .. ..   room = col_character(),\n  .. ..   title = col_character(),\n  .. ..   content = col_character(),\n  .. ..   video_recording = col_character(),\n  .. ..   keywords = col_character(),\n  .. ..   speakers = col_character(),\n  .. ..   co_authors = col_character()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr> \n - attr(*, \".tt\")= 'tt' chr \"user2025.csv\"\n  ..- attr(*, \".files\")='data.frame':\t1 obs. of  3 variables:\n  .. ..$ data_files: chr \"user2025.csv\"\n  .. ..$ data_type : chr \"csv\"\n  .. ..$ delim     : chr \",\"\n  ..- attr(*, \".readme\")=List of 2\n  .. ..$ node:<externalptr> \n  .. ..$ doc :<externalptr> \n  .. ..- attr(*, \"class\")= chr [1:2] \"xml_document\" \"xml_node\"\n  ..- attr(*, \".date\")= Date[1:1], format: \"2025-04-29\"\n - attr(*, \"class\")= chr \"tt_data\"\n```\n\n\n:::\n\n```{.r .cell-code}\nuser25 <- tues_data$user2025\n```\n:::\n\n\n\n\n\n# How to use `tidytuesdayR` as a contributor\n\nDo you have an idea of a dataset that might be of interest to other tidytuesdayers? Great! the new functions in `tidytuesdayR` make it super easy to contribute a dataset. \n\nAll you have to do is follow [these instructions](https://dslc-io.github.io/tidytuesdayR/articles/curating.html) \n\n### Step 1: write a cleaning script \n\nThis function opens a cleaning.R script that you can use to write the code you need to get your data file from its raw state into a state that other people can use. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntt_clean()\n```\n:::\n\n\n\n### Step 2: save your clean data to .csv\n\nOnce you have written your cleaning.R script and checked that it produces clean dataframes, you can save your datafile. This function will save your dataframes as .csv in your submission folder. It will also open a .md file with a table that you can complete that describes each of the variables in your dataset. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntt_save_dataset(nameofyourdf)\n```\n:::\n\n\n\n### Step 3: introduce your data\n\nThis function opens another .md file that you can use to write an introduction to your dataset. You can describe the data, where it comes from and suggest some questions that people might like to explore. You also want to think about an image that can go along with the data. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntt_intro()\n```\n:::\n\n\n\n### Step 4: meta data\n\nThis bit was cool- this function walks you through a question and answer session, filling in all the details needed for the meta data. Once you are done answering the questions, it creates a meta.yaml file. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntt_meta()\n```\n:::\n\n\n\n### Step 5: submit\n\nThis bit creates a pull request, but relies on the project you are working in is linked to github. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntt_submit()\n```\n:::\n\n\n\n### DONE!",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}